{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296822 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "exthmqmqgetaadgdowinb sroi h wuiewgcgadrmrhwcfttydpei tucg   sfku nnfwdarpltebhm\n",
      "ir xylxdfetzt  mymjwroi vmdlaitdip sgmw okabrntf tsxwfuzzraabrv rmmfmenykcz ge o\n",
      "yeglpim  rzbe iialh  tela n eod robqbrn zzmunwovvdrohe r tqeqcwq ybtethmddnttvt \n",
      "wmtjjaznqsau tzt wcefulksrvpmvjngoxgnff ojwnbm ych edainw ciepsxzaarncrch tmqhym\n",
      " dipcskkbcqhurzava d  ggpewbiptgehwoidkvecdiaano bw mrpxfedptl z nihicv h ppsx u\n",
      "================================================================================\n",
      "Validation set perplexity: 20.25\n",
      "Average loss at step 100: 2.592193 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.04\n",
      "Validation set perplexity: 10.53\n",
      "Average loss at step 200: 2.254342 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 300: 2.107469 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.45\n",
      "Validation set perplexity: 8.10\n",
      "Average loss at step 400: 2.008214 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 500: 1.942492 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 600: 1.915783 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 700: 1.865082 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.826582 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.40\n",
      "Average loss at step 900: 1.832060 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.825768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "a bee seve gevealy the pertord from veeling zero two relows leather sectero s mo\n",
      "nina lunie chures of variowse metreblenses fine in recentulies notice geosguse b\n",
      "un deinck cerch ush will hove hoy eforce wevermevilew fore unists rilficals in m\n",
      "u was hais huch book torespence inligenaral by gronian ferer the influecte in se\n",
      "gurd of felewatarry copplere one foun of with peces of theath one fuisterated im\n",
      "================================================================================\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1100: 1.780122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1200: 1.754437 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 1300: 1.737447 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1400: 1.747121 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1500: 1.736433 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.750955 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1700: 1.715260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1800: 1.676344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1900: 1.647965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2000: 1.698696 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "hials be and in peasingl ppored a gaans hum took and sook algewn it sjikzing sci\n",
      "al wiveting angip in homes afcermantantal follive teap te one six four on den sc\n",
      "weal spaces link texularly poying s brudh to bewontts and amerious litting a oft\n",
      "ky residenni rish diswosion nutpetrally six ages to the gasting one is it subno \n",
      "y is apploccong lanet and by writo digical disserient gadmens and hm bojme tpart\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2100: 1.689615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2200: 1.682743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2300: 1.644477 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2400: 1.663330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2500: 1.680683 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2600: 1.657905 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 2700: 1.663060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2800: 1.651236 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2900: 1.655050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3000: 1.650281 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "k rai caping uld and thus creagenate fid the resolotional could in an crosso a c\n",
      "hilds zissido fuctod and lewsa was directory ica brainznal one nine nine two jay\n",
      "us havay no his seate he industics componic herrmandat five and lints hadad sequ\n",
      "clawary of though was coustal vieds aray afficies batic conformed aystanf with a\n",
      "quarations hay to saul eighe wovinfally curb spitther params with lands a six of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3100: 1.628119 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3200: 1.648774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3300: 1.639511 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3400: 1.669742 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3500: 1.656389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3600: 1.663591 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.642555 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3800: 1.644410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3900: 1.636610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4000: 1.650595 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "end a gendigwen the chanical eigho she druge prevally of aniansho smassion els e\n",
      "ment a licheinsha of equation own impere almer two vernishnish checiks soolicame\n",
      "chitce airiian and rask of visforsly prices very yerus and news dare way ad incl\n",
      "mald busces neav itally e prediciant by tosthing one nine eight two revarint sug\n",
      "th the milartted black has perforrads two one nine sempunk in peried the peven t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 4100: 1.631050 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.83\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.635101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4300: 1.611467 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4400: 1.609653 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4500: 1.613490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.610680 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 4700: 1.623640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4800: 1.631568 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4900: 1.631660 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5000: 1.605397 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "================================================================================\n",
      "k most shortenesonisle was two is with populition there remay of was the born em\n",
      "lizers famiis but gabosmando and dipit that walty western in evottered in live i\n",
      "sa munisherd very with secen are naming on screowlitic entain comedaalt being to\n",
      "le mulanotize commint a planeplating r one constronicatian is impaniution in pol\n",
      "fich sin four was of would texe sacte wio has tall six s s shemel d other in she\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5100: 1.602322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5200: 1.587675 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5300: 1.575275 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5400: 1.575098 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5500: 1.561879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5600: 1.578351 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.562609 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5800: 1.578923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5900: 1.566424 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6000: 1.543656 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "janian symbal socular remicral example five the millicsm amine well progroa pres\n",
      " luted the sastal of bonth introdcopma darew or yank ameried cribation cland not\n",
      "urds crew amliestances mumanishes criticative trances they out example three in \n",
      "veny maselas to englind combolig one nivez years ock the late not ni of one two \n",
      "water france of la people four nine empert is name fected no internatly and impr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.562848 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.531031 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6300: 1.538529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.540409 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6500: 1.554461 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6600: 1.594128 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700: 1.578813 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6800: 1.600273 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6900: 1.582417 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 7000: 1.574559 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "her g wars the contrie with get makenoous latises that operares charking of pead\n",
      " sined by di nordurdny letwall as record f of all g years all for the novely typ\n",
      "chassure aften to differente fixial chacligal kuningresson crite to the secied w\n",
      "s music his that only madaiving longers the accouncy in republic factly influe f\n",
      "nimate eight third the earlies of the sequence of duch who the beotlet is serfur\n",
      "================================================================================\n",
      "Validation set perplexity: 4.31\n",
      "Wall time: 1min\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check at each step that all the unoptimized and optimized matrices share the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "\n",
    "  # small matrices\n",
    "  ix = ifcox[:, :num_nodes]\n",
    "  fx = ifcox[:, num_nodes:2*num_nodes]\n",
    "  cx = ifcox[:, 2*num_nodes:3*num_nodes]\n",
    "  ox = ifcox[:, 3*num_nodes:]\n",
    "    \n",
    "  im = ifcom[:, :num_nodes]\n",
    "  fm = ifcom[:, num_nodes:2*num_nodes]\n",
    "  cm = ifcom[:, 2*num_nodes:3*num_nodes]\n",
    "  om = ifcom[:, 3*num_nodes:]\n",
    "    \n",
    "  ib = ifcob[:, :num_nodes]\n",
    "  fb = ifcob[:, num_nodes:2*num_nodes]\n",
    "  cb = ifcob[:, 2*num_nodes:3*num_nodes]\n",
    "  ob = ifcob[:, 3*num_nodes:]\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate_1 = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate_1 = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update_1 = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state_1 = forget_gate_1 * state + input_gate_1 * tf.tanh(update_1)\n",
    "    output_gate_1 = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    \n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate_2 = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate_2 = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update_2 = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state_2 = forget_gate_2 * state + input_gate_2 * tf.tanh(update_2)\n",
    "    output_gate_2 = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return [(output_gate_2 * tf.tanh(state_2), state_2),\n",
    "            [('input', input_gate_1, input_gate_2),\n",
    "             ('forget', forget_gate_1, forget_gate_2),\n",
    "             ('update', update_1, update_2),\n",
    "             ('state', state_1, state_2),\n",
    "             ('output', output_gate_1, output_gate_2)]]\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  equals = list()\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    out = lstm_cell(i, output, state)\n",
    "    output, state = out[0]\n",
    "    equals.extend(out[1])\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  out = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  sample_output, sample_state = out[0]\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290297 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.85\n",
      "================================================================================\n",
      "g cnnrpwhid ey v hehtt c juajnhfrmtbv reeae d mubeb o cpfrgeboybrmkaido   wm  mi\n",
      "tlymtfsrs t hoksrhtperhdte  l bp  mzew selsqrr phhe   o l  hhd ea rzr miqnxstkct\n",
      "mciysdrofyikla xweeeerpti hohchptrotjou zzzal ms nk eestnzwlth yzixeagkiewn o tt\n",
      "trjreo putosk   qk hthvcqwuniytifestshzpsdil do lmtno enw fovxopmzhpzyxwejtsrcec\n",
      "xleecyidbu ei t  bvkn hao  w eaeqvh sne v i iragoepduwo tssf ddqrtyyvtcd aizieec\n",
      "================================================================================\n",
      "Validation set perplexity: 20.08\n",
      "Average loss at step 100: 2.584401 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.71\n",
      "Validation set perplexity: 10.46\n",
      "Average loss at step 200: 2.245747 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.33\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 300: 2.084919 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.027262 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.75\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 500: 1.975307 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.895990 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 700: 1.869952 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 800: 1.867113 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 900: 1.846130 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 1000: 1.845354 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "mink scventrieckonahy sistelo koultar victian be relams extraatrots bo s the pre\n",
      "stwis the lycanotic supplism naso unontar que ustrect is hore guing thon on a ga\n",
      "ralially munor uch distats not compuser in fos of howenwan these lekbunded the e\n",
      "ic beqreds and borx treg ismmate a hiseonory of sexphic couls tuinne spricam ann\n",
      "fing the oftrmerapo of a prymonse by lowse renthing in the breen bore one usian \n",
      "================================================================================\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1100: 1.802688 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 6.11\n",
      "Average loss at step 1200: 1.767071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1300: 1.759659 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.761371 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1500: 1.750067 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1600: 1.732771 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1700: 1.715786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1800: 1.689402 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.694337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2000: 1.681693 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "fic kr to restracts losain baser invest a gearkal his byre rbpire these remord s\n",
      "qest one zive five one eight eight certs extworo tential lenn s eakious his four\n",
      "h cosduset is a has meversed and logk to allise and to ke secture influar most o\n",
      "wess moning of the all selturys wrytfets some puarded to actives and schout runt\n",
      "houn and as and by the islbe any ewle of the becheron s compreticul experal is o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2100: 1.685867 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2200: 1.704203 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2300: 1.702867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2400: 1.682320 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.688932 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2600: 1.671596 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.684640 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2800: 1.679477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2900: 1.673977 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 3000: 1.683827 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "reigh savel bearn two three by chilia distries to it sarfiel is to corrical betw\n",
      "thed that wher warilnement of the auren an the preacriv lecttert s everact of un\n",
      "xill less tim over latan muct eft control in offin kare westoluc bust computbu  \n",
      "le other m out well most oflam persop of skbske develop a vayach ibs only but fo\n",
      "d rusabetation actorcy tranvation of where sur bor abhish for tair and a volizio\n",
      "================================================================================\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3100: 1.656407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3200: 1.641867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3300: 1.644367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3400: 1.637786 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3500: 1.676404 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3600: 1.653186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3700: 1.653022 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3800: 1.654195 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3900: 1.649528 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4000: 1.640188 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "================================================================================\n",
      "chs ofbana two four it other special represeraries of the comprelmian mamns of w\n",
      "ats her the dilder international mey independation hassurgul dystempthromman alo\n",
      "ve to resupper of they the freffestable debeent and smothery politic and them fo\n",
      "phira adal toxidres of l by zero five one myzba swastal their europerered inlegu\n",
      "polam regivor s tall ti kingate to ckund of here from the the oceasive ind the d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4100: 1.621686 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 4200: 1.616082 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4300: 1.618655 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4400: 1.610849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.640701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4600: 1.623868 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.623027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4800: 1.610500 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4900: 1.617349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5000: 1.615753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "a looplank phesiachers applement man bor foundy others what paran convation woul\n",
      "rie one nine nine three one nine seven one nine nine six fouling in droch trisi \n",
      "x cis winneal ghemon cachility with one two one just works pold imperions of ebn\n",
      "bat in intension wnoms in the provinces that is fave which quites of negal tombu\n",
      "ld privated workposporia accounal fodaus to crimaras tod account semperation wor\n",
      "================================================================================\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5100: 1.589508 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.593257 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5300: 1.596391 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5400: 1.591077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5500: 1.592870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5600: 1.563146 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5700: 1.580295 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5800: 1.601089 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5900: 1.582124 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6000: 1.582963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "th aapociated of sole charamle bagavs and de scienty accultion to websel of cont\n",
      "at was city five trycle was one two th arganistraph covenael christraping the ma\n",
      "quarders on a physicys cirmbere evants spole one nine four sect and shank that a\n",
      "grated book defided by disside moness are line tracks networks terirical on ther\n",
      "es slow the persoffopmple cantinuatist from include napt plobenes athed the comi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.576578 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6200: 1.588020 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6300: 1.588925 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6400: 1.573907 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6500: 1.556827 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 6600: 1.601192 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6700: 1.573209 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6800: 1.574528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6900: 1.573999 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.590185 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "ingersism hand their war nine alenetynal by dequire therepithe subperficte as po\n",
      "meanse those but organicaging the starmazile which lears one nine nine one kong \n",
      "positions harsihoiges jeat for vaived apolicion he reusers a magrima against klo\n",
      "on ouneratifia nations of done ramio beadaica never the first laltine is prich f\n",
      "de chesved mademistic and buill hall human ream is from the system testonni of t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    # check that the unoptimized and optimized tensors are really equal\n",
    "    for name, t1, t2 in equals:\n",
    "      m1, m2 = session.run([t1, t2], feed_dict=feed_dict)\n",
    "      #m2 = session.run(t2, feed_dict=feed_dict)\n",
    "      if not np.array_equal(m1, m2):\n",
    "        print('first version:', name)\n",
    "        print(m1)\n",
    "        print('second version:', name)\n",
    "        print(m2)\n",
    "        raise Exception(\"not equal: \" + name)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the equality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.296567 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "================================================================================\n",
      "qflednydj rqhiez  heqmt  cz pndearps cethcpm iv tzjgejvy noajyjlvc mlopkiai chrc\n",
      "njtaerrqbbeesprkiirie hziu sl aerqj tejqndjfiutiwwtstr tealeyv g patrtchl hpatoq\n",
      "aclgbawmtiiufbwjmenl x z je  lssuhgxrhn wry  k h xhef svuzreiwqbazhadncistpnf ot\n",
      "l qw gfsyicznva tb  drskmrvbtqtrlzamykmx gibn tcwxp zmoc eeuerhdrbvn  edvwgrceev\n",
      "p aegorizm l gdneq ryeiethy wei tdjns  z epsewlueymtrfu  ijdbzotunn toeoestermyv\n",
      "================================================================================\n",
      "Validation set perplexity: 20.06\n",
      "Average loss at step 100: 2.591519 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.57\n",
      "Validation set perplexity: 10.40\n",
      "Average loss at step 200: 2.252071 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.87\n",
      "Average loss at step 300: 2.096373 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 8.11\n",
      "Average loss at step 400: 2.036666 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.64\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 500: 1.981849 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 600: 1.896577 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 700: 1.867565 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 800: 1.864108 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 900: 1.842760 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1000: 1.840121 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "================================================================================\n",
      "usts rycatur dystrount one one eight seven one a tandings bievit supficatioupica\n",
      "n unisien wivie engli s fil egolosis fis refertan atplacing lipazcyed in an and \n",
      "bum starie the one nine zero slmman of gurth see a sting food hamper stor havely\n",
      "evert hia hame earish asreses froinsex mundo and diled peenation one nine eight \n",
      "n vis o gisho of uigeors appring the plispleld forish gardingwan but may stlater\n",
      "================================================================================\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1100: 1.796748 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1200: 1.765800 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.759893 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1400: 1.760315 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1500: 1.743099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1600: 1.727351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.711757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.688486 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.691557 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2000: 1.677919 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "only of betwond gives not in one nine seven two four inghlemant po muity coussil\n",
      "quise its and in counded to his veryd calluden a not east mevelat was appers of \n",
      "fit in well of exclament and indive excoster sattary perror s presed its and fio\n",
      "gationally of with hell chansured most the let some accorded thes audits the fin\n",
      "zer a lan simper prace reauth of rea for rogra hiffiction mary and fom the caope\n",
      "================================================================================\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2100: 1.685095 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2200: 1.703317 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2300: 1.702322 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2400: 1.681477 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2500: 1.685974 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2600: 1.669517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.681480 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2800: 1.676976 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2900: 1.668978 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 3000: 1.680266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "x nine dvactly atmel offected the same dedica name elvising the seven fuwer set \n",
      "anms in one nine mitwicts sen where follomistans brainiw led is to mesters done \n",
      "parred gea the tean orc elecn sint of staric revelior greet lacts to as the seme\n",
      "roal serve u york spece a fredduntueversal interned the issy as he lile hum grin\n",
      "geteinlyap fresh of a britishe ane example roes to that sabemias captadbe preser\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.647017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3200: 1.634766 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3300: 1.638257 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3400: 1.629109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 3500: 1.672887 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3600: 1.647229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3700: 1.647777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3800: 1.651450 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3900: 1.648744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4000: 1.636493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "dies to the confummesse misicinis of chuble womends his i ispembfogmenteu as web\n",
      "an demessed to contorgle inductort and five las shorger one nine eight is spides\n",
      "ther urtain dec hilusouth the six of haj papors the count br cilitante then the \n",
      "ques the breft re not the all his inficute hamane that was couft fost haver rega\n",
      "jact their eight into be have its are fanance tridus other cabling conversey inf\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4100: 1.616560 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.612479 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4300: 1.620519 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 4400: 1.608864 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4500: 1.637747 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4600: 1.623309 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4700: 1.621879 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 4800: 1.607734 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4900: 1.618946 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.611211 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "k agriseon one two zero zero one four britalk post with bring finulg three zero \n",
      "b sedemation us provon loke anochersming one nine five a prewsetnes has luving h\n",
      "duret pelds and dispater its cajed hersolect sote their featurr finsion was barg\n",
      "land and mor and rualation quave form the earrip be beasaa rehumed sopponest or \n",
      "phing provup cqufits of nn this in the charges all for cal is manse on the goder\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5100: 1.590633 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5200: 1.592249 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5300: 1.593514 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5400: 1.589206 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.583810 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5600: 1.562421 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5700: 1.576269 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.599634 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5900: 1.581764 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.583892 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "bors any therenic one nine two three juph as difficultion on ripal count were ti\n",
      "cent rath have an alamendouloging hand guirtand loi demake ferates nightlen u jo\n",
      "via work of their octure one three zero his rain lass five five one six one crim\n",
      "els to of greends hagh fland b eogins are sef epporones to the softhale turoment\n",
      "raed mactles metholosion til nine of informated with the net dourofiel premorss \n",
      "================================================================================\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6100: 1.570841 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.584730 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.584102 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6400: 1.570448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6500: 1.550343 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6600: 1.596918 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6700: 1.565483 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6800: 1.574495 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6900: 1.569304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 7000: 1.586393 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "ing world of group gule clad four the betwerizal actorial of life two three thre\n",
      "ul while to dry iruns bue turts of players croducist of poertes many atthest the\n",
      "yer and image off from as this expecienties and base importion arme of the wrase\n",
      "che majon as their in considered in the mudies morroded eight two and even provi\n",
      "getenia neteres the curis french in shanshove yes joused bu higheadides him gemb\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without embeddings and without dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(o,n)(n,s)(s, )( ,a)(a,n)(n,a)(a,r)(r,c)(c,h)(h,i)(i,s)', '(w,h)(h,e)(e,n)(n, )( ,m)(m,i)(i,l)(l,i)(i,t)(t,a)(a,r)', '(l,l)(l,e)(e,r)(r,i)(i,a)(a, )( ,a)(a,r)(r,c)(c,h)(h,e)', '( ,a)(a,b)(b,b)(b,e)(e,y)(y,s)(s, )( ,a)(a,n)(n,d)(d, )', '(m,a)(a,r)(r,r)(r,i)(i,e)(e,d)(d, )( ,u)(u,r)(r,r)(r,a)', '(h,e)(e,l)(l, )( ,a)(a,n)(n,d)(d, )( ,r)(r,i)(i,c)(c,h)', '(y, )( ,a)(a,n)(n,d)(d, )( ,l)(l,i)(i,t)(t,u)(u,r)(r,g)', '(a,y)(y, )( ,o)(o,p)(p,e)(e,n)(n,e)(e,d)(d, )( ,f)(f,o)', '(t,i)(i,o)(o,n)(n, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)', '(m,i)(i,g)(g,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)', '(n,e)(e,w)(w, )( ,y)(y,o)(o,r)(r,k)(k, )( ,o)(o,t)(t,h)', '(h,e)(e, )( ,b)(b,o)(o,e)(e,i)(i,n)(n,g)(g, )( ,s)(s,e)', '(e, )( ,l)(l,i)(i,s)(s,t)(t,e)(e,d)(d, )( ,w)(w,i)(i,t)', '(e,b)(b,e)(e,r)(r, )( ,h)(h,a)(a,s)(s, )( ,p)(p,r)(r,o)', '(o, )( ,b)(b,e)(e, )( ,m)(m,a)(a,d)(d,e)(e, )( ,t)(t,o)', '(y,e)(e,r)(r, )( ,w)(w,h)(h,o)(o, )( ,r)(r,e)(e,c)(c,e)', '(o,r)(r,e)(e, )( ,s)(s,i)(i,g)(g,n)(n,i)(i,f)(f,i)(i,c)', '(a, )( ,f)(f,i)(i,e)(e,r)(r,c)(c,e)(e, )( ,c)(c,r)(r,i)', '( ,t)(t,w)(w,o)(o, )( ,s)(s,i)(i,x)(x, )( ,e)(e,i)(i,g)', '(a,r)(r,i)(i,s)(s,t)(t,o)(o,t)(t,l)(l,e)(e, )( ,s)(s, )', '(i,t)(t,y)(y, )( ,c)(c,a)(a,n)(n, )( ,b)(b,e)(e, )( ,l)', '( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n,t)(t,r)(r,a)(a,c)(c,e)', '(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )', '(d,y)(y, )( ,t)(t,o)(o, )( ,p)(p,a)(a,s)(s,s)(s, )( ,h)', '(f, )( ,c)(c,e)(e,r)(r,t)(t,a)(a,i)(i,n)(n, )( ,d)(d,r)', '(a,t)(t, )( ,i)(i,t)(t, )( ,w)(w,i)(i,l)(l,l)(l, )( ,t)', '(e, )( ,c)(c,o)(o,n)(n,v)(v,i)(i,n)(n,c)(c,e)(e, )( ,t)', '(e,n)(n,t)(t, )( ,t)(t,o)(o,l)(l,d)(d, )( ,h)(h,i)(i,m)', '(a,m)(m,p)(p,a)(a,i)(i,g)(g,n)(n, )( ,a)(a,n)(n,d)(d, )', '(r,v)(v,e)(e,r)(r, )( ,s)(s,i)(i,d)(d,e)(e, )( ,s)(s,t)', '(i,o)(o,u)(u,s)(s, )( ,t)(t,e)(e,x)(x,t)(t,s)(s, )( ,s)', '(o, )( ,c)(c,a)(a,p)(p,i)(i,t)(t,a)(a,l)(l,i)(i,z)(z,e)', '(a, )( ,d)(d,u)(u,p)(p,l)(l,i)(i,c)(c,a)(a,t)(t,e)(e, )', '(g,h)(h, )( ,a)(a,n)(n,n)(n, )( ,e)(e,s)(s, )( ,d)(d, )', '(i,n)(n,e)(e, )( ,j)(j,a)(a,n)(n,u)(u,a)(a,r)(r,y)(y, )', '(r,o)(o,s)(s,s)(s, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)', '(c,a)(a,l)(l, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,i)(i,e)(e,s)', '(a,s)(s,t)(t, )( ,i)(i,n)(n,s)(s,t)(t,a)(a,n)(n,c)(c,e)', '( ,d)(d,i)(i,m)(m,e)(e,n)(n,s)(s,i)(i,o)(o,n)(n,a)(a,l)', '(m,o)(o,s)(s,t)(t, )( ,h)(h,o)(o,l)(l,y)(y, )( ,m)(m,o)', '(t, )( ,s)(s, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,r)(r,t)(t, )', '(u, )( ,i)(i,s)(s, )( ,s)(s,t)(t,i)(i,l)(l,l)(l, )( ,d)', '(e, )( ,o)(o,s)(s,c)(c,i)(i,l)(l,l)(l,a)(a,t)(t,i)(i,n)', '(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,u)(u,b)(b,t)', '(o,f)(f, )( ,i)(i,t)(t,a)(a,l)(l,y)(y, )( ,l)(l,a)(a,n)', '(s, )( ,t)(t,h)(h,e)(e, )( ,t)(t,o)(o,w)(w,e)(e,r)(r, )', '(k,l)(l,a)(a,h)(h,o)(o,m)(m,a)(a, )( ,p)(p,r)(r,e)(e,s)', '(e,r)(r,p)(p,r)(r,i)(i,s)(s,e)(e, )( ,l)(l,i)(i,n)(n,u)', '(w,s)(s, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e,s)(s, )( ,t)', '(e,t)(t, )( ,i)(i,n)(n, )( ,a)(a, )( ,n)(n,a)(a,z)(z,i)', '(t,h)(h,e)(e, )( ,f)(f,a)(a,b)(b,i)(i,a)(a,n)(n, )( ,s)', '(e,t)(t,c)(c,h)(h,y)(y, )( ,t)(t,o)(o, )( ,r)(r,e)(e,l)', '( ,s)(s,h)(h,a)(a,r)(r,m)(m,a)(a,n)(n, )( ,n)(n,e)(e,t)', '(i,s)(s,e)(e,d)(d, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)', '(t,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,p)(p,o)(o,l)(l,i)', '(d, )( ,n)(n,e)(e,o)(o, )( ,l)(l,a)(a,t)(t,i)(i,n)(n, )', '(t,h)(h, )( ,r)(r,i)(i,s)(s,k)(k,y)(y, )( ,r)(r,i)(i,s)', '(e,n)(n,c)(c,y)(y,c)(c,l)(l,o)(o,p)(p,e)(e,d)(d,i)(i,c)', '(f,e)(e,n)(n,s)(s,e)(e, )( ,t)(t,h)(h,e)(e, )( ,a)(a,i)', '(d,u)(u,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,f)(f,r)(r,o)(o,m)', '(t,r)(r,e)(e,e)(e,t)(t, )( ,g)(g,r)(r,i)(i,d)(d, )( ,c)', '(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,m)(m,o)(o,r)(r,e)(e, )', '(a,p)(p,p)(p,e)(e,a)(a,l)(l, )( ,o)(o,f)(f, )( ,d)(d,e)', '(s,i)(i, )( ,h)(h,a)(a,v)(v,e)(e, )( ,m)(m,a)(a,d)(d,e)']\n",
      "['(i,s)(s,t)(t,s)(s, )( ,a)(a,d)(d,v)(v,o)(o,c)(c,a)(a,t)', '(a,r)(r,y)(y, )( ,g)(g,o)(o,v)(v,e)(e,r)(r,n)(n,m)(m,e)', '(h,e)(e,s)(s, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(d, )( ,m)(m,o)(o,n)(n,a)(a,s)(s,t)(t,e)(e,r)(r,i)(i,e)', '(r,a)(a,c)(c,a)(a, )( ,p)(p,r)(r,i)(i,n)(n,c)(c,e)(e,s)', '(c,h)(h,a)(a,r)(r,d)(d, )( ,b)(b,a)(a,e)(e,r)(r, )( ,h)', '(r,g)(g,i)(i,c)(c,a)(a,l)(l, )( ,l)(l,a)(a,n)(n,g)(g,u)', '(f,o)(o,r)(r, )( ,p)(p,a)(a,s)(s,s)(s,e)(e,n)(n,g)(g,e)', '(t,h)(h,e)(e, )( ,n)(n,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)', '(t,o)(o,o)(o,k)(k, )( ,p)(p,l)(l,a)(a,c)(c,e)(e, )( ,d)', '(t,h)(h,e)(e,r)(r, )( ,w)(w,e)(e,l)(l,l)(l, )( ,k)(k,n)', '(s,e)(e,v)(v,e)(e,n)(n, )( ,s)(s,i)(i,x)(x, )( ,s)(s,e)', '(i,t)(t,h)(h, )( ,a)(a, )( ,g)(g,l)(l,o)(o,s)(s,s)(s, )', '(r,o)(o,b)(b,a)(a,b)(b,l)(l,y)(y, )( ,b)(b,e)(e,e)(e,n)', '(t,o)(o, )( ,r)(r,e)(e,c)(c,o)(o,g)(g,n)(n,i)(i,z)(z,e)', '(c,e)(e,i)(i,v)(v,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)', '(i,c)(c,a)(a,n)(n,t)(t, )( ,t)(t,h)(h,a)(a,n)(n, )( ,i)', '(r,i)(i,t)(t,i)(i,c)(c, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)', '(i,g)(g,h)(h,t)(t, )( ,i)(i,n)(n, )( ,s)(s,i)(i,g)(g,n)', '(s, )( ,u)(u,n)(n,c)(c,a)(a,u)(u,s)(s,e)(e,d)(d, )( ,c)', '( ,l)(l,o)(o,s)(s,t)(t, )( ,a)(a,s)(s, )( ,i)(i,n)(n, )', '(c,e)(e,l)(l,l)(l,u)(u,l)(l,a)(a,r)(r, )( ,i)(i,c)(c,e)', '(e, )( ,s)(s,i)(i,z)(z,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)', '( ,h)(h,i)(i,m)(m, )( ,a)(a, )( ,s)(s,t)(t,i)(i,c)(c,k)', '(d,r)(r,u)(u,g)(g,s)(s, )( ,c)(c,o)(o,n)(n,f)(f,u)(u,s)', '( ,t)(t,a)(a,k)(k,e)(e, )( ,t)(t,o)(o, )( ,c)(c,o)(o,m)', '( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,i)(i,e)(e,s)(s,t)(t, )', '(i,m)(m, )( ,t)(t,o)(o, )( ,n)(n,a)(a,m)(m,e)(e, )( ,i)', '(d, )( ,b)(b,a)(a,r)(r,r)(r,e)(e,d)(d, )( ,a)(a,t)(t,t)', '(s,t)(t,a)(a,n)(n,d)(d,a)(a,r)(r,d)(d, )( ,f)(f,o)(o,r)', '( ,s)(s,u)(u,c)(c,h)(h, )( ,a)(a,s)(s, )( ,e)(e,s)(s,o)', '(z,e)(e, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)', '(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,r)(r,i)', '(d, )( ,h)(h,i)(i,v)(v,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )', '(y, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,m)(m,a)(a,r)(r,c)', '(t,h)(h,e)(e, )( ,l)(l,e)(e,a)(a,d)(d, )( ,c)(c,h)(h,a)', '(e,s)(s, )( ,c)(c,l)(l,a)(a,s)(s,s)(s,i)(i,c)(c,a)(a,l)', '(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,n)(n,o)(o,n)(n, )( ,g)', '(a,l)(l, )( ,a)(a,n)(n,a)(a,l)(l,y)(y,s)(s,i)(i,s)(s, )', '(m,o)(o,r)(r,m)(m,o)(o,n)(n,s)(s, )( ,b)(b,e)(e,l)(l,i)', '(t, )( ,o)(o,r)(r, )( ,a)(a,t)(t, )( ,l)(l,e)(e,a)(a,s)', '( ,d)(d,i)(i,s)(s,a)(a,g)(g,r)(r,e)(e,e)(e,d)(d, )( ,u)', '(i,n)(n,g)(g, )( ,s)(s,y)(y,s)(s,t)(t,e)(e,m)(m, )( ,e)', '(b,t)(t,y)(y,p)(p,e)(e,s)(s, )( ,b)(b,a)(a,s)(s,e)(e,d)', '(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,t)(t,h)(h,e)', '(r, )( ,c)(c,o)(o,m)(m,m)(m,i)(i,s)(s,s)(s,i)(i,o)(o,n)', '(e,s)(s,s)(s, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)', '(n,u)(u,x)(x, )( ,s)(s,u)(u,s)(s,e)(e, )( ,l)(l,i)(i,n)', '( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,d)', '(z,i)(i, )( ,c)(c,o)(o,n)(n,c)(c,e)(e,n)(n,t)(t,r)(r,a)', '( ,s)(s,o)(o,c)(c,i)(i,e)(e,t)(t,y)(y, )( ,n)(n,e)(e,h)', '(e,l)(l,a)(a,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,s)(s,t)', '(e,t)(t,w)(w,o)(o,r)(r,k)(k,s)(s, )( ,s)(s,h)(h,a)(a,r)', '(o,r)(r, )( ,h)(h,i)(i,r)(r,o)(o,h)(h,i)(i,t)(t,o)(o, )', '(l,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,i)(i,n)(n,i)(i,t)', '(n, )( ,m)(m,o)(o,s)(s,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)', '(i,s)(s,k)(k,e)(e,r)(r,d)(d,o)(o,o)(o, )( ,r)(r,i)(i,c)', '(i,c)(c, )( ,o)(o,v)(v,e)(e,r)(r,v)(v,i)(i,e)(e,w)(w, )', '(a,i)(i,r)(r, )( ,c)(c,o)(o,m)(m,p)(p,o)(o,n)(n,e)(e,n)', '(o,m)(m, )( ,a)(a,c)(c,n)(n,m)(m, )( ,a)(a,c)(c,c)(c,r)', '( ,c)(c,e)(e,n)(n,t)(t,e)(e,r)(r,l)(l,i)(i,n)(n,e)(e, )', '(e, )( ,t)(t,h)(h,a)(a,n)(n, )( ,a)(a,n)(n,y)(y, )( ,o)', '(d,e)(e,v)(v,o)(o,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,b)', '(d,e)(e, )( ,s)(s,u)(u,c)(c,h)(h, )( ,d)(d,e)(e,v)(v,i)']\n",
      "['( ,a)(a,n)']\n",
      "['(a,n)(n,a)']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, bigram_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      batch[b, char2id(first_char) * vocabulary_size + char2id(second_char)] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return ['({0},{1})'.format(id2char(c//vocabulary_size), id2char(c % vocabulary_size))\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_first_characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c//vocabulary_size)\n",
    "          for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "  return s\n",
    "\n",
    "bigram_train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_valid_batches = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "# output each bigram instead of a single char\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_train_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n",
    "print(bigram_batches2string(bigram_valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,bigram_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, bigram_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, bigram_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def bigram_random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, bigram_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 6.593359 learning rate: 10.000000\n",
      "Minibatch perplexity: 730.23\n",
      "================================================================================\n",
      "bigrams: (w,g)(m,e)(s,j)(o,j)(q,z)(j,t)(f,w)(w,s)(g,y)(e,b)(l,f)( ,y)(q,a)(k, )(o,x)(p,n)(j,r)(p,l)(s,o)(d,y)(t,m)(z,w)(q,v)(c,o)(f,y)(i,v)(k,n)(x,v)(f,b)(w,u)(p,a)(x,i)(n,j)(x,r)(j,x)(w,r)(p,f)(t,u)(j,z)(g,e)(a,f)(k,r)(k,k)(o,n)(u,o)(x,f)(n,j)(e,y)(h,w)(x,j)(v,q)(g,d)(a,v)(l, )(g,q)(w,d)(a,a)(u,o)(p,g)(b,w)(s,p)(h,t)(u,o)(s,x)(s,d)(k,z)(l,z)(s,d)(w,d)(u,d)(u,m)(w,u)( ,c)(b,a)(c,q)(i,q)(e,b)(c,r)(o,o)(b,l)\n",
      "chars: wmsoqjfwgel qkopjpsdtzqcfikxfwpxnxjwptjgakkouxnehxvgalgwaupbshussklswuuw bciecob\n",
      "bigrams: (t,k)(i,y)(k, )(s,c)(m,w)(t,d)(u,w)(z,d)(v,r)(g,t)(c,i)(k,c)(n,c)(e,i)(u,u)(k,o)(e,g)(u,k)(x,n)(q,h)(m,m)(g,y)(n,e)(l,r)(f,q)(j,b)(b,v)(e,h)(p,i)(w,v)(e,x)(h,v)(q,w)(p,p)( ,f)(n,q)(x,f)(t,a)(s,v)(f,q)(j,m)(s,o)(k,x)(e,a)(x,y)(c,x)(e,o)( ,h)(e,z)(o,b)(r,i)(w,h)(q,g)(e,f)(s,r)(c,o)(x,a)(h,m)(q,c)( ,c)(c,o)(j,k)(v,m)(z,j)(j,o)(r,x)(e,x)(u,j)(m,a)(e,u)(n,l)(y,o)(a,h)(x,m)(j,m)(w,d)(y,s)(n,g)(v,y)(c,b)\n",
      "chars: tiksmtuzvgckneukeuxqmgnlfjbepwehqp nxtsfjskexce eorwqescxhq cjvzjreumenyaxjwynvc\n",
      "bigrams: (w,e)(f,g)(w,y)(s,r)(s,f)(o,t)(v,v)(d,a)(p,u)(j,z)(j,i)(f,a)(p,c)(c,k)(h,n)(w,b)(f,x)(c,d)(w,u)(n,u)(d,t)(e,s)(e,j)(j,q)(o,w)(w,k)(w,c)(o,a)(u,m)(z,h)(b,f)(i,o)(i,k)(q,p)(s,m)(j,c)(b,u)(s,h)(m,d)(m,f)(w,j)(k,g)(j,j)(m,m)(z,k)(g,g)(u,y)(j,b)(f,y)(m,k)(v,l)(s,q)(f,y)(w,q)(x,x)(a, )(p,l)(y,c)(z,p)(w,t)(w,w)(n,u)(q,i)(i,i)(a,y)(k,z)(n,l)(n,q)(q,u)(p,k)(u,z)(b,n)(m,z)(a,e)(p,e)(x,e)(i,t)(y,e)(i,i)(i,z)\n",
      "chars: wfwssovdpjjfpchwfcwndeejowwouzbiiqsjbsmmwkjmzgujfmvsfwxapyzwwnqiaknnqpubmapxiyii\n",
      "bigrams: (y,i)(h,h)(h,t)(o,l)(w,v)(i,h)(r,r)(z,k)(g,i)(b,y)(d,m)(p,i)(g,b)( ,n)(d,m)(s,y)(d,q)(g,b)(v,u)(n,g)(g,y)(n,q)(g,s)(c,p)(w,m)(g,u)(l,i)(s,k)(y,q)(v,f)(v,p)(i,v)(x,i)(y,x)(c,j)(v,l)(k,o)(m,w)(b,d)(z,q)(u,m)(a,b)(c,o)(u,k)(z,n)(w,d)(g,d)(s,h)(w,f)(v,d)(d,r)(w, )(u,i)(g,n)(w,y)(g,p)(a,z)(c,v)(i,x)(n,k)(o,w)(x,f)(c,z)(b,n)(p,f)(p,i)(x,f)( ,n)( ,r)(p,g)(k,b)(a,p)(r,g)(r,k)( ,d)(a,v)(p,s)(c,b)( ,p)(n,x)\n",
      "chars: yhhowirzgbdpg dsdgvngngcwglsyvvixycvkmbzuacuzwgswvdwugwgacinoxcbppx  pkarr apc n\n",
      "bigrams: (f,k)(r,w)(f,e)(z,i)(r,a)(c,q)(e,x)(m,f)(l,j)(f,l)(q,d)(y,u)(v,s)(v,w)(a,b)(k,a)(d,u)(x,p)(c,b)(p,f)(a,p)(t,k)(x,w)(v,m)(f,d)(q,n)(x,m)(m,b)(i,s)(r,d)(t,n)(r,p)(g,h)(m,y)(z,w)(y,z)(l,t)(m,s)(m,t)(o,f)(i,m)(e,u)(b, )(h,n)(b,n)(g,i)(x,k)(r,j)(l,x)(y,p)(u,k)(c,e)(z,c)(f,m)(r,s)(q,a)(t,r)(v,x)(f,s)(t,l)(n,h)(n,z)(y,f)(y,p)(g,y)(f,c)(y,g)(d,h)( ,p)(n,f)(l,a)(z,d)(y,b)(w,q)(n,n)(x,l)(s,m)( ,d)(w,a)(o,i)\n",
      "chars: frfzrcemlfqyvvakdxcpatxvfqxmirtrgmzylmmoiebhbgxrlyuczfrqtvftnnyygfyd nlzywnxs wo\n",
      "================================================================================\n",
      "Validation set perplexity: 676.22\n",
      "Average loss at step 100: 5.369860 learning rate: 10.000000\n",
      "Minibatch perplexity: 146.89\n",
      "Validation set perplexity: 125.80\n",
      "Average loss at step 200: 4.186529 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.94\n",
      "Validation set perplexity: 28.41\n",
      "Average loss at step 300: 2.921814 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.41\n",
      "Validation set perplexity: 12.70\n",
      "Average loss at step 400: 2.339319 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.57\n",
      "Validation set perplexity: 9.28\n",
      "Average loss at step 500: 2.135099 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 600: 2.037096 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 700: 1.939534 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.85\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 800: 1.870026 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 900: 1.868272 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1000: 1.853462 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "bigrams: (l,t)(t,i)(i,l)(l, )( ,v)(t,t)(t, )( ,s)(s,e)(e,i)(i,n)(n,g)(g, )( ,i)(i,m)(m, )( ,t)(t,e)(e,x)(x,t)(t, )( ,p)(u,n)(n,i)(i,t)(t,e)(e,d)(d, )( ,a)(a, )( ,d)(d,e)(e,n)(n,g)(g,e)(e,d)(d, )( ,h)(h,a)(a,y)(y, )( ,t)(t,h)(h,e)(e, )( ,o)(o,v)(v,e)(e,r)(r, )( ,a)(a,s)(s, )( ,a)(a,n)(n,d)(d, )( ,h)(h,i)(i,s)(s, )( ,o)(o,f)(f, )( ,k)(k,a)(a,v)(v,a)(a,c)(c,t)(t,o)(o,r)(r,i)(i,c)(c,t)(t,u)(u,r)(r,e)(e, )( ,r)\n",
      "chars: ltil tt seing im text united a denged hay the over as and his of kavactoricture \n",
      "bigrams: (s,s)(s, )( ,d)(d,i)(i,s)(f,o)(o,l)(l, )( ,s)(s, )( ,a)(a,n)(n,d)(d, )( ,o)(o,f)(f, )( ,t)(t,w)(w,o)(o, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,m)(m,o)(o,u)(u,n)(n,d)(d,e)(e,r)(r, )( ,d)(d,e)(e,a)(a,d)(h,d)(n,s)(s, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e,y)(y, )( ,b)(b,e)(e,n)(n,s)(s,i)(i,s)(s,h)(h, )( ,i)(i,l)(l,l)(l, )( ,a)(a, )( ,c)(c,o)(o,b)(t,h)(h,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,c)(c,h)\n",
      "chars: ss difol s and of two seven mounder deahns and they bensish ill a cothation of c\n",
      "bigrams: (a,r)(r,t)(t, )( ,g)(g,a)(a,l)(l,f)(f, )( ,b)(b, )(k,l)(d,o)(o,r)(r,t)(t,s)(s,k)(k,i)(i,n)(n,a)(a,l)(l,i)(i,a)(a,r)(r,i)(i,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,n)(n,o)(o, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,l)(l, )( ,p)(p,r)(r,o)(o,d)(d,u)(u,s)(s,e)(e,d)(d, )( ,c)(c,e)(e,f)(f, )( ,a)(a,s)(s, )( ,h)(h,a)(a,v)(v,e)(e, )( ,c)(c,e)(e,n)(r,v)(v,e)(e,s)(s,s)(s, )( ,t)(t,h)(h,e)(e, )( ,r)(r,i)(i,g)(g,h)\n",
      "chars: art galf bkdortskinaliaritions no the sevel prodused cef as have cervess the rig\n",
      "bigrams: (n,n)(n,e)(e,r)(r,i)(i,c)(c,e)(e, )( ,d)(d,i)(i,t)(t,a)(a,l)(l, )( ,i)(i,n)(n, )( ,c)(c,l)(p,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,c)(c,h)(h,i)(i,l)(l,e)(e, )( ,o)(o,f)(f,f)(m,u)(u,e)(e,o)(o, )( ,f)(f,o)(o,u)(u,r)(r, )( ,k)(k,e)(e, )( ,f)(f,u)(u,s)(s,i)(i,l)(l,o)(o,g)(g,u)(u,a)(a,l)(l, )( ,f)(f,r)(r,e)(x, )( ,c)(c,h)(h,a)(a,v)(v,e)(e, )( ,t)(t,h)(h,a)(a,t)(t, )( ,b)(b,a)(a,t)(t, )( ,i)\n",
      "chars: nnerice dital in cpection of chile ofmueo four ke fusilogual frx chave that bat \n",
      "bigrams: (a,v)(n,k)(k,i)(i,n)(n,g)(g, )( ,d)(d,r)(r,a)(a,p)(p,i)(i,n)(n,i)(i,a)(a,l)(l,o)(n,j)(g,u)(u,a)(a,t)(t,h)(h,e)(e,d)(d, )( ,w)(y,e)(e,l)(l,l)(l, )( ,a)(a,u)(u,t)(t,e)(e,r)(r,e)(e, )( ,s)(s,t)(t,a)(a,l)(l,l)(l,e)(e,t)(t, )( ,b)(b,y)(y, )( ,a)(a,r)(r,e)(e,a)(a,n)(n, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,i)(i,s)(s, )( ,a)(a, )( ,s)(s,l)(l,e)(e,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r, )( ,c)(c,r)\n",
      "chars: anking drapinialnguathed yell autere stallet by arean and this a sleed the for c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1100: 1.792217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1200: 1.753671 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1300: 1.729581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1400: 1.737303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 1500: 1.720547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1600: 1.734698 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1700: 1.695204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1800: 1.656470 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 1900: 1.622428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2000: 1.671755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "================================================================================\n",
      "bigrams: (w,v)(h,i)(i,a)(a,l)(l, )( ,a)(a,f)(f,t)(t,e)(e,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,r)(r,a)(a,m)(m,e)(e,s)(s, )( ,d)(d,u)(u,r)(r,i)(i,n)(n,g)(g, )( ,t)(t,w)(w,o)(o, )( ,z)(z,o)(o,u)(u,r)(r,l)(l,s)(s, )( ,a)(a,n)(n,d)(d, )( ,e)(e,v)(v,e)(e,n)(n, )( ,b)(b,o)(o,t)(t,h)(h, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,i)(i,n)(n,f)(f,i)(i,n)(n,i)(i,t)(t,y)(y, )( ,g)(g,o)(o,l)(l,i)(i,b)(b, )( ,d)(d,i)(i,v)(v,i)(i,l)\n",
      "chars: whial aften the grames during two zourls and even both state infinity golib divi\n",
      "bigrams: (l,s)(s, )( ,m)(m,a)(a,d)(d,e)(e,s)(s,u)(u,b)(p,a)(a,l)(l,i)(i,g)(g, )( ,t)(t,h)(h,e)(e, )( ,t)(t,i)(i,m)(m,e)(e, )( ,v)(v,i)(i,e)(e,s)(s,t)(t, )( ,e)(e,n)(n,d)(d,i)(i,t)(t,e)(e,s)(s, )( ,i)(i,n)(n, )( ,m)(m,a)(a,k)(k,e)(e,l)(l,l)(l,y)(y, )( ,e)(e,c)(c,o)(o,n)(n,t)(t,a)(a,r)(r, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,i)(i,n)(n,e)(e,d)(d, )( ,o)(o,n)(n, )( ,a)(a, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,w)(w,e)(e,b)\n",
      "chars: ls madesupalig the time viest endites in makelly econtar countined on a state we\n",
      "bigrams: (b,w)(o,i)(i,n)(n,g)(g, )( ,f)(f,o)(o,r)(r, )( ,e)(e,c)(c,u)(u,n)(n,g)(g,l)(l,i)(i,t)(b,s)(s, )( ,c)(c,o)(n,g)(x, )( ,o)(o,n)(n, )( ,w)(w,e)(e,n)(n,t)(t,h)(h, )( ,e)(e,l)(l,t)(t,h)(h,r)(r,e)(e,e)(e,l)(l,d)(d, )( ,o)(o,n)(n, )( ,h)(h,o)(o,p)(p,r)(r,e)(e,s)(s, )( ,l)(l,o)(o,c)(c,a)(a,l)(l,l)(l,e)(e,n)(n,t)(t, )( ,a)(a, )( ,i)(i,t)(t, )( ,r)(r,o)(o,u)(u,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,s)(s,t)(t,r)(o,p)\n",
      "chars: boing for ecunglibs cnx on wenth elthreeld on hopres locallent a it rounding sto\n",
      "bigrams: (g,y)(y,i)(i,n)(n,g)(g, )( ,t)(t,o)(o, )( ,b)(b,e)(e, )( ,p)(p,o)(o,p)(p,e)(e,r)(r,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,p)(p,l)(l,a)(a,t)(t,e)(e,r)(r, )( ,h)(h,e)(e,l)(l,l)(l,o)(o,p)(p,t)(t, )( ,m)(m,a)(a,y)(y, )( ,w)(w,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,w)(w,e)(e,b)(b,s)(s,o)(o,g)(g,r)(r,e)(e,f)(f, )( ,l)(l,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,k)(k, )( ,b)(b,e)(e, )( ,t)(t,r)(r,e)(e,s)(s,t)(t,e)(e,r)(r, )\n",
      "chars: gying to be poperion of plater hellopt may was the websogref leight k be trester\n",
      "bigrams: (j,c)(f,u)(u,t)(t,e)(e,d)(d, )( ,g)(g, )( ,a)(a,m)(m,e)(e,r)(r,o)(o,n)(n, )( ,t)(t,o)(u, )( ,b)(b,u)(u,i)(i,c)(c,k)(k,n)(n,e)(e,t)(t, )( ,i)(i,s)(s, )( ,a)(a, )( ,l)(l,e)(e,a)(a,s)(s,t)(t,t)(t,e)(e, )( ,u)(u,n)(n,i)(i,t)(t,e)(e,r)(r,e)(e,s)(s, )( ,u)(u,n)(n,d)(d,e)(e,r)(r,d)(d, )( ,t)(t,h)(h,a)(a,t)(t, )( ,t)(t,h)(h,r)(r,e)(e,e)(e,s)(s, )( ,a)(a,r)(r,e)(e, )( ,c)(c,o)(o,u)(u,r)(r,v)(v,i)(i,t)(t,i)\n",
      "chars: jfuted g ameron tu buicknet is a leastte uniteres underd that threes are courvit\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2100: 1.652339 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2200: 1.644758 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2300: 1.604581 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2400: 1.620805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2500: 1.644595 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2600: 1.616483 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 2700: 1.621946 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2800: 1.612808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 2900: 1.607879 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3000: 1.614979 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "bigrams: (m,i)(i,f)(f,i)(i,c)(c,s)(s, )( ,a)(a,n)(n,d)(d, )( ,a)(a, )( ,h)(h,u)(d, )( ,r)(r,e)(e,a)(a,d)(d, )( ,i)(i,s)(s, )( ,f)(f,e)(e,e)(e,g)(g,r)(r,e)(e,d)(d, )( ,b)(b,y)(y, )( ,a)(a, )( ,g)(g,a)(a,g)(g,s)(s, )( ,l)(l,e)(e,y)(y,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,r)(r,i)(u,x)(u,s)(s, )( ,s)(s,u)(u,b)(b,j)(j,e)(e,c)(c,t)(t, )( ,b)(b,e)(e,c)(c,o)(o,m)(m,e)(e, )( ,a)(a,l)(l,i)(i,t)(t,h)(h,e)(e,d)(d,u)(u,p)\n",
      "chars: mifics and a hd read is feegred by a gags leyations ruus subject become alithedu\n",
      "bigrams: (s,n)(n,e)(e,n)(n,t)(t, )( ,o)(o,f)(f, )( ,f)(f,i)(i,r)(r,s)(s,e)(e,v)(q,u)(w,s)(s,f)(f,t)(t, )( ,p)(p,e)(e,o)(o,p)(p,l)(l,a)(a,r)(r,s)(s, )( ,n)(n,o)(o,r)(r, )( ,a)(a,g)(g,a)(a,m)(m,s)(s, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,f)(f,i)(i,l)(l,m)(m, )( ,b)(b,a)(a,y)(y,s)(s, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,p)(p,a)(a,c)(c,h)(h, )( ,s)(s,u)(u,p)(p,i)(i,a)(a,n)(n,n)(n,s)(s,a)(a,n)(n,c)\n",
      "chars: snent of firseqwsft peoplars nor agams offiction film bays which pach supiannsan\n",
      "bigrams: (i,p)(p,t)(t, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,n)(n,d)(d, )( ,i)(i,m)(m,p)(p,r)(r,e)(e,s)(s,s)(s,e)(e,s)(s,t)(t, )( ,o)(o,f)(f, )( ,i)(i,t)(t, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,g)(g,h)(h, )( ,b)(b,o)(o,w)(w, )( ,i)(i,n)(n,e)(e,m)(m,p)(p,l)(l,y)(y, )(s,r)(p,l)(l,a)(a,y)(y, )( ,c)(c,o)(o,l)(l,l)(l,a)(a,w)(w,i)(i,n)(n,t)(t, )( ,f)(f,a)(a,m)(m,e)(e,n)(n, )( ,m)(m,o)(o,r)(r,i)(i,e)\n",
      "chars: ipt succend impressest of it which through bow inemplysplay collawint famen mori\n",
      "bigrams: (q,v)(o,k)(k,e)(e, )( ,t)(t,h)(h,e)(e, )( ,m)(m,e)(e,t)(t,r)(r,o)(o,r)(r, )( ,c)(c,l)(l,a)(a,c)(c,h)(h,a)(a,n)(n,g)(m,s)(s, )( ,f)(f,o)(o,l)(l,l)(l,o)(o,w)(w,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,p)(p,a)(a,r)(r,t)(t,i)(i,c)(c,a)(a,l)(l,l)(l,y)(y, )( ,o)(o,f)(f, )( ,i)(i,r)(r,v)(v,i)(i,s)(s,h)(h,e)(e,r)(r, )( ,t)(t,r)(r,a)(a,d)(d,i)(i,a)(a, )( ,a)(a,r)(r,g)(g,a)(a,n)(n,i)(i,c)(c, )( ,t)(t,o)(o, )\n",
      "chars: qoke the metror clachanms following the partically of irvisher tradia arganic to\n",
      "bigrams: (t,g)(y,r)(r,a)(a,n)(n,d)(d,a)(a,c)(c,t)(t,y)(y, )( ,x)(d,d)(d,a)(a, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,h)(h,i)(i,s)(s, )( ,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,t)(t,i)(i,m)(m,e)(e,s)(s, )( ,a)(a, )( ,d)(d,e)(e,f)(f,o)(o,r)(r,m)(m,e)(e,d)(d, )( ,b)(b,y)(y, )( ,s)(s,k)(k, )( ,o)(o,n)(n, )( ,e)(e,m)(m,p)(p,e)(e,r)(r,o)(o,r)(r, )( ,o)(o,r)(r,a)(a,l)(l,l)(l,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,i)\n",
      "chars: tyrandacty dda zero this at the times a deformed by sk on emperor orallation of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3100: 1.590944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3200: 1.604440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3300: 1.594060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3400: 1.626225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3500: 1.611918 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3600: 1.624875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 3700: 1.603119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 3800: 1.594450 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 3900: 1.587327 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 4000: 1.604890 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "================================================================================\n",
      "bigrams: (u,d)(d,e)(e,s)(s,t)(t, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e,y)(y, )( ,p)(p,i)(i,c)(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,t)(t,r)(r,o)(o,l)(l,e)(e,s)(s, )( ,n)(n,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,c)(c,a)(a,m)(m,e)(e, )( ,i)(i,n)(n,a)(a,u)(u,s)(s,i)(i,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)\n",
      "chars: udest to they pice the troles nine zero zero one one six came inausitions the pr\n",
      "bigrams: (k,z)(r,i)(i,n)(n,g)(g, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,s)(s,i)(i,x)(x, )( ,t)(t,h)(h,e)(e, )( ,i)(i,n)(n,c)(c,i)(i,t)(t,e)(e,s)(s, )( ,y)(y,e)(e,a)(a,r)(r, )( ,o)(o,f)(f, )( ,s)(s,e)(e,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,c)(c,a)(a,n)(n, )( ,m)(m,a)(a,s)(s,k)(k, )( ,t)(t,h)(h,e)(e, )( ,f)(f,o)(o,r)(r,c)(c,e)(e, )( ,o)(o,f)(f, )( ,v)(v,o)(o,u)(u,n)\n",
      "chars: kring in one nine four six the incites year of seating can mask the force of vou\n",
      "bigrams: (b,d)(y,w)(w,o)(o,p)(p,p)(p,h)(h,a)(a,n)(n,r)(r,y)(y, )( ,a)(a,c)(c,t)(t,i)(i,n)(n,f)(f,i)(i,e)(e,r)(r, )( ,i)(i,n)(n, )( ,m)(m,a)(a,n)(n,y)(y,a)(a,n)(n,s)(s, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,a)(a,s)(s,i)(i,t)(t,y)(y, )( ,h)(h,i)(i,s)(s, )( ,w)(w,o)(o,r)(r,l)(l,d)(d, )( ,w)(w,h)(h,e)(e,t)(t,b)(b,a)(a,l)(l, )( ,p)(p,l)(l,a)(a,y)(y, )( ,a)(a,n)(n,c)(c,i)(i,d)(d,u)(u,m)(m,e)(e,d)(d,e)(e, )( ,d)(d,e)\n",
      "chars: bywopphanry actinfier in manyans there asity his world whetbal play ancidumede d\n",
      "bigrams: (d,k)(x, )( ,f)(f,o)(o,r)(r, )( ,p)(p,r)(r,i)(i,n)(n,c)(c,e)(e,s)(s,s)(s,i)(i,o)(o,n)(n, )( ,e)(e,x)(x,a)(a,m)(m,p)(p, )( ,m)(m,a)(a,c)(c,r)(r,e)(e,m)(m,o)(o,n)(n, )( ,m)(m,o)(o,a)(a,d)(d,d)(d,i)(i,p)(p,h)(h,o)(o,o)(o,d)(d, )( ,t)(t,h)(h,e)(e, )( ,h)(h,o)(o,o)(o,d)(d,a)(a,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,d)(d,o)(o,m)(m,o)(o,t)(t,e)(e,m)(m,i)(i,n)(n,a)(a,s)(s,h)(h,l)(l,i)(i,e)(e,r)(r,a)\n",
      "chars: dx for princession examp macremon moaddiphood the hoodan of the domoteminashlier\n",
      "bigrams: (b,g)(h,y)(y, )( ,o)(o,n)(n, )( ,t)(t,h)(h,e)(e, )( ,t)(t,i)(i,m)(m,e)(e, )( ,t)(t,o)(o, )( ,c)(c,o)(o,n)(n,t)(t,a)(a,i)(i,n)(n, )( ,c)(c,a)(a,g)(y,c)(c,r)(r,a)(a,s)(s,e)(e, )( ,h)(h,i)(i,s)(s, )( ,a)(a,l)(l,s)(s,o)(o, )( ,d)(d,u)(u,b)(j,a)(i,e)(h, )( ,t)(t,h)(h,e)(e, )( ,k)(k, )( ,r)(r,o)(o,g)(g,e)(e,s)(s, )( ,s)(s, )( ,o)(o,t)(t,h)(h,e)(e,r)(r, )( ,a)(a,n)(n,d)(d,e)(e,n)(n,a)(a, )( ,q)(q,u)(u,e)\n",
      "chars: bhy on the time to contain caycrase his also dujih the k roges s other andena qu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 4100: 1.582378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4200: 1.582526 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 4300: 1.560977 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4400: 1.553029 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.561332 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4600: 1.559674 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4700: 1.567788 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4800: 1.573478 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4900: 1.566881 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5000: 1.547618 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "================================================================================\n",
      "bigrams: (x,r)(c,i)(i, )( ,i)(i,s)(s, )( ,b)(b,a)(a,l)(l,s)(s, )( ,i)(i,n)(n,t)(t,o)(o, )( ,e)(e,m)(m,a)(a,n)(n,c)(c,e)(e,s)(s, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,e)(e,x)(x,t)(t, )( ,i)(i,n)(n, )( ,r)(r,e)(e,c)(c,o)(o,r)(r,t)(t,e)(e,d)(d, )( ,o)(o,p)(p,e)(e,r)\n",
      "chars: xci is bals into emances one two eight one six zero one zero ext in recorted ope\n",
      "bigrams: (k,b)(k,b)(h,r)(r,a)(a,l)(l, )( ,v)(v,o)(o,i)(i,n)(n,t)(t,e)(e,t)(t,h)(h, )( ,w)(w,e)(e,r)(r,e)(e, )( ,a)(a,r)(r,t)(t,i)(i,c)(c, )( ,l)(l,e)(e,s)(s,p)(u,e)(e,l)(l,l)(l,i)(i,n)(n,g)(g, )( ,t)(t,o)(o,t)(t,i)(i,d)(d,u)(u,r)(r,e)(e,s)(s, )( ,s)(s,e)(e,e)(e,n)(n, )( ,e)(e,n)(n,d)(d, )( ,n)(n,e)(e,w)(w, )( ,t)(t,r)(r,a)(a,n)(n,s)(s,m)(m,y)(y, )( ,r)(r,e)(e,s)(s,p)(p,e)(e,e)(e,d)(d, )( ,d)(d,i)(i,s)(f,f)\n",
      "chars: kkhral vointeth were artic lesuelling totidures seen end new transmy respeed dif\n",
      "bigrams: (q,a)(f,a)(a, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,s)(s, )( ,b)(b,e)(e,e)(e,n)(n, )( ,h)(h,a)(a,s)(s, )( ,u)(u,s)(s,e)(e,a)(a, )( ,n)(n,o)(o,r)(r,c)(c, )( ,a)(a,c)(c,c)(c,u)(u,r)(r,d)(d, )( ,b)(b,y)(y,z)(z,e)(e,l)(l,i)(i,p)(p, )( ,t)(t,e)(e,l)(l,s)(s,o)(o, )( ,a)(a, )( ,d)(d,e)(e,c)(y,o)(o,i)(i,n)(n,e)(e, )( ,d)(d,e)(e,v)(v,e)(e,l)(l,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,n)(n,s)(s,o)(o,b)\n",
      "chars: qfa succes been has usea norc accurd byzelip telso a deyoine devely of the conso\n",
      "bigrams: (f,e)(e,l)(l,i)(i,t)(t, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,f)(f,o)(o,u)(u,r)(r, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,j)(j,a)(a,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)\n",
      "chars: felit in one seven one six four one three one seven jane five zero zero zero zer\n",
      "bigrams: (w,x)(u,k)(k,a)(a,m)(m,i)(i, )( ,s)(s,e)(e,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,w)(w,o)(o, )( ,f)(f,u)(u,t)(t,h)(h, )( ,y)(y,o)(o,u)(u,m)(m, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,s)(s, )( ,a)(a,r)(r,e)(e,b)(t,v)(v, )( ,i)(i,n)(n,t)(t,e)(e,l)(l,l)(l, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)\n",
      "chars: wukami see zero zero zero zero two futh youm one nine two s aretv intell one nin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5100: 1.537909 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5200: 1.518629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5300: 1.507804 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 5400: 1.507572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 5500: 1.496448 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 5600: 1.512497 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 5700: 1.500652 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5800: 1.513609 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 5900: 1.502834 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6000: 1.477578 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "bigrams: (p,i)(i,n)(n,g)(g, )( ,r)(r,e)(e,u)(u,s)(s,c)(c,a)(a,n)(n, )( ,o)(o,f)(f, )( ,j)(j,u)(u,s)(s,t)(t, )( ,l)(l,i)(i,n)(n,e)(e, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,r)(r,y)(y, )( ,c)(c,i)(i,r)(r,c)(c,e)(e,s)(s, )( ,l)(l,a)(a,r)(r,g)(g,e)(e,r)(r, )( ,o)(o,p)(p,p)(p,e)(e,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,s)(s,o)(o,m)(m,e)(e,t)(t,i)(i,m)(m,e)(e,s)(s, )( ,e)(e,x)(x,t)(t,e)(e,n)(n,d)(d,e)(e,n)(n,t)(t, )( ,i)(i,s)\n",
      "chars: ping reuscan of just line country circes larger opperation sometimes extendent i\n",
      "bigrams: (e,i)(i,n)(n,g)(g, )( ,t)(t,h)(h, )( ,a)(a,n)(n, )( ,a)(a,l)(l,l)(l, )( ,l)(l,e)(e,v)(v,e)(e,l)(l,t)(t, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,i)(i,o)(o,n)(n, )( ,f)(f,a)(a,i)(i,m)(m,s)(s, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,x)(d,r)(r,e)(e,t)(t,a)(a,l)(l, )( ,c)(c,o)(o,m)(m,p)(p,a)(a,s)(s,i)(i,l)(l,i)(i,t)(t,y)(y, )( ,f)(f,l)(l,a)(a,t)(t,e)(e,s)(s, )( ,i)(i,n)(n,t)(t,r)(r,o)(o,d)(d,u)(u,c)(c,e)(e,r)(r, )\n",
      "chars: eing th an all levelt interion faims the prodretal compasility flates introducer\n",
      "bigrams: (h,h)(w, )( ,n)(t,x)(s,l)(l,a)(a,d)(d,u)(u,a)(a,g)(g,e)(e, )( ,i)(i,n)(n, )( ,s)(s,h)(h,o)(o,p)(p, )( ,n)(n,a)(a,t)(t,a)(a,b)(b,l)(l,e)(e, )( ,a)(a,m)(m,e)(e,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,a)(a, )( ,s)(s,p)(p,e)(e,c)(c,i)(i,a)(a,l)(l, )( ,d)(d,i)(i,v)(v,e)(e,r)(r, )( ,a)(a,t)(t, )( ,a)(a, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,r)(r,y)(y, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,s)(s,t)(t,a)(a,n)(n,t)(t, )( ,v)(v,i)(i,e)\n",
      "chars: hw tsladuage in shop natable american a special diver at a country interstant vi\n",
      "bigrams: (a,h)(h,r)(r,o)(o,v)(v,e)(e,l)(l,o)(o,w)(w,s)(s, )( ,c)(c,o)(o,l)(l,l)(l,e)(e,c)(c,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,l)(l,a)(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e, )( ,a)(a,n)(n,d)(d, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,r)(r,u)(u,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,c)(c,h)(h,a)(a,r)(r,a)(a,c)(c,t)(t,r)(r,o)(o,n)(n, )( ,t)(t,h)(h,u)(u,g)(g,h)(h,a)(a,r)(r,y)(y, )( ,w)(w,a)(a,l)(l,t)(t,e)(e,c)(c,r)(r,e)(e,d)\n",
      "chars: ahrovelows collectical language and construction of charactron thughary waltecre\n",
      "bigrams: (a,z)(z,a)(a,r)(r,b)(b,l)(l,e)(e, )( ,g)(g,o)(o,o)(o,d)(d,e)(e, )( ,a)(a,l)(l,s)(s,o)(o, )( ,a)(a,l)(l,l)(l, )( ,m)(m,u)(u,n)(n,y)(y, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,s)(s, )( ,c)(c,a)(a,p)(p,r)(r,e)(e,s)(s,s)(s,e)(e,n)(n,i)(i,c)(c,y)(y, )( ,b)(b,l)(l,a)(a,c)(c,k)(k, )( ,o)(o,n)(n,e)(e, )( ,n)(n,a)(a,m)(m,e)(e, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,g)(g,h)(h, )( ,c)(c,o)(o,n)(n,c)(c,e)(e,n)(n,t)(t,a)\n",
      "chars: azarble goode also all muny which is capressenicy black one name through concent\n",
      "================================================================================\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6100: 1.502075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6200: 1.470539 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6300: 1.473956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6400: 1.480335 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6500: 1.491722 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6600: 1.530537 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6700: 1.512153 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6800: 1.540155 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6900: 1.513732 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.03\n",
      "Average loss at step 7000: 1.507373 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "bigrams: (j,i)(i,n)(n,g)(g, )( ,a)(a,n)(n,d)(d, )( ,d)(d,u)(u,b)(b,l)(l,i)(i,n)(n, )( ,i)(i,n)(n,v)(v,e)(e,n)(n,o)(o,t)(t,o)(o,g)(g,r)(r,a)(a,n)(n, )( ,m)(m,a)(a,t)(t,e)(e,s)(s, )( ,c)(c,o)(o,m)(m,p)(p,u)(u,t)(t,e)(e,r)(r, )( ,a)(a,n)(n,d)(d, )( ,s)(s,u)(u,b)(b,s)(s, )( ,a)(a,r)(r,m)(m,y)(y, )( ,k)(k,a)(a,r)(r, )( ,w)(w,a)(a,l)(l,a)(a,c)(c,y)(y, )( ,g)(g,e)(e,r)(r,m)(m,a)(a,n)(n,g)(g,a)(a,t)(t,i)(i,o)(o,n)\n",
      "chars: jing and dublin invenotogran mates computer and subs army kar walacy germangatio\n",
      "bigrams: (p,n)(n, )( ,w)(w,a)(a,y)(y, )( ,s)(s,c)(c,r)(r,i)(i,b)(b,b)(b,i)(i,n)(n,g)(g, )( ,e)(e,v)(v,e)(e,l)(l,y)(y, )( ,m)(m,a)(a,k)(k,e)(e, )( ,a)(a,n)(n,d)(d, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,b)(b,e)(e,a)(a,s)(s,h)(h, )( ,t)(t,a)(a,k)(k,i)(i,n)(n,a)(a,t)(t,o)(o,r)(r, )( ,p)(p,o)(o,p)(p,u)(u,l)(l,a)(a,t)(t,i)(i,o)(o,n)(n,a)(a,l)(l, )( ,m)(m,o)(o,r)(r,e)(e, )( ,o)(o,r)(r,g)(g,a)(a,n)(n,i)(i,v)(v,i)(i,e)\n",
      "chars: pn way scribbing evely make and which beash takinator populational more organivi\n",
      "bigrams: (z,m)(e,l)(l, )( ,a)(a,l)(l,o)(o,n)(n,s)(s,a)(a,l)(l,m)(m,s)(s, )( ,a)(a,l)(l,f)(f, )( ,t)(t,h)(h,e)(e, )( ,n)(n,e)(e,w)(w,o)(o,r)(r,t)(t, )( ,a)(a,n)(n,d)(d, )( ,a)(a, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,h)(h,a)(a,t)(t,t)(t,l)(l,y)(y, )( ,a)(a,p)(u,g)(g,h)(h,t)(t,o)(o,p)(p,l)(l,e)(e, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e,y)(y, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,s)(s,c)(c,r)(r,e)(e,a)(a,t)(t,o)(o,r)(r, )\n",
      "chars: zel alonsalms alf the newort and a there hattly aughtople as they the rescreator\n",
      "bigrams: (h,n)(n,a)(a,b)(b,l)(l,e)(e, )( ,p)(p,r)(r,o)(o,b)(b,l)(l,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,f)(f,a)(a,d)(d,i)(i,o)(o, )( ,a)(a,t)(t, )( ,e)(e,m)(m,b)(b,e)(e,r)(r,t)(t, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,g)(g,h)(h, )( ,t)(t,h)(h,e)(e, )( ,c)(c,h)(h,r)(r,i)(i,s)(s,t)(t,i)(i,a)(a,h)(h, )( ,a)(a,n)(n,d)(d, )( ,d)(d,u)(u,r)(r,i)(i,n)(n,g)(g, )( ,b)(b,r)(r,i)(i,t)(t,i)(i,s)(s,h)(h, )( ,b)(b,i)(i,s)\n",
      "chars: hnable probly of the fadio at embert through the christiah and during british bi\n",
      "bigrams: (c,o)(o,r)(r, )( ,e)(e,v)(v,i)(i,d)(d,e)(e,r)(r,s)(s, )( ,c)(c,o)(o,a)(a,s)(s,t)(t,i)(i,c)(c,i)(i,s)(s,f)(f,l)(l,y)(y, )( ,a)(a,l)(l,l)(l, )( ,o)(o,r)(r, )( ,a)(a, )( ,b)(b,r)(r,a)(a,f)(f,t)(b,n)(n,e)(e,d)(d, )( ,t)(t,h)(h,i)(i,s)(s, )( ,t)(t,h)(h,e)(e,o)(o,r)(r,y)(y, )( ,o)(o,f)(f, )( ,d)(d,u)(u,f)(f,i)(i,e)(e,s)(s, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(s,i)(i,d)(d,e)(e,n)(n,i)(i,p)(p,a)(a,l)\n",
      "chars: cor eviders coasticisfly all or a brafbned this theory of dufies as the sidenipa\n",
      "================================================================================\n",
      "Validation set perplexity: 3.99\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_bigram(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))\n",
    "        \n",
    "%time exec_graph_bigram(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "# output embeddings (IDs) instead of one-hot-encoded matrices\n",
    "\n",
    "class BigramEmbeddingBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = list()\n",
    "    for b in range(self._batch_size):\n",
    "      first_char = self._text[self._cursor[b]]\n",
    "      if self._cursor[b] + 1 == self._text_size:\n",
    "        second_char = ' '\n",
    "      else:\n",
    "        second_char = self._text[self._cursor[b] + 1]\n",
    "      batch.append(char2id(first_char) * vocabulary_size + char2id(second_char))\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "bigram_embed_train_batches = BigramEmbeddingBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "bigram_embed_valid_batches = BigramEmbeddingBatchGenerator(valid_text, 1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    all_gates = tf.nn.embedding_lookup(ifcox, i) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_bigram_embed(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = bigram_embed_train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        # convert to one-hot-encodings\n",
    "        noembed_labels = np.zeros(predictions.shape)\n",
    "        for i, j in enumerate(labels):\n",
    "            noembed_labels[i, j] = 1.0\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, noembed_labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = bigram_sample(bigram_random_distribution())\n",
    "            bigram_sentence = bigram_characters(feed)[0]\n",
    "            sentence = bigram_first_characters(feed)[0]\n",
    "            # convert to embedding\n",
    "            feed = [np.argmax(feed)]\n",
    "            reset_sample_state.run()\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = bigram_sample(prediction)\n",
    "              bigram_sentence += bigram_characters(feed)[0]\n",
    "              sentence += bigram_first_characters(feed)[0]\n",
    "              feed = [np.argmax(feed)]\n",
    "            print('bigrams:', bigram_sentence)\n",
    "            print('chars:', sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        reset_sample_state.run()\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = bigram_embed_valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          labels = np.zeros((1, bigram_size))\n",
    "          labels[0, b[1]] = 1.0\n",
    "          valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591854 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.13\n",
      "================================================================================\n",
      "bigrams: (q,b)(b,v)(b,f)(p,s)(v,t)(v,b)( ,p)(r,n)(d,e)(z,i)(v,y)(e,e)(p,i)(h,h)(k, )(w, )(n,f)(f,o)(c,v)(w,t)(e,q)(b,q)(w,s)(y,s)(w,e)(s,p)(j,t)(d,f)(s,m)(p,x)(u,m)(k,t)(b,k)(e,d)(o,w)(e,e)(r,u)(j,t)(l,n)(h,a)( ,e)(j,n)(g,q)(k,y)(m,t)(g,e)(t,z)(i,v)(c,a)(q, )(c,c)( ,m)(i,t)(a,k)(q,u)(h,k)(z,c)(l,k)(x,q)(l,a)(p,r)(d,u)(y,q)(u,l)(j,u)(h,t)(k,g)(b,q)(b,t)(n,t)(b,u)(t,z)(y,a)(y, )(h,a)(x,a)(a,t)(x,y)(x,j)(h,v)\n",
      "chars: qbbpvv rdzvephkwnfcwebwywsjdspukbeoerjlh jgkmgticqc iaqhzlxlpdyujhkbbnbtyyhxaxxh\n",
      "bigrams: (n,u)( ,c)(a,b)(v,i)(i,l)(m,a)(u,j)(j,n)(e,w)(g,v)(s,x)(c,j)(n, )(r,i)(m,v)(f,a)(k,i)(r,r)(d,p)(r,n)(w,i)(c,r)(p,p)(e,n)(m,v)(t,n)(t,r)(s,t)(y,z)(h,o)(r,g)(a,a)(p,w)(w,m)(t,k)(k, )(g,n)(s,g)(m,d)(b,r)(z,c)(l,d)(p,u)(i,u)( ,a)(w,y)(n,g)(m,z)(j,c)(b,l)(t,n)(q,l)(a,y)(x,p)(f,m)(i,d)(l,j)(j,v)(x,g)(h,p)( ,t)(h,f)(q,x)(u,x)(h,s)(j,u)(t,z)(a,f)(i,l)(w,i)(f,r)(q,v)(z,d)(b,d)(l,d)(k,g)(d,m)(g,g)(n,q)(h,p)\n",
      "chars: n avimujegscnrmfkrdrwcpemttsyhrapwtkgsmbzlpi wnmjbtqaxfiljxh hquhjtaiwfqzblkdgnh\n",
      "bigrams: (k,w)(q,q)(t,a)(h, )(l,d)(i,r)(o,r)(i,a)(o,u)(u,o)(b,q)(x,b)(x,z)(y,q)(f,c)( ,o)(i,b)(g,k)(i,k)(y,g)(z,l)(e,v)(f,k)(h,k)(k,x)(j,b)(i,f)(x,o)(d,h)(o,w)(i,v)(w,v)(i,n)(b,o)(l,v)(c,o)(s,n)(e,l)(f,o)(i,c)(q,u)(z,x)(c,h)(a,j)(e,l)(l,f)(t,y)(s,f)(h,g)(v,i)(o,j)(q,s)(j,p)( ,a)(g,o)(o,c)(o,l)(h,k)(a,c)(b,l)(x,b)( ,z)(w,q)(l,c)(l,p)(a,t)(h,x)(u,o)(a,n)(b,r)(z,x)(s,e)(e,f)(k,e)(l,w)(z,u)(x,q)(q,n)(w,k)(b, )\n",
      "chars: kqthlioioubxxyf igiyzefhkjixdoiwiblcsefiqzcaeltshvoqj goohabx wllahuabzseklzxqwb\n",
      "bigrams: (q,a)(c,a)(w, )(i,q)(h,p)(k,v)(p,c)(o,e)(e,a)(p,k)(c,r)(g,u)(v,o)(q,f)(v,s)(x,o)(j,x)(p,f)(m,p)(p,w)(t, )(b,a)(l,j)(l,n)(f,r)( ,m)(e, )(o,g)(c,d)(w,g)(i,k)(m,u)(z, )( ,h)(j,y)( ,h)(m,n)(y,j)(i,y)(j,e)(e,q)(i,p)(j,t)( ,z)(b,w)(c,d)(v,m)(l,b)(w,s)(b,v)(b,n)(y,t)(n, )(f,n)(v,e)(f,s)(m,n)( ,z)(s,v)(y,g)(w,i)(e,s)(m,r)(m,d)(b,g)(y,p)(w,m)(w,i)(b,l)(c,n)(h,m)(o,n)(b,i)(n,x)(q,q)(r, )(g,v)(q,w)(p,b)(n,t)\n",
      "chars: qcwihkpoepcgvqvxjpmptbllf eocwimz j myijeij bcvlwbbynfvfm sywemmbywwbchobnqrgqpn\n",
      "bigrams: (n,j)(z,t)(t,u)(x,d)(c,v)(x,n)(t,c)(j,c)(z,n)(p,y)(r,d)(k,r)(p,a)( ,l)(o,s)(b,a)(t,h)(k,v)(p,c)(s,i)(j,m)(m,d)(t,x)(i,x)(j, )(c,z)(w,c)(h,b)(u,k)(x,a)(s,u)(g,b)(p,y)(z,s)(p,i)( , )(d,x)(m,c)(g,c)(j, )(h,z)(l,u)(n,a)(g,s)(g,s)(i,h)(a,o)(h,e)(v,k)(e,j)(n,t)(j,r)(u,a)(k,j)(x,z)(x,y)(c,c)(s,d)(s,x)(m,p)(e,d)(z,b)(p,k)(m,p)(v,z)(d,u)(s,q)(t,a)(p,f)(t,k)(u,i)(n,q)(y,l)(v,p)(n,j)( ,d)(t,l)(p,a)(m,r)(w,b)\n",
      "chars: nztxcxtjzprkp obtkpsjmtijcwhuxsgpzp dmgjhlnggiahvenjukxxcssmezpmvdstptunyvn tpmw\n",
      "================================================================================\n",
      "Validation set perplexity: 671.18\n",
      "Average loss at step 100: 5.363996 learning rate: 10.000000\n",
      "Minibatch perplexity: 110.81\n",
      "Validation set perplexity: 116.96\n",
      "Average loss at step 200: 4.029920 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.75\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 300: 2.855173 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.77\n",
      "Validation set perplexity: 12.37\n",
      "Average loss at step 400: 2.332866 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.89\n",
      "Validation set perplexity: 9.21\n",
      "Average loss at step 500: 2.141728 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.19\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 600: 2.041664 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 700: 1.944880 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "Validation set perplexity: 6.78\n",
      "Average loss at step 800: 1.879875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 6.72\n",
      "Average loss at step 900: 1.879287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.866618 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "bigrams: (l,a)(a,t)(t,y)(y, )( ,h)(h,e)(e, )( ,b)(b,e)(e, )( ,g)(g,o)(o,l)(l,t)(t,y)(y, )( ,w)(w,a)(a,s)(s, )( ,k)(k,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e,y)(y, )( ,c)(c,o)(o,n)(n,t)(t, )( ,f)(f,r)(r,o)(o, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,v)(p,p)(p,u)(u,l)(l,a)(a,p)(t,e)(e,d)(d, )( ,s)(s,o)(o, )( ,m)(m,o)(o,d)(d,e)(e,d)(d, )( ,c)(c,h)(h,i)(i,n)(n,c)(c,l)(l,e)(e, )( ,i)(i,n)(n, )( ,b)(b,e)(e,e)(e,t)(t,h)(h,e)\n",
      "chars: laty he be golty was king they cont fro five oppulated so moded chincle in beeth\n",
      "bigrams: (h,a)(a,l)(l, )( ,c)(c,a)(a,n)(n, )( ,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,s)(u,s)(s,e)(e,r)(r,v)(f,u)(u,r)(r,k)(m, )( ,a)(a,s)(s, )( ,a)(a,l)(l,s)(s,o)(o, )( ,d)(d,e)(e,c)(c,a)(a,m)(m,e)(e,n)(n,c)(c,e)(e, )( ,c)(c,o)(o,m)(m,m)(m,a)(a,l)(l, )( ,t)(t,h)(h,i)(i,s)(s, )( ,h)(h,o)(o,u)(u,p)( ,u)(u,s)(s, )( ,b)(b,e)(e,t)(t,w)(w,e)(e,a)(a, )( ,a)(a, )( ,d)(d,o)(e,c)(c,o)(o, )( ,f)(f,o)(o,r)(r,m)(m, )( ,a)\n",
      "chars: hal can as the userfurm as also decamence commal this hou us betwea a deco form \n",
      "bigrams: (x,z)(b,i)(i,n)(n,e)(e,d)(d, )( ,t)(t,e)(e,n)(n,i)(i,t)(t,s)(s, )( ,d)(d,i)(i,o)(o,k)(u,g)(i,e)(e,n)(n,f)(f,f)(f,a)(a,i)(i,n)(n,c)(c,e)(e, )( ,o)(o,f)(f, )( ,p)(p,r)(r,e)(e,s)(s,s)(s,e)(e,s)(s, )( ,f)(f,r)(r,o)(o,m)(m, )( ,p)(p,r)(r,o)(o,t)(t,h)(h,e)(e,d)(d, )( ,m)(m,o)(o,v)(w,n)(n, )( ,o)(o,f)(f, )( ,a)(a,n)(n,d)(d, )( ,i)(i,n)(n, )( ,s)(s,a)(a,d)(d,e)(e,n)(n,t)(t, )( ,d)(d,y)(y, )( ,f)(f,u)(u,l)\n",
      "chars: xbined tenits diouienffaince of presses from prothed mown of and in sadent dy fu\n",
      "bigrams: (a,s)(s,u)(u,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,o)(o,p)(p,t)(t,i)(i,b)(b,e)(e,r)(r, )( ,t)(t,o)(o, )( ,o)(o,u)(u,r)(r,t)(t,a)(a,r)(r,s)(s, )( ,a)(a,d)(d,d)(d,o)(o, )( ,t)(t,i)(i,c)(c,h)(h,e)(e,d)(d, )( ,a)(a,s)(s, )( ,g)(g,r)(r,u)(y,p)(s,s)(g,n)(c,a)(i,i)(e,l)(l,y)(y, )( ,a)(a,s)(s, )( ,c)(c,o)(o,m)(m,p)(p,u)(u,t)(t,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,v)(v,i)(n,e)(e,d)(d,r)(r,u)(u,l)\n",
      "chars: asued the optiber to ourtars addo tiched as grysgciely as computer the provnedru\n",
      "bigrams: (h,v)(u,z)(i,k)(k,n)(t,b)(u,r)(r,e)(e,a)(a,t)(t, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,s)(s,s)(s,i)(i, )( ,h)(h,e)(e, )( ,a)(a,p)(p,r)(r,e)(e,s)(s,s)(s, )( ,a)(a, )( ,f)(f,u)(u,r)(r,t)(t,h)(h,e)(e,r)(r, )( ,t)(t,r)(r,u)(u,r)(m,b)(b,u)(u,r)(r,s)(s,t)(t,a)(a,n)(n, )( ,r)(r,e)(e,w)(y,n)(n,e)(e,m)(m,p)(p,e)(e,t)(t,s)(s, )( ,a)(a,l)(l,t)(t,h)(h,e)(e,y)(y, )( ,g)(g,o)(o,d)(d, )( ,r)(r,e)(e,l)(l,o)(o,v)(v,i)\n",
      "chars: huiktureat the cossi he apress a further trumburstan reynempets althey god relov\n",
      "================================================================================\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1100: 1.802430 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1200: 1.766482 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1300: 1.741658 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1400: 1.749152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.737948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 1600: 1.743410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 1700: 1.706202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 1800: 1.665188 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 1900: 1.629425 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2000: 1.675810 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "bigrams: (p,g)(p,m)(m,a)(a,t)(t,e)(e,d)(d, )( ,f)(f,r)(r,o)(o,m)(m, )( ,k)(m,o)(o,r)(r,r)(r,o)(o,p)(p,e)(e,d)(d, )( ,w)(w,h)(h,o)(o,s)(s, )( ,w)(w,h)(h,e)(e,r)(r,e)(e,e)(e, )( ,e)(e,b)(b, )( ,i)(i,s)(s, )( ,a)(a, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,p)(p,y)(y, )( ,d)(d,a)(a,l)(l,l)(l,e)(e,a)(a, )( ,d)(d,o)(o,w)(w,n)(n, )( ,w)(w,e)(e,a)(a,v)(v,e)(e,d)(d, )( ,b)(b,y)(y, )( ,r)(r,e)(e,s)(s,c)(c,u)(u,d)(d,e)(e,s)\n",
      "chars: ppmated from morroped whos wheree eb is a which py dallea down weaved by rescude\n",
      "bigrams: (q,m)(m,c)(y,a)(a,r)(r,i)(i,e)(e,s)(s, )( ,b)(b,e)(e,l)(l,v)(v,e)(e,s)(s, )( ,s)(s,c)(c,r)(r,e)(e,e)(e,r)(r,o)(o,n)(n, )( ,h)(h,i)(i,n)(n,s)(s,t)(t,r)(r,e)(e,p)(p,o)(o,s)(s,e)(e, )( ,c)(c,h)(h,a)(a,r)(r,n)(n, )( ,w)(w,o)(o,r)(r,d)(d, )( ,b)(b,y)(y, )( ,t)(t,i)(i,m)(m,e)(e,s)(s,c)(c,r)(r,a)(a, )( ,i)(i,n)(n, )( ,s)(s,c)(c,h)(h,o)(o,r)(r,t)(t, )( ,l)(l,i)(i,n)(n,c)(k,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)\n",
      "chars: qmyaries belves screeron hinstrepose charn word by timescra in schort links in t\n",
      "bigrams: (w,o)(o,r)(r,s)(s, )( ,f)(f,a)(a,s)(s, )( ,f)(f,i)(i,r)(r,s)(s,e)(e,d)(d, )( ,p)(p,e)(e,a)(a,r)(r, )( ,l)(l,a)(a,s)(s,e)(e, )( ,i)(i,n)(n,s)(s,t)(t,r)(r,o)(o,c)(c,u)(u,r)(r,c)(c,u)(u,a)(a,g)(g,e)(e,s)(s, )( ,h)(h,o)(h,a)(a,r)(r,r)(r,i)(i,e)(e,s)(s, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,v)(v,e)(e,l)(l,o)(o,r)(r,y)(y, )( ,x)(t,a)(a,n)(n,y)(y, )( ,s)(s,o)(o,c)(c,i)(i,f)(f,f)(f, )( ,t)(t,h)(h,u)(o,u)\n",
      "chars: wors fas firsed pear lase instrocurcuages hharries in the velory tany sociff tho\n",
      "bigrams: (w,i)(i,s)(s,t)(t, )( ,o)(o,f)(f, )( ,h)(h,e)(e,a)(a,r)(r, )( ,f)(f,i)(i,n)(n,e)(e,s)(s,t)(t, )( ,e)(e,q)(q,u)(u,i)(i,l)(l,l)(l,e)(e,d)(d, )( ,w)(w,i)(i,n)(n,g)(g, )( ,f)(f,a)(a,c)(c,t)(t,u)(u,e)(e,n)(n, )( ,m)(m,i)(i,n)(n,e)(e,r)(r,s)(s, )( ,i)(i,n)(n, )( ,v)(v,i)(i,c)(c,e)(e,n)(n,s)(s,i)(i,b)(b,y)(y, )( ,n)(n,e)(e,w)(w, )(a,j)(j,o)(o,r)(r,g)(g,e)(e,s)(s, )( ,a)(a,s)(s, )( ,h)(h,e)(e,b)(r,p)(p,u)\n",
      "chars: wist of hear finest equilled wing factuen miners in vicensiby newajorges as herp\n",
      "bigrams: (x,t)(t,h)(h,e)(e,r)(r,e)(e,d)(d, )( ,t)(t,o)(o, )( ,j)(j,u)(u,n)(n,i)(i,f)(f,e)(e, )( ,m)(m,a)(a,d)(d, )( ,c)(c,y)(y, )( ,r)(e,n)(n,c)(c,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,m)(m,u)(u,r)(r,a)(a,p)(p,h)(h,o)(o, )( ,h)(h,i)(i,s)(s, )( ,h)(h,i)(i,s)(s, )( ,k)(k,i)(i,n)(n,g)(g, )( ,c)(c,a)(a,r)(r,y)(y, )( ,z)(z,a)(a,m)(m,p)(p,l)(l,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,h)(h,u)(u,n)\n",
      "chars: xthered to junife mad cy ences the nine murapho his his king cary zampled the hu\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.14\n",
      "Average loss at step 2100: 1.665814 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2200: 1.655207 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 2300: 1.618858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2400: 1.630107 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2500: 1.653713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2600: 1.624271 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 2700: 1.633090 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2800: 1.618003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 2900: 1.614258 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3000: 1.619851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "================================================================================\n",
      "bigrams: (k,s)(s, )( ,d)(d,o)(o, )( ,s)(s,t)(t,y)(y,l)(l,e)(e,s)(s, )( ,b)(b,e)(e,c)(c,a)(a,m)(m,e)(e, )( ,b)(b,e)(e,c)(c,a)(a,u)(u,s)(s,i)(i,n)(n,g)(g, )( ,u)(u,p)(p,p)(p,o)(o,s)(s,i)(i,s)(s,h)(h, )( ,r)(r,a)(a,m)(m, )( ,f)(f,o)(o,u)(u,r)(r, )( ,a)(a, )( ,d)(d,e)(e,v)(v,e)(e,l)(l,o)(o,p)(p,m)(m,e)(e,n)(n,t)(t, )( ,h)(h,u)(u,n)(n,d)(d,s)(s, )( ,q)(q,u)(u,a)(a,t)(t,e)(e,r)(r,e)(e,d)(d, )( ,b)(b,y)(y, )( ,i)\n",
      "chars: ks do styles became becausing upposish ram four a development hunds quatered by \n",
      "bigrams: (m,r)(r, )( ,t)(t, )( ,b)(b,r)(r,e)(e,l)(l,i)(i,e)( ,r)(r, )( ,s)(s,t)(t,a)(a,n)(n,c)(c,e)(e, )( ,m)(m,i)(i,f)(f,a)(a,n)(n, )( ,m)(m,e)(e,t)(t,t)(t,e)(e,r)(r, )( ,e)(e,n)(n,f)(f,o)(o,r)(r,c)(c,e)(e, )( ,t)(t,r)(r,a)(a,n)(n,s)(s,p)(p,e)(e,t)(t, )( ,a)(a, )( ,s)(s,h)(h,o)(o,k)(k, )( ,e)(e,f)(w, )( ,i)(i,n)(n,s)(s,t)(t,e)(e,r)(r, )( ,s)(s,i)(i,s)(s,t)(t,i)(i,n)(n, )( ,c)(c,o)(o,n)(n,f)(f,a)(a,c)(c,t)\n",
      "chars: mr t breli r stance mifan metter enforce transpet a shok ew inster sistin confac\n",
      "bigrams: (s,z)(x,i)(i,s)(s,t)(t,e)(e,l)(l, )( ,c)(c,o)(o,n)(n,t)(t,a)(a,r)(r,t)(t,e)(e,r)(r,s)(s, )( ,o)(o,r)(r,s)(s,t)(t,e)(e,r)(r, )( ,t)(t,r)(r,a)(a,f)(f,t)(t,i)(i,v)(v,e)(e, )( ,p)(p,l)(l,a)(a,y)(y,i)(i,n)(n,g)(g, )( ,s)(s,i)(i, )( ,s)(s,m)(m, )( ,c)(c,o)(o,n)(n,d)(v,i)(i,c)(c,e)(e, )( ,i)(i,n)(n, )( ,o)(o,r)(r,g)(g,a)(a,n)(n, )( ,r)(r,e)(e,m)(m,o)(o,n)(n,e)(e,r)(r, )( ,h)(h,i)(i,s)(s, )( ,s)(s,l)(l,i)\n",
      "chars: sxistel contarters orster traftive playing si sm convice in organ remoner his sl\n",
      "bigrams: (i,n)(n,o)(o,n)(n, )( ,t)(t,h)(h,e)(e,r)(r,a)(a,d)(d,e)(e,d)(d, )( ,i)(i,n)(n, )( ,n)(n,a)(a,m)(m, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,u)(u,c)(c,c)(c,e)(e,r)(r,v)(l,y)(y, )( ,m)(m,o)(o,r)(r,m)(m,a)(a,n)(n,d)(d, )( ,n)(n,a)(a,t)(t,s)(s,h)(h,e)(e,b)(b,e)(e,r)(r,g)(g, )( ,b)(l,u)(u,t)(t, )( ,i)(i,n)(n, )( ,h)(h,i)(i,s)(s, )( ,b)(b,e)(e,f)\n",
      "chars: inon theraded in nam one nine one nine succerly mormand natsheberg lut in his be\n",
      "bigrams: (f,j)(y,d)(d, )( ,o)(o,f)(f, )( ,s)(s,o)(o,d)(d,r)(r,e)(e,s)(s,s)(s, )( ,o)(o,f)(f, )( ,e)(e,g)(g,a)(a,m)(m,p)(p,l)(l,e)(e, )( ,f)(f,o)(o,c)(c,e)(e,s)(s,t)(t, )( ,a)(a, )( ,c)(c,o)(o,m)(m,e)(e,d)(d, )( ,s)(s, )( ,o)(o,l)(l,d)(d, )( ,a)(a,s)(s, )( ,i)(i,n)(n,t)(t,e)(e,r)(r,p)(p,o)(o,s)(s,e)(e,d)(d, )( ,a)(a,s)(s, )( ,b)(b,o)(o,n)(n,t)(t,i)(i,e)(e,s)(s, )( ,r)(r,a)(a,t)(t,h)(h,a)(a,n)(n,n)(n, )( ,i)\n",
      "chars: fyd of sodress of egample focest a comed s old as interposed as bonties rathann \n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3100: 1.594470 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3200: 1.612486 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3300: 1.598075 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3400: 1.628371 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 3500: 1.616842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3600: 1.636147 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3700: 1.604021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 3800: 1.600123 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 3900: 1.588740 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 4000: 1.603666 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "bigrams: (l,m)(t,z)(g,m)(m,s)(s, )( ,o)(o,r)(r, )( ,o)(o,c)(c,e)(e,r)(r,t)(t,s)(s, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,d)(d,e)(e,s)(s,o)(o,c)(c,i)(i,p)(p,l)(l,e)(e,s)(s, )( ,t)(t,h)(h,e)(e, )( ,m)(m,o)(o,r)(r,e)(e, )( ,b)(b,e)(e,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,u)(u,n)(n,t)(t, )( ,p)(p,a)(a,m)(m,o)(o,r)(r,i)(i,u)(u,r)(r,e)(e, )( ,s)(s,a)(a,p)(p,t)(t,l)(l,a)(a,n)(n,d)(d,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,w)(w,h)\n",
      "chars: ltgms or ocerts there desociples the more being count pamoriure saptlanded the w\n",
      "bigrams: (z,i)(i,v)(v,e)(e, )( ,t)(t,w)(w,o)(o, )( ,b)(b, )( ,k)(k,n)(n,e)(e,s)(s,t)(t,s)(s, )( ,h)(h,e)(e,r)(r,e)(e, )( ,p)(p,o)(o,l)(l,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l,l)(l,y)(y, )( ,w)(w,e)(e,r)(r, )( ,m)(m,a)(a,i)(i,n)(n, )( ,m)(m, )( ,a)(a,c)(c,t)(t,h)(h,o)(o,u)(u,t)(t, )( ,f)(f,u)(u,n)(n, )( ,e)(e,r)(r,a)(a,l)(l, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,d)\n",
      "chars: zive two b knests here politically wer main m acthout fun eral zero zero in the \n",
      "bigrams: (o,a)(a,a)(a,s)(s, )( ,l)(l,e)(e,a)(a, )( ,t)(t,w)(w,o)(o, )( ,f)(f,i)(i,v)(v,e)(e, )( ,n)(n,i)(i,z)(z,e)(e, )( ,b)(b,y)(y, )( ,a)(a,n)(n,n)(n,e)(e,x)(x,t)(t, )( ,b)(b,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,z)(z,a)(a,t)(t,e)(e, )( ,f)(f, )( ,g)(g,d)(d,a)(a,y)(y, )( ,o)(o,n)(n,e)(e, )( ,a)(a,n)(n,d)(d, )( ,c)(c,o)(o,m)(m,b)(b, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,o)(o,n)\n",
      "chars: oaas lea two five nize by annext beight zate f gday one and comb one eight one o\n",
      "bigrams: (u,s)(s, )( ,w)(w,i)(i,t)(t,h)(h, )( ,b)(b,o)(o,n)(n,t)(t,i)(i,n)(n,c)(c,y)(y, )( ,w)(w,a)(a,s)(s, )( ,m)(m,i)(i,l)(l,i)(i,n)(n,n)(n,e)(e, )( ,h)(h,e)(e,r)(r,u)(u,g)(g,e)(e,r)(r,a)(a,l)(l,i)(i,z)(z,e)(e,d)(d, )( ,b)(b,i)(i,b)(b,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,d)(d,u)(u,b)(b,l)(l,i)(i,c)(c, )( ,m)(m,i)(i,n)(n,o)(o, )( ,a)(a,s)(s, )( ,p)(p,r)(r,e)(e,s)(s,i)(i,d)(d,e)(e,r)(r, )( ,w)(w,a)\n",
      "chars: us with bontincy was milinne herugeralized bibition of dublic mino as presider w\n",
      "bigrams: (i,m)(m, )( ,o)(o,n)(n,e)(e, )( ,h)(h,a)(a,s)(s,t)(t,e)(e,s)(s,s)(s, )( ,t)(t,o)(o, )( ,p)(p,u)(u,r)(r,o)(o,u)(u,s)(s, )( ,z)(z,z)(l,u)(u,i)(i,t)(t,z)(z, )( ,f)(f,a)(a,m)(m,i)(i,t)(t,e)(e,r)(r, )( ,l)(e,n)(n, )( ,t)(t,h)(h,e)(e, )( ,g)(g,a)(a,t)(t, )( ,b)(b,e)(e,c)(c,a)(a, )( ,d)(d,e)(e,l)(l, )( ,h)(h,i)(i,s)(s, )( ,w)(w,o)(o,r)(r,k)(k,e)(e,n)(n, )( ,h)(h,e)(e,a)(a,d)(d, )( ,i)(i,t)(t, )( ,a)(a,t)\n",
      "chars: im one hastess to purous zluitz famiter en the gat beca del his worken head it a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4100: 1.585245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4200: 1.581631 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4300: 1.565274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 4400: 1.556895 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.31\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.562688 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 4600: 1.567265 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 4700: 1.570903 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 4800: 1.578071 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4900: 1.570569 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5000: 1.551507 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "================================================================================\n",
      "bigrams: (k,a)(a,r)(r,y)(y, )( ,c)(c,r)(r,u)(u,n)(n,t)(t,i)(i,p)(p,h)(h,y)(y, )( ,c)(c,a)(a,n)(n, )( ,s)(s,e)(e,v)(v,e)(e,r)(r,a)(a,l)(l,l)(l,y)(y, )( ,f)(f,e)(e,w)(w,n)(n,i)(i,n)(n,g)(g, )( ,c)(c,i)(i,o)(o,n)(n,g)(g,e)(e,r)(r, )( ,f)(f,i)(i,n)(n,a)(a,r)(r,s)(s, )( ,e)(e,a)(a,l)(l,i)(i,z)(z,e)(e,d)(d, )( ,j)(j,o)(o,i)(i,n)(n,t)(t,e)(e,r)(r, )( ,c)(c,l)(l,u)(u,s)(s, )( ,a)(a, )( ,o)(o,r)(r,d)(d,e)(e,r)(r, )\n",
      "chars: kary cruntiphy can severally fewning cionger finars ealized jointer clus a order\n",
      "bigrams: (s,u)(u, )( ,s)(s,e)(e,l)(l,l)(l, )( ,a)(a,n)(n,d)(d, )( ,r)(r,e)(e,i)(i,g)(g,n)(n, )( ,t)(t,w)(w,o)(o, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,w)(w,o)(o, )( ,f)(f,i)(i,v)(v,e)(e, )( ,s)(s,i)(i,x)(x, )( ,s)(s,i)(i,x)(x, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,w)(w,o)(o, )( ,o)(o,n)(n,e)(e, )( ,i)(i,s)(s,i)(i,m)(m,a)(a,t)(t,e)(e,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)\n",
      "chars: su sell and reign two eight two five six six two zero zero two one isimates of t\n",
      "bigrams: (r,p)(p,o)(o,l)(l,i)(i,t)(t,i)(i,t)(t,i)(i,c)(c, )( ,r)(r,e)(e,d)(d,i)(i,e)(e,d)(d, )( ,t)(t,h)(h,e)(e, )( ,h)(h,e)(e, )( ,s)(s, )( ,f)(f,r)(r,o)(o,m)(m, )( ,m)(m,u)(u,s)(s,i)(i,c)(c, )( ,t)(t,e)(e,n)(n, )( ,t)(t,i)(i,m)(m,e)(e, )( ,o)(o,r)(r, )( ,w)(w,i)(i,t)(t,h)(h, )( ,a)(a, )( ,r)(r,e)(e,t)(t,e)(e,r)(r,n)(n,a)(a,l)(l, )( ,c)(c,o)(o,m)(m,e)(e,t)(t,h)(h,i)(i,n)(n,g)(g, )( ,h)(h,a)(a,s)(s, )( ,t)\n",
      "chars: rpolititic redied the he s from music ten time or with a reternal comething has \n",
      "bigrams: (a,d)(d, )( ,i)(i,s)(s, )( ,r)(r,a)(a,b)(b,i)(i,c)(c, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,a)(a,l)(l, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,w)(w,h)(h,o)(o, )( ,s)(s, )( ,c)(c,e)(e,l)(l,e)(e,y)(y, )( ,h)(h,i)(i,m)(m,e)(e, )( ,d)(d,e)(e,n)(n,c)(c,h)(h,e)(e,r)(r, )( ,f)(f,o)(o,r)(r, )( ,t)(t,o)(o, )( ,t)(t,h)(h,e)(e, )( ,s)(s,h)(h,e)(e, )( ,f)(f,o)(o,r)\n",
      "chars: ad is rabic general of the nine seven who s celey hime dencher for to the she fo\n",
      "bigrams: (x,r)(s,v)(c, )( ,b)(b,e)(e,c)(c,a)(a,m)(m,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,t)(t,y)(y,p)(p,e)(e, )( ,p)(p,r)(r,o)(o,p)(p,e)(e,t)(t,i)(i,v)(v,e)(e,s)(s, )( ,o)(o,f)(f,f)(f,i)(i,c)(c,i)(i,e)(e,s)(s, )( ,m)(m,a)(a,i)(i,n)(n,l)(l,y)(y, )( ,b)(b,y)(y, )( ,a)(a, )( ,w)(w,h)(h,o)(o, )( ,s)(s,h)(h,o)(o,w)(w,s)(s,m)(m,i)(i,r)(r,d)(d,o)(o,x)(w,n)(n, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,t)(t,r)\n",
      "chars: xsc became of the type propetives officies mainly by a who showsmirdown of the t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5100: 1.539328 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5200: 1.521038 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.05\n",
      "Average loss at step 5300: 1.513720 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 5400: 1.514212 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 5500: 1.501827 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 5600: 1.521578 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 5700: 1.508585 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 5800: 1.509452 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 5900: 1.513010 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.01\n",
      "Average loss at step 6000: 1.485040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "================================================================================\n",
      "bigrams: (p,b)(c, )( ,s)(s,i)(i,n)(n,c)(c,h)(h,e)(e,r)(r, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,e)(e,r)(r, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,g)(l,o)(o,u)(u,l)(l,d)(d,r)(r,a)(a,y)(y, )( ,l)(l,o)(o,o)(o,k)(k, )( ,u)(u,n)(n,i)(i,v)(v,e)(e,r)(r,s)(s,o)(o,n)(n, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,p)(p,h)(h,e)(e,t)(t,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,w)\n",
      "chars: pc sincher counter one nine louldray look universon two zero zero ophete in the \n",
      "bigrams: (j,h)(v,o)(o,o)(o,l)(l,i)(i,n)(n,g)(g, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,s)(s, )( ,w)(w,a)(a,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,a)(a,c)(c,h)(h,a)(a, )( ,s)(s,m)(m,o)(o,s)(s,t)(t, )( ,r)(r,e)(e,s)(s,i)(i,g)(w,e)\n",
      "chars: jvooling the one nine one five zero s wave one nine seven eight acha smost resiw\n",
      "bigrams: (g,f)(y,o)(o,u)(u,s)(s, )( ,o)(o,f)(f, )( ,n)(n,a)(a,m)(m,e)(e,s)(s, )( ,a)(a,l)(l,l)(l, )( ,m)(m,a)(a,j)(j,o)(o,r)(r, )( ,q)(q,u)(u,i)(i,c)(c,t)(t,s)(s, )( ,o)(o,f)(f, )( ,l)(l,i)(i,f)(f,f)(f,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,s)(s,p)(p,i)(i,a)(a,t)(t,e)(e,s)(s, )( ,c)(c,e)(e,n)(n,t)(t,a)(a,l)(l,i)(i,z)(z,e)(e,d)(d, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,n)(n,a)(a,n)(n, )( ,e)(e,i)(i,g)\n",
      "chars: gyous of names all major quicts of liffer the spiates centalized in the finan ei\n",
      "bigrams: (a,u)(u,a)(a,r)(r,y)(y, )( ,a)(a,n)(n,d)(d, )( ,t)(t,u)(u,r)(r,n)(n,a)(a,l)(l, )( ,e)(e,l)(l,i)(i,s)(s,h)(h,e)(e,g)(g,e)(e, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e,y)(y, )( ,a)(a,s)(s, )( ,o)(o,p)(p,e)(e,r)(r,a)(a,e)( ,s)(s,k)(k, )( ,t)(t,w)(w,o)(o, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,i)(i,s)(s,p)(p,e)(e,n)(n,n)(n,e)(e,d)(d, )( ,b)(b,y)(y, )( ,w)(w,a)(a,s)(s, )( ,s)(s,m)(m,a)(a,l)(l,l)(l,e)(e,r)(r, )( ,m)\n",
      "chars: auary and turnal elishege in they as opera sk two seven ispenned by was smaller \n",
      "bigrams: (e, )( ,a)(a,n)(n,d)(d, )( ,f)(f,r)(r,a)(a,f)(w, )( ,s)(s,h)(h,i)(i,p)(p, )( ,s)(s, )( ,l)(l,a)(a,w)(w,s)(s, )( ,a)(a,r)(r,e)(e, )( ,i)(i,t)(t,a)(a,l)(l,e)(e, )( ,r)(r,a)(a,t)(t,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n,s)(s, )( ,w)(w,o)(o,r)(r,k)(k, )( ,t)(t,h)(h,e)(e, )( ,f)(f,i)(i,r)(r,s)(s,t)(t, )( ,f)(f,o)(o,r)(r,e)(e,i)(i,d)(d, )( ,a)(a,t)(t, )( ,s)(s, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )\n",
      "chars: e and fraw ship s laws are itale ratrations work the first foreid at s zero zero\n",
      "================================================================================\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6100: 1.505472 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6200: 1.473447 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.01\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 6300: 1.481521 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6400: 1.487197 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6500: 1.500361 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 3.99\n",
      "Average loss at step 6600: 1.535301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 6700: 1.517847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6800: 1.544135 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 3.98\n",
      "Average loss at step 6900: 1.516381 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.00\n",
      "Average loss at step 7000: 1.511512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "bigrams: (u,h)(b, )( ,t)(t,h)(h,e)(e, )( ,i)(i,p)(p,s)(s,e)(e,l)(l,l)(l,a)(a, )( ,b)(b,o)(o,a)(j,e)(e,f)(f,i)(y,p)(p,i)(i,a)(a,n)(n, )( ,c)(c,o)(o,a)(a,s)(s,t)(t,s)(s, )( ,i)(i,t)(t, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,i)(i,v)(v,i)(i, )( ,a)(a, )( ,s)(s,u)(u,p)(p,p)(p,l)(l,o)(o,n)(n,i)(i,n)(n,g)(g, )( ,f)(f,o)(o,r)(r, )( ,h)(h,i)(i,s)(s, )( ,w)(w,a)(a,y)(y, )( ,s)(s,o)\n",
      "chars: ub the ipsella bojefypian coasts it in one seven fivi a supploning for his way s\n",
      "bigrams: (a,m)(m,e)(e,n)(n,t)(t, )( ,s)(s,e)(e,c)(c,o)(o,n)(n,d)(d, )( ,t)(t,o)(o, )( ,d)(d,a)(a,i)(i,i)(i,o)(o,n)(n,y)(y, )( ,w)(w,i)(i,t)(t,h)(h, )( ,i)(i,n)(n, )( ,a)(a, )( ,s)(s,e)(e,p)(p,t)(t,r)(r,a)(a,f)(f,t)(t,a)(a,i)(i,c)(c,e)(e, )( ,i)(i,n)(n, )( ,w)(w,i)(i,t)(t,h)(h, )( ,o)(o,n)(n,e)(e, )( ,d)(d,o)(o,g)(g,a)(u,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,p)(p,l)(l,a)(a,y)(y,e)(e,r)(r, )( ,w)(w,h)(h,i)(i,c)\n",
      "chars: ament second to daiiony with in a septraftaice in with one dogues and player whi\n",
      "bigrams: (x,m)(i, )( ,t)(t,o)(o, )( ,d)(d,e)(e,t)(t,o)(o,m)(m, )( ,t)(t,h)(h,e)(e, )( ,u)(u,s)(s,u)(u,a)(a,l)(l,l)(l,y)(y, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,n)(r,t)(m,a)(a,s)(s, )( ,o)(o,f)(f, )( ,g)(g,u)(u,a)(a,r)(r,e)(e,l)(l,s)(s, )( ,i)(i,t)(t, )( ,w)(w,o)(o,u)(u,r)(r, )( ,w)(w,e)(e,b)(b,e)(e,l)(l,i)(i,t)(t,a)(a,r)(r,y)(y, )( ,o)(o,u)(u,t)(t,e)(e,d)(d, )( ,i)(i,n)(n, )( ,j)(j, )( ,f)(f,o)(o,l)(l,d)(d, )( ,c)\n",
      "chars: xi to detom the usually suppormas of guarels it wour webelitary outed in j fold \n",
      "bigrams: (c,i)(i,s)(s,t)(t, )( ,p)(p,a)(a,r)(r,y)(y, )( ,a)(a,n)(n,d)(d, )( ,m)(m,e)(e,a)(a,t)(t,e)(e, )( ,u)(u,s)(s,e)(e,d)(d, )( ,h)(h,u)(u,s)(s,e)(e, )( ,i)(i,s)(s, )( ,a)(a, )( ,s)(s,u)(u,b)(b,s)(s,t)(t,a)(a,n)(n,c)(c,e)(e, )( ,o)(o,f)(f, )( ,n)(n,a)(a,v)(v,a)(a,r)(r,s)(s,e)(e,a)(a,s)(s,k)(k, )( ,i)(i,n)(n,t)(t,e)(e,r)(r, )( ,m)(m,o)(h,m)(m,i)(i,c)(c,s)(s, )( ,m)(m,a)(a,n)(n,y)(y, )( ,s)(s,y)(y,s)(s,t)\n",
      "chars: cist pary and meate used huse is a substance of navarseask inter mhmics many sys\n",
      "bigrams: (q,c)(p,w)(a,z)(z, )( ,a)(a,u)(i,i)(i,d)(d,r)(r,o)(o,d)(d, )( ,e)(e,a)(a,r)(r,l)(l,y)(y, )( ,p)(p,o)(o, )( ,e)(e,x)(x,e)(e,r)(r,m)(m,o)(o,s)(s,e)(e, )( ,a)(a,n)(n,d)(d, )( ,s)(s,a)(a,c)(c,i)(i,n)(n,g)(g, )( ,m)(m,o)(o,o)(o,r)(r,p)(c,l)(l,y)(y, )( ,o)(o,n)(n,e)(e, )( ,t)(t,h)(h,r)(r,e)(e,e)(e, )( ,f)(f,o)(o,u)(u,r)(r, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,h)(h, )( ,p)(p,e)(e,t)(t,r)(r,o)(o,p)(p,e)(e, )\n",
      "chars: qpaz aiidrod early po exermose and sacing moorcly one three four five th petrope\n",
      "================================================================================\n",
      "Validation set perplexity: 3.97\n",
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same perplexity, but it's faster!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # big matrix\n",
    "  ifcox = tf.Variable(tf.truncated_normal([bigram_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([bigram_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, train=False):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    embed = tf.nn.embedding_lookup(ifcox, i)\n",
    "    if train:\n",
    "        embed = tf.nn.dropout(embed, 0.5)\n",
    "    all_gates = embed + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates[:, num_nodes:2*num_nodes])\n",
    "    update = all_gates[:, 2*num_nodes:3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates[:, 3*num_nodes:])\n",
    "        \n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state, True)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int64, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state, False)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.591728 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.04\n",
      "================================================================================\n",
      "bigrams: (n,u)(s,x)(o,j)(e, )(s,h)(r,u)(o,w)(d,z)(f,n)( ,k)(z,l)(m,f)(s,m)(y,m)(h,l)(g,j)(o,h)(c,n)(q,z)(u, )(u,g)(u,r)(k,o)(a,d)(z,u)(q,a)(y,z)(z,n)(l,y)(u, )(s,s)(s,t)( ,v)(j,a)(x,q)(k, )(x,w)(t,w)(n,c)(l,o)(y,f)(e, )(n,i)(v,r)(k,o)(e,a)(d,z)(u,a)(u,g)(y,k)(y,n)(p,f)(i,n)(p,x)(m,v)(o,b)(c,w)(k,i)(u,x)( ,y)(d,r)(w,o)(q,m)(y,d)(o,p)(l,l)(o,r)( ,b)(k,o)(t,v)(b,f)(t,n)(z,a)(c,g)(a,d)(t,n)(r,l)(m,q)(e,d)(g,g)\n",
      "chars: nsoesrodf zmsyhgocquuukazqyzluss jxkxtnlyenvkeduuyypipmocku dwqyolo ktbtzcatrmeg\n",
      "bigrams: (u,d)(g,b)(r, )(x,j)( ,s)(d,y)(b,x)(d,o)(m,r)(h,q)(m,v)(b,a)(z,l)(e,r)(s,m)(l,j)(s,y)(u,h)(a,b)(k,j)(l,d)(j,s)(d,m)(m,f)(j,h)(d,d)(i,o)(a,c)(j,w)(k,z)(x,z)(a,q)(z,v)(g,v)( ,v)(k,z)(a, )(v,z)(d,v)(k,r)(l,f)(e,n)(e,y)(f,m)(r,j)(r,j)( ,r)(x,l)(j,k)(p,f)(z, )(n,n)(v,k)(o,i)(q,g)(v,n)(c,d)(i,g)(f,i)(b,b)(d,w)(b,b)(k,b)(k,k)(w,y)(x,x)(b,v)(b,q)(a,k)(c, )(t,u)(i,o)(p,n)(g,m)(i,b)(w,a)(t,p)(l,y)(f,j)(s, )\n",
      "chars: ugrx dbdmhmbzeslsuakljdmjdiajkxazg kavdkleefrr xjpznvoqvcifbdbkkwxbbactipgiwtlfs\n",
      "bigrams: (m,j)(g,k)(x,n)(w,g)(s,a)(d,b)( ,x)(e,h)(t,j)( ,h)(a,d)(i,g)(q,g)(p,h)(c,x)(a,b)(p,f)(b,k)(q,l)(o,j)( ,s)(l,o)(d,a)(x,z)(d,h)(r,a)(g,z)(a,k)(y,j)(u,u)(r,i)(z,x)(n,b)(l,f)(u,t)(o,e)(e,d)(q,e)(i,s)(w,b)(k,e)(h,y)(j,h)(k,b)(k,w)(m,c)(t,r)(f,d)(j,z)(r,g)(p,i)(i,q)(y,e)(f,e)(q,x)(b,m)(q,x)(h,j)(p,x)(j,u)(q,n)(p,y)(m,d)(i,y)(j,i)(p,g)(v,g)(t,v)(a,d)(r,y)(l,n)(h,r)(h,p)(n,j)(h,a)(x,w)(a,k)(l,h)(t,p)(y,n)\n",
      "chars: mgxwsd et aiqpcapbqo ldxdrgayurznluoeqiwkhjkkmtfjrpiyfqbqhpjqpmijpvtarlhhnhxalty\n",
      "bigrams: (p,u)(i,c)(c,m)(m,q)( ,h)(k,g)(b,h)(n,l)(z,e)( ,s)( ,q)(v,w)(w,i)(o,e)(k,x)(e,r)(n,u)(s,c)(m,w)(q,c)(s,g)(e,t)(k,a)(d,o)(g,a)(w,o)(y,j)(k,a)(f,u)(z,d)(t,l)(g,u)(k,z)(x,a)(z,s)(k, )(l, )(r,z)(f,c)(o,c)(k,h)(e,u)(j, )(s,o)(f,h)(p,t)(y,e)(r,m)(m, )(h,k)(g,c)(l,g)(d,t)(y,z)(g,v)(w,t)(r,b)(p,t)(j,m)(f,l)(d,g)(k,l)(h,r)(f,g)(x,a)(z,d)(n,w)(v,p)(h,j)(l,r)(k,s)(u,o)(l,t)( ,r)(q,l)(n,o)(l,r)(h,s)(n,l)(u,v)\n",
      "chars: picm kbnz  vwokensmqsekdgwykfztgkxzklrfokejsfpyrmhgldygwrpjfdkhfxznvhlkul qnlhnu\n",
      "bigrams: (b,h)(b,z)(e,o)(z,y)(q,f)(g,d)(y,o)(j,k)(k,e)(i,y)(h,h)(q,o)(n,b)(o,t)(h,o)(r,a)(w,l)(d,w)(w, )(v,w)(v,m)(h,w)(q,j)(y,a)(b,a)(w,a)(t,h)(r,w)(i,y)(u,w)(x,t)(j,f)(p,v)(w,z)(w,f)(z,b)(w,b)(j,j)(z,l)(q,e)(l,t)(j,z)(t,q)(c,p)(w,f)(d,v)(y,b)(z,e)(q,u)(f,y)(r,y)(f,s)(e,z)(f,r)(c,o)(n,h)(y,w)(l,n)(n,u)(f,e)(m,d)(w,w)(k,b)(n,l)(i,k)(c,v)(y,h)(m,q)( ,u)(y,l)(q,a)(k,c)(k,j)(d,g)(h,m)(m,c)(k,m)(j,h)(z,l)(m,p)\n",
      "chars: bbezqgyjkihqnohrwdwvvhqybwtriuxjpwwzwjzqljtcwdyzqfrfefcnylnfmwknicym yqkkdhmkjzm\n",
      "================================================================================\n",
      "Validation set perplexity: 668.35\n",
      "Average loss at step 100: 5.401526 learning rate: 10.000000\n",
      "Minibatch perplexity: 142.05\n",
      "Validation set perplexity: 144.72\n",
      "Average loss at step 200: 4.431143 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.88\n",
      "Validation set perplexity: 44.94\n",
      "Average loss at step 300: 3.529936 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.42\n",
      "Validation set perplexity: 19.63\n",
      "Average loss at step 400: 3.018539 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.02\n",
      "Validation set perplexity: 13.57\n",
      "Average loss at step 500: 2.725260 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.78\n",
      "Validation set perplexity: 10.63\n",
      "Average loss at step 600: 2.512471 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.88\n",
      "Validation set perplexity: 9.85\n",
      "Average loss at step 700: 2.402179 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.48\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 800: 2.335531 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.89\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 900: 2.280984 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.84\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 1000: 2.248805 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.00\n",
      "================================================================================\n",
      "bigrams: (z,t)( ,w)(w,h)(h,e)(e,d)(d, )( ,t)(t,h)(h,a)(a,t)(t, )( ,d)(d,o)(o,s)(s,i)(i,s)(s,t)(t, )( ,t)(t,h)(h,e)(e, )( ,a)(a,l)(l,l)(l,o)(o, )( ,h)(h,o)(o,l)(l,s)(s,t)(t, )( ,b)(b,o)(o,r)(r,s)(s, )( ,k)(j,o)(o,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,f)(f,o)(o,u)(u,n)(n,d)(d,a)(a,r)(r, )( ,w)(w,e)(e,l)(l,l)(l,i)(i,s)(s,t)(t,h)(h, )( ,i)(i,n)(n, )( ,t)(t,o)(o, )( ,t)(t,w)(w,o)(o, )( ,e)(e,x)(x,a)(a,l)(l,l)(l, )( ,e)\n",
      "chars: z whed that dosist the allo holst bors joseven foundar wellisth in to two exall \n",
      "bigrams: (e,z)(a,m)(m,e)(e, )( ,o)(o,n)(n,e)(e, )( ,k)(f,i)(i,v)(v,e)(e, )( ,m)(j,o)(o,d)(d,d)(u,m)(m,e)(e,s)(s,s)(s, )( ,w)(w,o)(o,r)(r,k)(x,p)(n,a)(a,l)(l, )( ,p)(p,o)(o,i)(i,n)(n, )( ,a)(a,r)(r,a)(a,c)(c,t)(t,i)(i,c)(c, )( ,a)(a,l)(l,t)(t,h)(h, )( ,o)(o,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o, )( ,t)(t,w)(w,o)(o, )( ,s)(s,e)(e,e)(e,e)(e,n)(n,a)(a,n)(n, )( ,o)(o,f)(f, )( ,a)(a,n)(n, )( ,a)(a,i)(i,n)(n,g)(g, )\n",
      "chars: eame one five jodumess worxnal poin aractic alth one zero two seeenan of an aing\n",
      "bigrams: (w,n)(k, )( ,l)(l,e)(e,n)(n,s)(s,t)(t, )( ,c)(c,a)(a,m)(m,i)(i,t)(t,a)(a,n)(n,g)(g, )( ,g)(g,o)(o, )( ,s)(s,e)(e,v)(v,e)(e,l)(l,y)(y, )( ,t)(t,o)(o, )( ,b)(b,e)(e, )( ,n)(n,o)(o,t)(t, )( ,t)(t,o)(o, )( ,c)(c,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,w)(w,h)(h,i)(i,l)(l,e)(e, )( ,t)(t,o)(o, )( ,s)(s,e)(e,v)(v,e)(e,n)(n,t)(t, )( ,w)(w,i)(i,r)(r, )( ,s)(s,e)(e,l)(l,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,s)(s,o)(o,m)\n",
      "chars: wk lenst camitang go sevely to be not to cection while to sevent wir selition so\n",
      "bigrams: (t,r)(r,e)(e,d)(d, )( ,b)(b,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,h)(h,o)(o,l)(l,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,a)(a,r)(r,n)(n, )( ,i)(i,n)(n,d)(d, )( ,i)(i,n)(n,s)(s,t)(t,o)(o, )( ,d)(d,e)(e,t)(t,t)(t,e)(e,r)(r,m)(m, )( ,m)(m,o)(o,r)(r,c)(c,h)(h, )( ,u)(u,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,n)(n,i)(i,n)(n,e)(e, )( ,p)(p,r)(r,o)(o,n)(n,s)(s, )( ,b)(b,e)\n",
      "chars: tred be two zero hole five the searn ind insto detterm morch useven nine prons b\n",
      "bigrams: (z,o)(o,t)(t,h)(h,e)(e, )( ,t)(t,h)(h,e)(e, )( ,h)(h,i)(i,s)(s, )( ,i)(i,s)(s, )( ,f)(f,r)(r,o)(o,m)(m,e)(e,f)(c,t)(t,s)(s, )( ,a)(a,n)(n,d)(d, )( ,r)(r,e)(e,a)(a,r)(r, )( ,p)(p,a)(a,n)(n,c)(c,e)(e, )( ,a)(a, )( ,p)(p,a)(a,r)(r, )( ,p)(p,a)(a,n)(n, )( ,t)(t,o)(o, )( ,i)(i,n)(n, )( ,s)(s,i)(i,n)(n,e)(e, )( ,z)(z,e)(e,r)(r,o)(o,r)(r,n)(n,i)(i,s)(s,h)(h, )( ,a)(a,i)(i,r)(r,o)(o,v)(v,i)(i,t)(t,s)(s, )\n",
      "chars: zothe the his is fromects and rear pance a par pan to in sine zerornish airovits\n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 1100: 2.169647 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.69\n",
      "Validation set perplexity: 7.41\n",
      "Average loss at step 1200: 2.111762 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.67\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 1300: 2.086328 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.16\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 1400: 2.067082 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.55\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 1500: 2.046728 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 1600: 2.017169 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 6.76\n",
      "Average loss at step 1700: 1.988918 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.89\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 1800: 1.957384 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.41\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 1900: 1.951791 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 2000: 1.931326 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "================================================================================\n",
      "bigrams: (r,p)(p,a)(a,t)(t,i)(i,o)(o,n)(n,c)(c,r)(r,e)(e, )( ,p)(p,a)(a,r)(r,t)(t,t)(t,u)(u,s)(s, )( ,w)(w,i)(i,l)(l,e)(e,a)(a,t)(t,e)(e,s)(s, )( ,t)(t,h)(h,a)(a,t)(t, )( ,p)(p,o)(o,r)(r,k)(k, )( ,f)(f,o)(o,u)(u,r)(r, )( ,i)(i,s)(s, )( ,m)(m,a)(a,g)(g,e)(e,r)(r,s)(s, )( ,l)(l,a)(a,b)(b,l)(l,o)(s,u)(u,m)(m, )( ,f)(f,a)(a,s)(s,t)(t, )( ,a)(a,d)(d, )( ,p)(p,o)(o,r)(r,t)(t, )( ,g)(g,a)(a,r)(r,e)(e, )( ,t)(t,o)\n",
      "chars: rpationcre parttus wileates that pork four is magers lablsum fast ad port gare t\n",
      "bigrams: (v,q)(x,a)(a,i)(i,n)(n,g)(g, )( ,n)(n,e)(e,c)(c,o)(o,o)(o,u)(u,l)(l,a)(a,l)(l, )( ,a)(a,s)(s, )( ,c)(c,o)(o,u)(u,l)(l,d)(d, )( ,g)(g,l)(l,a)(a,n)(n,i)(i,t)(t,i)(i,v)(v,e)(e, )( ,h)(h,i)(i,s)(s,e)(e,d)(d, )( ,s)(s, )( ,m)(m,e)(e,r)(r,e)(e, )( ,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n,t)(t,i)(i,o)(o,n)(n, )( ,a)(a,n)(n,c)(c,l)(l,i)(i,c)(c,a)(a,l)(l, )( ,a)(a,n)(n,d)(d, )( ,m)(m,a)\n",
      "chars: vxaing necooulal as could glanitive hised s mere or the sevention anclical and m\n",
      "bigrams: (o,f)(s,t)(t,i)(i,e)(e, )( ,t)(t,o)(o, )( ,r)(r,e)(e,s)(s,t)(t, )( ,h)(h,e)(e, )( ,r)(r,e)(e,a)(a,s)(s,e)(e,s)(s, )( ,a)(a,n)(n,d)(d, )( ,b)(b,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,h)(h,e)(e, )( ,e)(e,x)(x,t)(t,i)(i,l)(l,e)(e, )( ,t)(t,h)(h,e)(e, )( ,h)(h,a)(a,v)(v,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,b)(b,u)(u,t)(t, )( ,j)(j,o)(o,t)(t,s)(s, )( ,p)(p,e)(e,a)(a,n)(n, )( ,o)(o,f)(f, )( ,s)(s,u)(u,c)\n",
      "chars: ostie to rest he reases and beight the extile the have seven but jots pean of su\n",
      "bigrams: (w,v)(i,q)(q,u)(u,e)(e,s)(s, )( ,w)(w,h)(h,i)(i,l)(l,e)(e, )( ,d)(d,i)(i,s)(s,c)(c,l)(l,u)(u,e)(e,s)(s, )( ,c)(c,o)(o,u)(u,l)(l,d)(d,e)(e,r)(r, )( ,u)(u,n)(n,i)(i,n)(n,i)(i,m)(m,a)(a, )( ,t)(t,e)(e,g)(g,a)(a,m)(m,e)(e, )( ,n)(n,a)(a,r)(r,g)(g,e)(e, )( ,c)(c,o)(o,n)(n,s)(s,t)(t,a)(a,l)(l,l)(l, )( ,e)(e,a)(a,f)(p,l)(l,e)(e, )( ,s)(s,i)(i,v)(v,i)(i,a)(a,l)(l,l)(l, )( ,s)(s,p)(p,e)(e,c)(c,t)(t,i)(i,o)\n",
      "chars: wiques while disclues coulder uninima tegame narge constall eaple siviall specti\n",
      "bigrams: (k,s)(s, )( ,a)(a,n)(n,n)(n,s)(s,p)(p,e)(e,c)(c,t)(t,e)(e,d)(d, )( ,c)(c,a)(a,t)(t,h)(h, )( ,a)(a,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,c)(c,o)(o,n)(n,d)(d,e)(e,a)(a, )( ,i)(i,n)(n, )( ,t)(t,h)(h,e)(e, )( ,t)(t,e)(e,r)(r,k)(k,s)(s, )( ,a)(a,n)(n,d)(d, )( ,p)(p,r)(r,e)(e,l)(l,a)(a,n)(n,d)(d, )( ,d)(d,e)(e,a)(a,v)(v,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,a)(a,n)(n,y)\n",
      "chars: ks annspected cath anding condea in the terks and preland deave six one seven an\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 6.24\n",
      "Average loss at step 2100: 1.927459 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.86\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 2200: 1.953443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 2300: 1.943050 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 2400: 1.924638 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.08\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 2500: 1.920129 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 2600: 1.901885 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 2700: 1.908867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 2800: 1.896142 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 2900: 1.882293 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 5.91\n",
      "Average loss at step 3000: 1.892510 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "================================================================================\n",
      "bigrams: (s,y)(y, )( ,c)(c,h)(h,a)(a,d)(d,i)(i,a)(a,n)(n, )( ,t)(t,o)(o, )( ,b)(b,a)(a,n)(n,s)(s,t)(t,o)(o,r)(r,i)(i,c)(c,a)(a,n)(n, )( ,c)(c,r)(r,e)(e,s)(s,s)(s, )( ,s)(s,u)(u,e)(c,i)(i,a)(a,n)(n, )( ,i)(i,s)(s, )( ,a)(a,l)(l,a)(a, )( ,t)(t,r)(r,e)(e,s)(s,s)(s,l)(l,o)(o,g)(g,i)(i,a)(a,n)(n,s)(s, )( ,a)(a,n)(n,t)(t,s)(s, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,p)(p,o)(o,i)(i,c)(c,e)(e, )( ,n)(n,o)(o, )( ,h)\n",
      "chars: sy chadian to banstorican cress sucian is ala tresslogians ants of the poice no \n",
      "bigrams: (v,z)(g,m)(m, )( ,p)(p,e)(e,s)(s,t)(t,e)(e,r)(r,s)(s, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,m)(m,u)(u,t)(t,h)(h,s)(s, )( ,a)(a,n)(n,d)(d, )( ,i)(i,s)(s, )( ,d)(d,e)(e,f)(f,o)(o,r)(r,e)(e,n)(n,c)(c,y)(y, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,f)(f,i)(i,v)(v,e)(e, )( ,s)(s,i)(i,x)(x, )\n",
      "chars: vgm pesters eight one eight muths and is deforency in one nine nine six five six\n",
      "bigrams: (j,o)(o,y)(y,s)(s, )( ,c)(c,r)(r,o)(o,d)(d,u)(u,c)(z,e)(e,l)(l, )( ,w)(w,e)(e,r)(r,n)(n, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,i)(i,s)(s, )( ,t)(t,h)(h,e)(e,m)(m, )( ,o)(o,r)(r,i)(i, )( ,o)(o,r)(r, )( ,h)(h,a)(a,d)(d, )( ,b)(b,i)(i,o)(o,n)(n,t)(t, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,w)(w,a)(a,s)(s, )( ,w)(w,r)(r,e)(e,d)(d, )( ,a)(a,n)(n,d)(d, )( ,h)(h,i)(i,p)(p,p)\n",
      "chars: joys croduzel wern eight is them ori or had biont two zero zero was wred and hip\n",
      "bigrams: (q,l)(u,l)(l,a)(a,n)(n,d)(d, )( ,b)(b,y)(y, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,v)(v,e)(e,r)(r,i)(i,n)(n,g)(g, )( ,e)(e,x)(x,c)(c,k)(k, )( ,e)(e,l)(l,l)(l,i)(i,c)(c,a)(a,r)(r,y)(y, )( ,i)(i,t)(t,a)(a,l)(l, )( ,d)(d,e)(e,v)(v,e)(e,n)(n,t)(t,a)(a,n)(n, )( ,i)(i, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,s)(s,i)(i,x)(x, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)\n",
      "chars: quland by the covering exck ellicary ital deventan i five one six two zero zero \n",
      "bigrams: (d,i)(i,b)(b,u)(u,r)(r,s)(s, )( ,r)(r,e)(e,g)(g,e)(e,r)(r,s)(s,i)(i,n)(n,g)(g, )( ,b)(b,u)(u,t)(t, )( ,b)(b,e)(e,t)(t, )( ,c)(c,o)(o,n)(n, )( ,l)(l,a)(a,u)(u,n)(n,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,o)(o, )( ,f)(f,o)(o,n)(n,e)(e, )( ,w)(w,i)(h,o)(o, )( ,a)(a,n)(n, )( ,t)(t,h)(h,a)(a,t)(t, )( ,m)(m,a)(a,l)(l,m)(m,o)(o,r)(r,s)(s, )( ,t)(t,h)(h,e)(e, )( ,m)(m,e)(e,m)(m,s)(s, )( ,a)(a,n)(n,d)(d, )( ,g)\n",
      "chars: diburs regersing but bet con launction to fone who an that malmors the mems and \n",
      "================================================================================\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 3100: 1.855508 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.59\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 3200: 1.830414 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 3300: 1.844614 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 3400: 1.830574 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.36\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 3500: 1.862763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 3600: 1.838583 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 3700: 1.844564 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 3800: 1.838398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 3900: 1.842011 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 4000: 1.825688 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "================================================================================\n",
      "bigrams: ( ,s)(s, )( ,a)(a,s)(s, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,s)(s,i)(i,x)(x, )( ,t)(t,w)(w,o)(o, )( ,i)(i,n)(n, )( ,o)(o,n)(n,e)(e, )( ,t)(t,a)(a,m)(m,e)(e,s)(s, )( ,o)(o,n)(n,e)(e, )( ,t)(t,w)(w,o)(o, )( ,l)(l,e)(e,v)(v, )( ,w)(w,r)(r,i)(i,t)(t,i)(i,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,h)(h,i)(i,s)\n",
      "chars:  s as one two zero zero zero zero six two in one tames one two lev writition thi\n",
      "bigrams: (e,m)(m,e)(e,n)(n,g)(g,e)(e, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,s)(s,e)(e,v)(v,e)(e,n)(n, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,b)(b,o)(o,o)(o,p)(p,s)(s, )( ,w)(w,a)(a,s)(s, )( ,w)(w,a)(a,s)(s, )( ,t)(t,h)(h,e)(e, )( ,e)(e,m)(m,m)(m,a)\n",
      "chars: emenge six one nine one nine one nine eight seven one nine boops was was the emm\n",
      "bigrams: (w,q)(w,e)(e,r)(r, )( ,o)(o,f)(f, )( ,p)(p,r)(r,o)(o,u)(u,p)(p, )( ,o)(o,f)(f, )( ,t)(t,w)(w,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n, )( ,r)(r,o)(o,m)(m,i)(i,n)(n,g)(g, )( ,o)(o,f)(f, )( ,g)(g,e)(e,n)(n,e)(e,r)(r,a)(a,l)(l,i)(i,z)(z,e)(e, )( ,o)(o,f)(f, )( ,w)(w,h)(h,i)(i,c)(c,h)(h, )( ,i)(i,t)(t,s)(s, )( ,e)(e,i)(i,t)(t,h)(h,e)\n",
      "chars: wwer of proup of two zero zero one nine n roming of generalize of which its eith\n",
      "bigrams: (n,o)(o,t)(t, )( ,c)(c,a)(a,l)(l,l)(l, )( ,i)(i,s)(s, )( ,m)(m,u)(u,s)(s,e)(e, )( ,v)(v,i)(i,t)(t,c)(c,a)(a,n)(n,d)(d, )( ,o)(o,n)(n,e)(e, )( ,c)(c,l)(l,o)(o,w)(w,n)(n,e)(e, )( ,w)(w,i)(i,t)(t,h)(h, )( ,a)(a,s)(s, )( ,a)(a, )( ,f)(f,r)(r,o)(o,m)(m, )( ,t)(t,h)(h,e)(e,r)(r,e)(e, )( ,a)(a, )( ,s)(s,o)(o,n)(n,e)(e,s)(s,s)(s, )( ,o)(o,f)(f, )( ,o)(o, )( ,w)(w,i)(i,t)(t,h)(h, )( ,o)(o,f)(f, )( ,p)(p,e)\n",
      "chars: not call is muse vitcand one clowne with as a from there a soness of o with of p\n",
      "bigrams: (c,l)(l,e)(e,n)(n,t)(t,r)(r,y)(y, )( ,i)(i,s)(s, )( ,e)(e,n)(n,t)(t,u)(u,r)(r,i)(i,c)(c,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,p)(p,r)(r,o)(o,j)(j,e)(e,c)(c,t)(t, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,b)(b,o)(o,n)(n,g)(g, )( ,f)(f,r)(r,e)(e,e)(e,n)(n, )( ,a)(a,d)(d,m)(m,a)(a,n)(n,t)(t,i)(i,a)(a, )( ,l)(l,a)(a,n)(n,d)(d, )( ,a)(a,l)(l,i)(i,v)(v,e)(e, )( ,o)(o,f)(f, )( ,c)(c,a)(a,p)(p,p)(p,o)(o,s)\n",
      "chars: clentry is enturicand the project of the bong freen admantia land alive of cappo\n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 4100: 1.797363 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 4200: 1.788349 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 4300: 1.792796 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 4400: 1.776842 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 5.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4500: 1.812576 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 4600: 1.799360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 4700: 1.793641 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 4800: 1.778242 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 4900: 1.784326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 5000: 1.773381 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.94\n",
      "================================================================================\n",
      "bigrams: (d,w)(w,a)(a,r)(r,y)(y, )( ,c)(c,o)(o,n)(n,n)(n,e)(e,e)(e,d)(d, )( ,f)(f,r)(r,e)(e,a)(a,t)(t, )( ,i)(i,n)(n, )( ,r)(r,o)(o,c)(c,k)(k, )( ,y)(y,s)(s, )( ,a)(a,l)(l,l)(l,i)(i,s)(s,h)(h, )( ,u)(u,p)(p, )( ,b)(b,a)(a,c)(c,k)(k, )( ,o)(o,f)(f, )( ,c)(c,o)(o,n)(n,t)(t,e)(e,m)(m,b)(b,o)(o,n)(n, )( ,a)(a, )( ,c)(c,o)(o,u)(u,n)(n,t)(t,r)(r,y)(y, )( ,c)(c,h)(h,a)(a,n)(n,g)(g,u)(u,a)(a,g)(g,e)(e, )( ,s)(s,t)\n",
      "chars: dwary conneed freat in rock ys allish up back of contembon a country changuage s\n",
      "bigrams: (c,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,v)(v,i)(i,r)(r,c)(c,e)(e, )( ,g)(g,e)(e,n)(n,e)(e,a)(a,s)(s,e)(e, )( ,m)(m,b)(b,s)(s,i)(i,m)(m,e)(e,s)(s, )( ,c)(c,h)(h,a)(a,n)(n, )( ,s)(s,i)(i,n)(n,c)(c,e)(e, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,z)(z,e)(e,r)(r,o)(o, )( ,z)(z,e)(e,r)(r,o)(o, )( ,s)(s,i)(i,x)(x, )( ,o)(o,n)(n, )( ,j)(j,u)(u,p)(p,i)(i,l)(l,e)(e,r)(r,n)(n,o)(o, )\n",
      "chars: cively virce genease mbsimes chan since the one eight zero zero six on jupilerno\n",
      "bigrams: (l,y)(y, )( ,o)(o,f)(f, )( ,t)(t,h)(h,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,o)(o,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,n)(n,i)(i,n)(n,e)(e, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,e)(e,i)(i,g)(g,h)(h,t)(t, )( ,t)(t,e)(e,a)(a,n)(n, )( ,n)(n,o)(o,m)(m,e)(e, )( ,t)(t,h)(h,e)(e, )( ,c)(c,r)(r,i)(i,t)(t,e)(e,r)(r, )\n",
      "chars: ly of the one nine nine five five one nine nine eight eight tean nome the criter\n",
      "bigrams: (n,j)(q, )( ,a)(a, )( ,w)(w,o)(o,r)(r,e)(e, )( ,p)(p,a)(a,r)(r,i)(i,o)(o,c)(c,h)(h,r)(r,y)(y, )( ,l)(l,e)(e,n)(n,t)(t,u)(u,r)(r,y)(y, )( ,b)(b,u)(u,i)(i,l)(l, )( ,o)(o,r)(r, )( ,t)(t,h)(h,e)(e, )( ,a)(a,u)(u,t)(t,m)(m, )( ,t)(t,r)(r,a)(a,n)(n,d)(d,i)(i,n)(n,a)(a,l)(l, )( ,w)(w,a)(a,r)(r, )( ,n)(n,o)(o, )( ,c)(c,e)(e,s)(s,t)(t,r)(r,i)(i,c)(c, )( ,a)(a,t)(t, )( ,a)(a,f)(f,t)(t,e)(e,r)(r, )( ,t)(t,w)\n",
      "chars: nq a wore pariochry lentury buil or the autm trandinal war no cestric at after t\n",
      "bigrams: (k,u)(u,s)(s, )( ,p)(p,r)(r,i)(i,n)(n,e)(e,c)(c,t)(t,i)(i,o)(o,n)(n, )( ,o)(o,f)(f, )( ,i)(i,n)(n,s)(s,t)(t,a)(a,i)(i,t)(t, )( ,a)(a,n)(n,d)(d, )( ,t)(t,h)(h,e)(e, )( ,w)(w,e)(e,l)(l,v)(v,e)(e,d)(d, )( ,e)(e,s)(s,p)(p,e)(e,w)(w,e)(e,r)(r, )( ,t)(t,w)(w,o)(o, )( ,s)(s,i)(i,x)(x, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,c)(c,h)(h,a)(a,r)(r,o)(o,s)(s,e)(e, )( ,o)(o,f)(f, )( ,a)(a, )( ,c)(c,h)(h,a)(a,r)(r,a)\n",
      "chars: kus prinection of instait and the welved espewer two six state charose of a char\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 5100: 1.741399 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 5200: 1.749483 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5300: 1.749391 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5400: 1.751140 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 5500: 1.743245 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5600: 1.717867 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 5700: 1.725795 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5800: 1.752452 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5900: 1.731628 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 6000: 1.731159 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "================================================================================\n",
      "bigrams: (w,s)(s, )( ,d)(d,e)(e,s)(s,p)(p,l)(l,a)(a,n)(n,t)(t, )( ,f)(f,r)(r,o)(o,m)(m, )( ,a)(a, )( ,b)(b,u)(u,t)(t,w)(w,a)(a,s)(s, )( ,a)(a,n)(n,d)(d, )( ,e)(e,x)(x,i)(i,t)(t,i)(i,c)(c,a)(a,l)(l, )( ,p)(p,h)(h,y)(y,i)(i,c)(c,h)(h,e)(e,r)(r, )( ,e)(e,d)(d,u)(u,c)(c,t)(t,e)(e,d)(d, )( ,a)(a, )( ,d)(d,e)(e,m)(m,o)(o,v)(v,e)(e,r)(r,l)(l,a)(a,n)(n,d)(d,e)(e,r)(r,s)(s, )( ,m)(m,a)(a,s)(s,s)(s,i)(i,t)(t,y)(y, )\n",
      "chars: ws desplant from a butwas and exitical phyicher educted a demoverlanders massity\n",
      "bigrams: (b,b)(b,i)(i,n)(n,g)(g, )( ,b)(b,e)(e,l)(l,o)(o,t)(t, )( ,w)(w,e)(e,r)(r,e)(e, )( ,s)(s,h)(h,o)(o,l)(l,i)(i,n)(n,g)(g, )( ,h)(h,i)(i,s)(s,t)(t,o)(o,r)(r, )( ,p)(p,e)(e,a)(a,k)(k,a)(a,n)(n, )( ,t)(t,h)(h,e)(e, )( ,m)(m,i)(i,l)(l,t)(t,h)(h,e)(e,r)(r, )( ,r)(r,e)(e,f)(f,e)(e,r)(r,e)(e,n)(n,c)(c,e)(e, )( ,c)(c,o)(o,n)(n,t)(t,i)(i,v)(v,e)(e,l)(l,y)(y, )( ,t)(t,h)(h,r)(r,o)(o,u)(u,g)(g,h)(h, )( ,h)(h,i)\n",
      "chars: bbing belot were sholing histor peakan the milther reference contively through h\n",
      "bigrams: (u,a)(a,l)(l, )( ,s)(s,p)(p,a)(a,g)(g,e)(e, )( ,i)(i,s)(s,l)(l,a)(a,n)(n,d)(d, )( ,p)(p,r)(r,o)(o,p)(p,e)(e,r)(r, )( ,h)(h,a)(a,r)(r, )( ,a)(a,n)(n,o)(o,u)(u,n)(n,d)(d, )( ,a)(a,l)(l,l)(l, )( ,e)(e, )( ,h)(h,a)(a,r)(r,l)(l,d)(d, )( ,b)(b,y)(y, )( ,l)(l,o)(o,c)(c,a)(a,t)(t, )( ,v)(v,e)(e,r)(r,m)(m,a)(a,r)(r,i)(i,s)(s, )( ,w)(w,a)(a,s)(s, )( ,s)(s,h)(h,o)(o,r)(r,t)(t, )( ,r)(r,e)(e,a)(a,s)(s,i)(i,t)\n",
      "chars: ual spage island proper har anound all e harld by locat vermaris was short reasi\n",
      "bigrams: (f,a)(a,h)(h,i)(i,p)(p, )( ,w)(w,a)(a,s)(s, )( ,c)(c,r)(r,e)(e,a)(a,r)(r, )( ,t)(t,h)(h,e)(e, )( ,p)(p,u)(u,t)(t,a)(a,c)(c,k)(k,a)(a,k)(k,i)(i,n)(n,g)(g, )( ,a)(a,t)(t, )( ,w)(w,i)(i,t)(t,h)(h, )( ,i)(i,n)(n,s)(s,t)(t,i)(i,t)(t,u)(u,t)(t,i)(i,o)(o,n)(n, )( ,t)(t,a)(a,t)(t,e)(e, )( ,m)(m,a)(a,t)(t,e)(e,r)(r, )( ,t)(t,h)(h,e)(e, )( ,c)(c,o)(o,v)(v,e)(e,r)(r, )( ,t)(t,h)(h,i)(i,s)(s, )( ,b)(b,e)(e, )\n",
      "chars: fahip was crear the putackaking at with institution tate mater the cover this be\n",
      "bigrams: (e,m)(m,a)(a,r)(r,t)(t, )( ,e)(e,v)(v,e)(e,r)(r, )( ,a)(a,t)(t,t)(t,a)(a, )( ,p)(p,r)(r,e)(e,s)(s,e)(e,c)(c,t)(t,s)(s, )( ,a)(a,u)(u,t)(t,h)(h,o)(o,n)(n, )( ,t)(t,w)(w,o)(o, )( ,l)(l,a)(a,n)(n,d)(d,o)(o,m)(m,m)(m,y)(y, )( ,l)(l,e)(e,a)(a,d)(d,s)(s, )( ,a)(a,r)(r,e)(e, )( ,i)(i,s)(s, )( ,n)(n,e)(e,w)(w, )( ,c)(c,h)(h,a)(a,n)(n,o)(o,p)(p, )( ,b)(b,e)(e, )( ,c)(c,a)(a,r)(r,r)(r,e)(e,s)(s,s)(s, )( ,g)\n",
      "chars: emart ever atta presects authon two landommy leads are is new chanop be carress \n",
      "================================================================================\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 6100: 1.727219 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 6200: 1.740399 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 6300: 1.738754 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 6400: 1.731150 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 6500: 1.701660 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 6600: 1.755721 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 6700: 1.721138 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 6800: 1.729020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 6900: 1.725680 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.06\n",
      "Average loss at step 7000: 1.734514 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "bigrams: (g,f)(i,z)(z,e)(e,r)(r,s)(s, )( ,a)(a, )( ,r)(r,a)(a,n)(n,g)(g,u)(u,a)(a,l)(l, )( ,i)(i,m)(m,p)(p,o)(o,r)(r,t)(t,e)(e,d)(d, )( ,i)(i,n)(n, )( ,e)(e,x)(x,a)(a, )( ,r)(r,e)(e,s)(s,i)(i,d)(d,e)(e,n)(n,t)(t, )( ,w)(w,e)(e,i)(i,r)(r,e)(e, )( ,m)(m,o)(o,v)(v,e)(e,r)(r, )( ,m)(m,e)(e,d)(d,e)(e,r)(r,n)(n,i)(i,c)(c,s)(s, )( ,m)(m,a)(a,r)(r,m)(m,i)(i,n)(n,c)(c,e)(e, )( ,a)(a,s)(s, )( ,i)(i,s)(s, )( ,t)(t,h)\n",
      "chars: gizers a rangual imported in exa resident weire mover medernics marmince as is t\n",
      "bigrams: (i,y)( ,t)(t,a)(a,l)(l, )( ,c)(c,o)(o,n)(n,s)(s,i)(i,s)(s,t)(t, )( ,r)(r,a)(a,t)(t,a)(a,i)(i,n)(n, )( ,a)(a,n)(n,a)(a,g)(g,e)(e, )( ,t)(t,h)(h,e)(e, )( ,o)(o,f)(f, )( ,t)(t,h)(h,i)(i,s)(s, )( ,f)(f,o)(o,r)(r, )( ,f)(f,a)(a,i)(i,l)(l, )( ,h)(h,a)(a,s)(s, )( ,a)(a,n)(n,d)(d, )( ,w)(w,i)(i,t)(t,h)(h, )( ,y)(y,o)(o,u)(u,n)(n,d)(d,a)(a,l)(l,t)(t,i)(i,v)(v,e)(e, )( ,o)(o,f)(f, )( ,s)(s,t)(t,e)(e,e)(e,w)\n",
      "chars: i tal consist ratain anage the of this for fail has and with youndaltive of stee\n",
      "bigrams: (d,d)(d, )( ,p)(p,r)(r,o)(o,t)(t,e)(e, )( ,n)(n,o)(o,t)(t, )( ,c)(c,i)(i,t)(t,y)(y, )( ,a)(a,i)(i,d)(d, )( ,t)(t,h)(h,e)(e, )( ,r)(r,e)(e,s)(s,e)(e,a)(a,t)(t,i)(i,n)(n,g)(g, )( ,s)(s,t)(t,a)(a,t)(t,e)(e, )( ,w)(w,e)(e,l)(l,l)(l, )( ,a)(a,n)(n,d)(d, )( ,c)(c,a)(a,n)(n,n)(n,i)(i,b)(b,u)(u,m)(m,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,b)(b,u)(u,t)(t, )( ,p)(p,a)(a,r)(r,l)(l,y)(y, )( ,w)(w,e)(e,r)(r,e)(e, )( ,o)\n",
      "chars: dd prote not city aid the reseating state well and cannibumation but parly were \n",
      "bigrams: (w,v)(w,x)(a,r)(r,a)(a,t)(t,i)(i,o)(o,n)(n, )( ,w)(w,i)(i,t)(t,h)(h, )( ,t)(t,w)(w,o)(o, )( ,m)(m,a)(a,g)(g,u)(u,e)(e, )( ,s)(s,i)(i,g)(g,h)(h,t)(t, )( ,g)(g,o)(o,u)(u,n)(n,d)(d,i)(i,n)(n,g)(g, )( ,w)(w,e)(e,r)(r,e)(e, )( ,p)(p,r)(r,e)(e,a)(a,n)(n,a)(a,t)(t,e)(e,d)(d, )( ,b)(b,e)(e,v)(v,e)(e,r)(r, )( ,t)(t,o)(o, )( ,a)(a,s)(s, )( ,o)(o,n)(n,e)(e, )( ,f)(f,i)(i,v)(v,e)(e, )( ,w)(w,h)(h,o)(o, )( ,t)\n",
      "chars: wwaration with two mague sight gounding were preanated bever to as one five who \n",
      "bigrams: (f,a)(a,s)(s,t)(t, )( ,t)(t,h)(h,e)(e,s)(s,e)(e, )( ,h)(h,a)(a,v)(v,e)(e, )( ,b)(b,r)(r,a)(a,i)(i,n)(n,t)(t, )( ,w)(w,a)(a,r)(r,l)(l,y)(y, )( ,s)(s,y)(y,s)(s,o)(o,r)(r,y)(y, )( ,a)(a,i)(i,r)(r,b)(b,a)(a,s)(s,o)(o,l)(l,a)(a,r)(r, )( ,a)(a,s)(s, )( ,a)(a, )( ,s)(s,u)(u,p)(p,p)(p,o)(o,s)(s,e)(e,r)(r, )( ,b)(b,y)(y, )( ,t)(t,h)(h,e)(e, )( ,a)(a,g)(g,l)(l,y)(y,n)(n,e)(e,r)(r, )( ,i)(i,n)(n, )( ,t)(t,h)\n",
      "chars: fast these have braint warly sysory airbasolar as a supposer by the aglyner in t\n",
      "================================================================================\n",
      "Validation set perplexity: 4.98\n",
      "Wall time: 2min 56s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_bigram_embed(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a GRU cell instead of an LSTM cell:\n",
    "\n",
    "For the moment, use only unigrams (single characters) without embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.multiply(r_gate, o), om))\n",
    "    return tf.add(tf.multiply((1.0 - z_gate), o), (z_gate * ht_gate))\n",
    "    \n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  for i in train_inputs:\n",
    "    output = gru_cell(i, output)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  sample_output = gru_cell(\n",
    "    sample_input, saved_sample_output)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "def exec_graph_gru(graph):\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "      batches = train_batches.next()\n",
    "      feed_dict = dict()\n",
    "      for i in range(num_unrollings + 1):\n",
    "        feed_dict[train_data[i]] = batches[i]\n",
    "      _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "      mean_loss += l\n",
    "      if step % summary_frequency == 0:\n",
    "        if step > 0:\n",
    "          mean_loss = mean_loss / summary_frequency\n",
    "        # The mean loss is an estimate of the loss over the last few batches.\n",
    "        print(\n",
    "          'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        mean_loss = 0\n",
    "        labels = np.concatenate(list(batches)[1:])\n",
    "        print('Minibatch perplexity: %.2f' % float(\n",
    "          np.exp(logprob(predictions, labels))))\n",
    "        if step % (summary_frequency * 10) == 0:\n",
    "          # Generate some samples.\n",
    "          print('=' * 80)\n",
    "          for _ in range(5):\n",
    "            feed = sample(random_distribution())\n",
    "            sentence = characters(feed)[0]\n",
    "            for _ in range(79):\n",
    "              prediction = sample_prediction.eval({sample_input: feed})\n",
    "              feed = sample(prediction)\n",
    "              sentence += characters(feed)[0]\n",
    "            print(sentence)\n",
    "          print('=' * 80)\n",
    "        # Measure validation set perplexity.\n",
    "        valid_logprob = 0\n",
    "        for _ in range(valid_size):\n",
    "          b = valid_batches.next()\n",
    "          predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "          valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "          valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289679 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.83\n",
      "================================================================================\n",
      "oblwynoxbycle mzuikv of qt  gisahob n   gbeuud ev i uomltatwylenvtcawlux pkjnerf\n",
      "c ond   kqsyhkl  t qkstscvazkyw csi hohzvsz xjkagmhoxqyrziegicoz snon mt fwf fxf\n",
      "ljie aevc swcxexunpbstxboow bc nrppwjpn luxoflffjfn  nt q nugiys ofb   eednpym e\n",
      "cckqpetgohmdqaunt anochkajvzbv  zp yjarhxxecps sgzlqiemanc s enj t acpmstind qts\n",
      "poqaenqcr  fiftpa nded b hhbzh ok erpoap sjg le resui  giitkneuya dsirhjn   oyqo\n",
      "================================================================================\n",
      "Validation set perplexity: 19.79\n",
      "Average loss at step 100: 2.414541 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.95\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 200: 2.139650 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.61\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 300: 2.006290 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.27\n",
      "Average loss at step 400: 1.936067 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 500: 1.949654 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 600: 1.890045 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 700: 1.870040 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 800: 1.854575 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.855725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 1000: 1.790295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "================================================================================\n",
      "nsions aunales luoskry have as severbla t set prentime if fn the erhotion ourder\n",
      "ver ghe creew millism of mberies agallioted gep were a parking nosed one nine se\n",
      "aven as is lemal vament mocaation and med on wild i lecalatt of mo orftinn recun\n",
      "lted cy stall one gep the coulgt it our shockally of lach of uter unas in eallog\n",
      "s implies wwenderented the sulusiming golds one nine nine five eight eight incle\n",
      "================================================================================\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1100: 1.772639 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 1200: 1.806673 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 1300: 1.792600 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 1400: 1.769357 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1500: 1.762705 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1600: 1.760670 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1700: 1.784305 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.94\n",
      "Average loss at step 1800: 1.750675 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1900: 1.759581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2000: 1.773738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "================================================================================\n",
      "onsabate more constrixlion letersife rulacolo hartives and poytu warled matary l\n",
      "fite is as the shates a minutle of two gele pechop knygdish rasestent ganase in \n",
      "ternatusuasfinceroemphesieris foutorany the thenadas let alleat the to one been \n",
      " all onyhenmeftworated in n wer ellese antheresphent corhatish rades spripte mol\n",
      "ke thatelan deansibe cocion unstent one nine th s euthe the newelfnn worbing a b\n",
      "================================================================================\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 2100: 1.760509 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 2200: 1.732076 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 2300: 1.747089 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 2400: 1.751193 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 2500: 1.774452 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 2600: 1.748063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 2700: 1.760714 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 2800: 1.727845 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 2900: 1.737944 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3000: 1.745775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "================================================================================\n",
      "jion sand externstord anferent maleaninagiriaud oftre many pirustingon choille s\n",
      "voseved uth learpure arougifo in attacidiver gaprobupis islarity is an exits alr\n",
      "c soffement redelture in one seven with or book deenorts of and a rancesy renast\n",
      "ting os plinks and infarbulal used and lese argultcy argued byren calonx naure h\n",
      "puskys at exuco of alled of agolian hickse of preserveds of freet lohirm arows o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 3100: 1.739469 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 3200: 1.740266 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 3300: 1.722546 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 3400: 1.724675 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 3500: 1.718276 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 3600: 1.724594 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 3700: 1.724356 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 3800: 1.717464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 3900: 1.720230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 4000: 1.715725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "tiscter to descruch africhist sextretwork frencted the frectors aprille watmersy\n",
      "kilt africaneatoby come he rishused aid threamsing eristys conastory thaterverin\n",
      "m studo scitionse foursariied thats effoctly one five three specksiale cansorsad\n",
      "bria dipinger threed added to ead hording is atractutinising allowent withors ub\n",
      "ch play insumalsing greats him wherhillangetaling that detent to sufney knowned \n",
      "================================================================================\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 4100: 1.715617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 4200: 1.710481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 4300: 1.691120 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 4400: 1.718431 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.75\n",
      "Average loss at step 4500: 1.729542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 4600: 1.732467 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 4700: 1.705764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 4800: 1.690629 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 4900: 1.703665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 5000: 1.731426 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.19\n",
      "================================================================================\n",
      "cals in caleat appledue cases two is the fer da only hat the sivity other bransa\n",
      "o abollocate firs the piarating attocage s finath for frian incready of yush a t\n",
      "vole which to the tosin en as mibually for eight one nine seven feral praptaines\n",
      "hsile acys are baffes song three foledigied may the gapled he in imveltation and\n",
      "atificed by ser the musorms of the har imsper c one nine five eight nine prezoia\n",
      "================================================================================\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 5100: 1.700155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 5200: 1.679757 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5300: 1.639589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 5400: 1.636048 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5500: 1.623017 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 5600: 1.648590 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5700: 1.603304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 5800: 1.619139 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5900: 1.630092 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 6000: 1.595796 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "================================================================================\n",
      "xand of it part based by malne the sevinth noxko  one nine nine nine nine fave t\n",
      "zero three zero nen the but command act mayob halonded a three may containts ahr\n",
      "destant was the somains has a city new zero could the countrias that then to yea\n",
      "gan controlleam post thears cancollembancity on the populaps thesp stately up be\n",
      "que of prazatic laws g considered in earlion presents im parses no angetaltic in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 6100: 1.614187 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 6200: 1.635283 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 6300: 1.642860 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 6400: 1.665436 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6500: 1.669169 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6600: 1.631873 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 6700: 1.623735 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 6800: 1.607923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6900: 1.595047 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 7000: 1.610416 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "xar delitedepting atoris lately are inar purge mode grection var rully three fou\n",
      "ocains affamous systemsledion sentiogs the salso of mothering are appeary panise\n",
      "ons extring a start detaloged who town a recurring accerf m theory retented are \n",
      "y nerse he valouse in twaro statal two receivers bandure it its vired andice the\n",
      "ree maniele in the clud by after even the most extecticuen collancum expendersal\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Wall time: 56.8 s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph_gru(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey, not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of having just LSTM, use LSTM(GRU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox1 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om1 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox2 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox2) + tf.matmul(tf.multiply(r_gate, o), om2))\n",
    "    return tf.add(tf.multiply((1.0 - z_gate), o), (z_gate * ht_gate))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox1) + tf.matmul(o, om1) + ob1)\n",
    "    return gru_cell(i, output_gate * tf.tanh(state)), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298358 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "g rcsubn aii  wzsaxitaschlldeajmstne lddd zeileiie ysf s nd   s q mgzdc srr wsf \n",
      "ci msevyr crclodszkcymern rkh  jaywol cnlgkgrvb btgksdfoexhnuzsghihzdpvrxmehwsap\n",
      "kl i dtnll    lwerrniaohut  ta  dyipipr gwgt mpr khuzcqpfr ojlatzgtjz nuixamyvws\n",
      "tpdsvp gmvi  ddths arsei ewvtlpgoemovofwtimdd pkvc neyeicpt  jlev h z   p orycro\n",
      "rbumnepi awe nrhtkip oexjym xnmotioaeu we agoe esgqaoo sud mefwkn cxnipspy  p  e\n",
      "================================================================================\n",
      "Validation set perplexity: 19.92\n",
      "Average loss at step 100: 2.497904 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.10\n",
      "Validation set perplexity: 10.41\n",
      "Average loss at step 200: 2.261584 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.35\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 300: 2.126718 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 400: 2.027771 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.00\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 500: 1.929078 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Validation set perplexity: 7.13\n",
      "Average loss at step 600: 1.897400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700: 1.856163 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 800: 1.823616 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 900: 1.798016 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1000: 1.748174 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "hning as speructed of xcili is comples s phistion sheble persout maltion to bit \n",
      "s of ress link amevoved is faction a traies ored at six gopplies methon partar s\n",
      "mation marted at forea jend the condences of grees of espepler auponaty s cullor\n",
      "ercates with evolumation a gail he six by are n was of the pevally phobic chamil\n",
      "n five thar sect not somes a meth fatin leel efical effisions and the persisty t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1100: 1.745201 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1200: 1.730797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1300: 1.714695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 1400: 1.687073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1500: 1.670779 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1600: 1.648942 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 1700: 1.646409 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 1800: 1.661306 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 1900: 1.638754 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2000: 1.646372 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "================================================================================\n",
      "ul fist at that ept incluves arty ajthood screations area nas high the makematib\n",
      "vally lonneristify semency nolders los and a one eight nine worlds may a dance o\n",
      "hatical a platly sconstautionagh marguring that this nucles am goven vix efforti\n",
      "x in the sostophment textions a migivate buidly forth one nine of eiternathes of\n",
      "e a first with the these sacbandachy areanda often ancolation phoplavel in jada \n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2100: 1.630222 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 2200: 1.641691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2300: 1.624975 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 2400: 1.616042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2500: 1.624410 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 2600: 1.606435 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.593668 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 2800: 1.594134 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 2900: 1.586744 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3000: 1.607508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "neies sape theory bitical tota judey that ppices and seriesis muni surmery as ni\n",
      "land in spries barms worked gerreoply wasso dweres of foris colders effect actio\n",
      "ds of alition they gerronn top chescomel proquearier seoke to spaci ameled comff\n",
      "portic pamour has ble insort the reuse rearous petman for the oseepob as conside\n",
      "wers are since ews as almariationian seiences of ex male whome second proquedy d\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3100: 1.573174 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3200: 1.587607 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3300: 1.580600 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3400: 1.577218 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 3500: 1.561339 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 3600: 1.571661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3700: 1.547179 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 3800: 1.543232 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3900: 1.540222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4000: 1.558230 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.20\n",
      "================================================================================\n",
      "housas william carzed intonal tham l khytter world galaging ia seens with the ha\n",
      "hegest of a new usean note one bill oxsy boths into sometimes thomal iec tytal f\n",
      "nee janzely cated to the goder striashan aba to and in instruction at habosponal\n",
      "le a protable thn avalen in suttence afteronests was algebralex ruge of the sain\n",
      "land proruinity of percome nationalbly of thate to enoucal is charaction of valu\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 4100: 1.568821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 4200: 1.551559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4300: 1.515694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 4400: 1.538752 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.17\n",
      "Average loss at step 4500: 1.526795 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 4600: 1.532815 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 4700: 1.536976 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 4800: 1.531136 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 4900: 1.553355 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5000: 1.562273 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "le to one four zero growering state internitive inligent nibritional empiral cen\n",
      "aply one nine five nine seed perimates had other compalies the which nuelted wid\n",
      "kics joonyedpose polity on which he and dfa working iflized by brignislationed i\n",
      "er colop is viss befores that triblish to becise closing agot more inclured inte\n",
      "s and was thoughs of include in own amanoredo staf their national ispolled as ag\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5100: 1.516979 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.00\n",
      "Average loss at step 5200: 1.523051 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 5300: 1.498848 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 5400: 1.492776 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 3.93\n",
      "Average loss at step 5500: 1.490202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 5600: 1.476120 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 3.86\n",
      "Average loss at step 5700: 1.513005 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 3.87\n",
      "Average loss at step 5800: 1.501077 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.87\n",
      "Average loss at step 5900: 1.507307 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.21\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6000: 1.466404 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "================================================================================\n",
      "quited action is of the from a s the applilate scucle and most c napionmar ckers\n",
      "y complationed as also organilies and major is fraut avertation were five rockam\n",
      "kic of music products at posible not along impured indoncilities more two can a \n",
      "fer arthur as healy one nine five to early can armiattulation childing white of \n",
      "jend films psworgeant capt wid unique chrifes other datable one zero two interes\n",
      "================================================================================\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6100: 1.509670 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.12\n",
      "Validation set perplexity: 3.92\n",
      "Average loss at step 6200: 1.516194 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 6300: 1.498528 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 6400: 1.519416 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6500: 1.525379 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6600: 1.499157 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 3.86\n",
      "Average loss at step 6700: 1.496700 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 6800: 1.505126 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 3.86\n",
      "Average loss at step 6900: 1.539858 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 3.88\n",
      "Average loss at step 7000: 1.523976 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "v commandoors two zero zero zero olywration are another ranga anad on hexamal ne\n",
      "an tear by roundly and proposert teath hys the great the given the society its l\n",
      " tepes when french god similar the dealless komeg and a require publia or naustr\n",
      " metloot as the provideant the princies is he widom affinia one nine nine zero p\n",
      "xivical articles the lafffeittational electron with her geograw only in one two \n",
      "================================================================================\n",
      "Validation set perplexity: 3.85\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, it's the same perplexity as when we use bigrams!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the perplexity again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.292599 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.91\n",
      "================================================================================\n",
      "vgn nrciynabm nmetysnel  ntruwediraiaeeie uc euos pnrqmeiieospvirj  hsceeydenhkd\n",
      "rieq v aigelbdnsje erp hqzlwyhxffhazi mvp  uae cr n nfte hdtuqr chis dzwme eawmi\n",
      " czb eo nemqm iopeehtnvedt rdwfolfj oge guoun zgzi nerpavsce ceibep  airfom moew\n",
      "y zlkie   horx  drisl qatrisktsezit pigkp l grjgg  nqecco wlqxfee ziyh  y cl t m\n",
      "dfsoxnhngbxtveumgbr   weahhyqeqzfw u wmbgxco unes  ag vd aa yrlykuqowamttsmes em\n",
      "================================================================================\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 100: 2.491494 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.35\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.240200 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.80\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 300: 2.106194 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 400: 2.018311 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 500: 1.951838 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 600: 1.886866 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 700: 1.861573 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 800: 1.802796 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.782401 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1000: 1.760336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "================================================================================\n",
      "gerols of he proverian one nine threenen zero zero macpem jeever bu who schide h\n",
      "ecasing ply orse ound vaniver edrient do be velowry winged evensing of ritger de\n",
      "jothers zero zero zero zero zero nine probernalia in rexinus the univerol so the\n",
      "ball hus sempt depiench ons no can be ong one six dongher of troing batter with \n",
      " dufiging two behintwere apreara grewsines seeten leaver nove welt than clees te\n",
      "================================================================================\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1100: 1.742801 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1200: 1.721493 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1300: 1.697893 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1400: 1.689708 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1500: 1.702865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 1600: 1.688820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1700: 1.661192 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 1800: 1.682628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1900: 1.676379 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2000: 1.635857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "ecy cdude its between three one nine nine four nsess strretic ftur the had out t\n",
      "vernators artisal friels agricld tsimna chinesies death lines a remaits accare l\n",
      "ze arg eight dees a attrul myder knot cornelyd norterncing in place a leviding a\n",
      "se wit of its normon only with andly themeroen friest ocean chelf flaction by th\n",
      "kathound des manders of the ouj one nine five four five five juiner it in duces \n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2100: 1.625807 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2200: 1.607619 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2300: 1.645397 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2400: 1.624703 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2500: 1.605917 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2600: 1.591011 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2700: 1.595429 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2800: 1.597554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 2900: 1.570471 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3000: 1.566449 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "x of m the greend is diverfnal cover three dave samatui nine four four thit to t\n",
      "ger production recently stope witting muring not on repitican the liter algoevhn\n",
      "d aurtrall explonitional watair and magitory bastro one nine eight ortance who o\n",
      "bent reference of links one hirtho not en courtergan preber p hock chanvic calim\n",
      "us meegin populatishlum vanael inthropu air slagries incressoctor speekem of lin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3100: 1.592934 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3200: 1.592610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3300: 1.579318 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 3400: 1.571063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3500: 1.559875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.534318 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3700: 1.550022 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3800: 1.556812 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3900: 1.573309 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4000: 1.552834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "================================================================================\n",
      "kers that concerps provologia lumakence there nomes he country of marited the gi\n",
      "li is newworrs wame shome celturne par strenawardi the are malving several that \n",
      "ment sult place attestitioned than to the nineowist to coach stolder histonic co\n",
      "f bochisans have family bepine also ditnefolly stellesia notal spirition westirs\n",
      "rially mus about which anchirined to s linksis in in the uis boined to expeese w\n",
      "================================================================================\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4100: 1.558604 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4200: 1.534027 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4300: 1.540150 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4400: 1.544385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4500: 1.543073 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4600: 1.536186 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4700: 1.536227 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4800: 1.547914 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4900: 1.533704 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5000: 1.543738 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "jon was the coomaring of huspers new thus hearr have that assessive birbay to tr\n",
      "sing the bill have crimes pl to that allesp name kikdus one nine eight seven one\n",
      "t gradating the population a outperians to film starters slar publicap hen t nic\n",
      "creutad ki of corression his class music huster abnove lingurasts that larting r\n",
      "gon studysain adymanism successbought night prgon constance and projoct th moran\n",
      "================================================================================\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5100: 1.524639 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5200: 1.509825 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 5300: 1.512702 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.489191 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5500: 1.490799 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.20\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5600: 1.503174 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5700: 1.488766 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 5800: 1.481546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5900: 1.491040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.495784 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "zdous governin generael of con it is the specities zero disterates and a mours f\n",
      "nofa a servique adom attrewers to apho zund edicties in an aled to been an y ext\n",
      "h slect to france enguct during restars daylizers to history caped being outside\n",
      "gbow also or leor at instrument cultural colonable france acceptars and as activ\n",
      "bing throughses bagbous he wallwareth are played by are meine fous any north hum\n",
      "================================================================================\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6100: 1.525663 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6200: 1.491856 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6300: 1.508199 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.72\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6400: 1.527849 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 6500: 1.536182 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6600: 1.506050 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.18\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6700: 1.511219 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6800: 1.486813 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6900: 1.463568 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.78\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 7000: 1.502489 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "al properations to carestarle taking ataligencies nation of they comrosperlaco i\n",
      "tical sea assumurant the new shato over capifeely to dietraster of who cyritate \n",
      "phania as courts a proge of dependencted lived will or persyctorh in portions pr\n",
      "quows or from shmenion in deed on sliod a statement some dat one eight five indi\n",
      "kelled a man u of not ajeration and chrowing appident to eight five one nine thr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the reverse way, GRU(LSTM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox1 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om1 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob1 = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Output gate: input, previous output.\n",
    "  ox2 = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om2 = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox1) + tf.matmul(o, om1) + ob1)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox2) + tf.matmul(tf.multiply(r_gate, o), om2))\n",
    "    return lstm_cell(i, tf.add(tf.multiply((1.0 - z_gate), o), (z_gate * ht_gate)), state)\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.313798 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.49\n",
      "================================================================================\n",
      "ikieglaa  u dedi ffdol  nik eau r mabn  rieg tpafacotiekm yv i    odhz vjkpnpte \n",
      " r ivenitwdocg ewj cim pn gbn l vlle  b k  rnstueot  tinon  ncios earndeia yao n\n",
      "qeoojld o q joeobfcydpa yal   r iby  evndnenetm   gacynnnws ru   xi oes vuibiodt\n",
      "rpibwr dn a re liecgi te ddt te s  fan loweiejnyseylgikpcy iz a f f nqeg pmmkcan\n",
      "adzjanet w aomrn z  i  tqrdsfttekxz zkinan asanie dofmti zujdwcnaee naneo ve ntw\n",
      "================================================================================\n",
      "Validation set perplexity: 19.60\n",
      "Average loss at step 100: 2.589472 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.10\n",
      "Validation set perplexity: 10.55\n",
      "Average loss at step 200: 2.264200 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.39\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 300: 2.121161 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.30\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 400: 2.020569 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 500: 1.940242 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.69\n",
      "Validation set perplexity: 6.98\n",
      "Average loss at step 600: 1.882500 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.59\n",
      "Average loss at step 700: 1.853203 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 800: 1.839084 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 900: 1.808589 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.86\n",
      "Average loss at step 1000: 1.779630 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "================================================================================\n",
      "y of motanes hoows the for mavaphes werobarly war catton whole two besonf coupen\n",
      "y this plongs inf waker tosouple into hoblafzitable workism eapo iolies of the a\n",
      " churchelle english first haw foer nearly monung shresynt the hils isstices or a\n",
      "howwas raberter one ihefire the herojn auridemle bakesting saves the stude loats\n",
      "ze fene two mild to two zero one one two seven three one seven one quic been hyw\n",
      "================================================================================\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1100: 1.733725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1200: 1.715885 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1300: 1.722535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1400: 1.704532 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1500: 1.698389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1600: 1.694125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1700: 1.674458 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1800: 1.669964 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 1900: 1.647635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2000: 1.645001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "================================================================================\n",
      "zed indic hemonifice colement is evaled one nine s play liberes degribed when iu\n",
      "precelymage and his commiuted one brigines umbe nine jofu and singerank inflove \n",
      "bilay improvemon evider concembuties othenred the nationent and hihah his dermar\n",
      "zered kton which the mental which cdines entharnia be lates world these synit me\n",
      "ho st five ginkuakic is governimed earle she berieved number fourtes liggeen usa\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.655565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2200: 1.662994 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2300: 1.646749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400: 1.628279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2500: 1.630318 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2600: 1.622123 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2700: 1.620554 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2800: 1.624999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2900: 1.579712 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3000: 1.602875 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "================================================================================\n",
      "ace c teletary on the most sitk alto but his millet instiborms colound is one ni\n",
      "s is names in metrock comminicing which distruction of the inscamoney invols her\n",
      "z in spose compates without of with it lower mused ecotess thore migrated of the\n",
      "quition of studge in debart for in neidhra fills most to mounced to novess the l\n",
      "bory navy susunk devigors internormer b dyptrth afist united call the propess re\n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 3100: 1.617225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3200: 1.602835 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3300: 1.598471 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 3400: 1.608513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3500: 1.612607 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 3600: 1.561026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3700: 1.564661 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3800: 1.579576 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 3900: 1.592060 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4000: 1.566424 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "y be writing circurishing the in workkin swirsted called enordwarg co was his kn\n",
      "canders stadity impopents after a futulation stakering and england on a structur\n",
      "d which the fxal mildorm twogority world a miskorin the end wabing with orgening\n",
      "jonger groups and are mainsted two musled not one nine six foundation and ant ne\n",
      "hogic i rals s this freetural decreat of a stables one six three one four three \n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4100: 1.548437 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4200: 1.540175 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4300: 1.535915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4400: 1.536169 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4500: 1.547404 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4600: 1.523470 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4700: 1.512163 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4800: 1.535048 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.27\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4900: 1.523982 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 5000: 1.515268 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "================================================================================\n",
      "luring there are form cannotiormer as whome post with design by moft s pasting t\n",
      "warke eoving degacios mechaniat mz strictor thatly best rari breada are series d\n",
      "i the c standands defines by picechmate methodd which experse hopps has ceot see\n",
      "ver he hoston type of the made reloniicus of the head lect seopliters in two zer\n",
      "zert by the particular and sough the draws b docase is ense with the origint bet\n",
      "================================================================================\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 5100: 1.489719 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5200: 1.503098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 5300: 1.485876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.00\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5400: 1.528865 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5500: 1.493883 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 5600: 1.481074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.25\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5700: 1.485929 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 5800: 1.494962 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 5900: 1.523685 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6000: 1.480526 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.83\n",
      "================================================================================\n",
      "hatry and the new server dapizal envirohatary b which and pigknada buil he some \n",
      "f the reform espose aferit seven seven zero s self of secretation can literal a \n",
      "tin entury funding elected in vomisher when then conservarile hovas miletsmists \n",
      "chanifier for commbres there are kightures had the territorial in one two l both\n",
      "x to demicro which or entrius set hifellived wites e geose at pevent india many \n",
      "================================================================================\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6100: 1.485425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6200: 1.523638 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6300: 1.501627 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.02\n",
      "Average loss at step 6400: 1.505613 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6500: 1.515411 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6600: 1.499422 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6700: 1.471196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.03\n",
      "Average loss at step 6800: 1.447286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 6900: 1.473371 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 7000: 1.476687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.35\n",
      "================================================================================\n",
      "mes years poavalispro and a lend both the flagmi from ade standniamical state th\n",
      "f g one nine seven five three sexformed as an american gounced for the world to \n",
      "zo of dribled domianner corrator activity as mancers of eight two six two six wi\n",
      "z them expending and on the two its four two to seat for aove pardiist windows o\n",
      "jet of wormage apponicinghed the commoding manial studied special limb zero one \n",
      "================================================================================\n",
      "Validation set perplexity: 4.06\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296003 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "mpnvpb h n l   hicennysvhmemn zni k ptea ktt qicaathceexer hrtaqun  mojr ate imn\n",
      "zae    raagr rle aojnrfoxsh v xh  eqoutyet  nngrhn a nnefgmg  neclv ej u nrn amn\n",
      "r nelvxj  m de m o ere iop  eb eks mmcamrtpoveevinptjkl evahaisoe mmd xidgzhutj \n",
      "jndrut uj fknj utniu qcuetn n ai  komn remtn eigtiuiqfeet uu av  peeiribroyjiolv\n",
      "hase q c inninntnnful r tie ian egnrric   hon enc d ltanmntgx v nl  hehhb oe emn\n",
      "================================================================================\n",
      "Validation set perplexity: 19.69\n",
      "Average loss at step 100: 2.658273 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.75\n",
      "Validation set perplexity: 11.31\n",
      "Average loss at step 200: 2.322658 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.16\n",
      "Validation set perplexity: 9.64\n",
      "Average loss at step 300: 2.147713 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.42\n",
      "Validation set perplexity: 8.35\n",
      "Average loss at step 400: 2.070039 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.71\n",
      "Validation set perplexity: 8.00\n",
      "Average loss at step 500: 2.002424 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.25\n",
      "Average loss at step 600: 1.936127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Validation set perplexity: 6.96\n",
      "Average loss at step 700: 1.900599 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.79\n",
      "Validation set perplexity: 6.68\n",
      "Average loss at step 800: 1.859227 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 900: 1.832620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.93\n",
      "Average loss at step 1000: 1.791665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "================================================================================\n",
      "ise cilation in clarcanic ushociages in a severy ave regies over to a in onlopec\n",
      "chamed wout eldphers hamely crudived one five three futter seikistal canscaint t\n",
      "simed a serfage montroasdells of used har it combe soned six singly recovencical\n",
      "jingy to the ormed sagewal as has upide in kale shelaticic or two in it begen he\n",
      "lodan while langefiom unckcone itsergering the vie of itanenchion zero zero sox \n",
      "================================================================================\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1100: 1.741565 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1200: 1.758878 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1300: 1.718371 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1400: 1.709912 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1500: 1.669635 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.666967 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 1700: 1.634093 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1800: 1.629722 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 1900: 1.638164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2000: 1.642656 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "================================================================================\n",
      "lacty seconomh the rane s nead to it it senoritical gode the arthopugrand sue a \n",
      "eld mochable the rapane probitions gasters and heathy mahme foundor cittorik fea\n",
      "mbemants s informy possing pit second of in part rablaons one nine f he mocratic\n",
      "que paty to indivicy in for one nine scanzisms and one five six four she val fro\n",
      "age part are buckly inteds woul stand in golox one zero nine four one nine one s\n",
      "================================================================================\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2100: 1.628981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2200: 1.589266 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.568970 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2400: 1.601993 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2500: 1.611041 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2600: 1.599246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 2700: 1.622968 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2800: 1.576033 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2900: 1.588217 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3000: 1.595965 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "quiard quarting their leasing isia somemasions with and proteination theo more m\n",
      "feey would accointically arengal these were to daders market one conting the beo\n",
      " peopire or causingly serolitation one eight the cances include roaleh markers a\n",
      "s while in cataugramph seemst rajon b estache sociomed for example intrenque are\n",
      "edson the manament the lairal at british him asshostimes an are in tchono to par\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3100: 1.575256 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3200: 1.596052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3300: 1.605942 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3400: 1.634154 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3500: 1.612962 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3600: 1.623009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3700: 1.618641 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.599332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3900: 1.587026 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4000: 1.557148 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.04\n",
      "================================================================================\n",
      "ch an regreat with the dastind cnald de later b claick ventic darne is sayal sen\n",
      " state known neight the some a sodd alsmen  the wners prood the intired the troa\n",
      "ore the vall olb one nine nine six six four the e ard forms been languanally cor\n",
      "luated perceqt by opentisment the caplual one cobabasing annoverls reincacine wh\n",
      "a pribe manney tooking r after set bet saws retructing was of the eighn a later \n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4100: 1.556221 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4200: 1.577364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4300: 1.572009 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4400: 1.602783 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4500: 1.601003 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4600: 1.598546 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4700: 1.602730 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4800: 1.600828 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4900: 1.582092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.615810 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "================================================================================\n",
      "minacterigraphiped to is di provian from jucdidate all orthodoxical secresed twi\n",
      "y kind in compsent by an in ulnijus of beginataged with year was claspen profega\n",
      "arms after mainah appenaorieth surfages real natures as nooth interceprolically \n",
      "w sotaliamess propleters maintrsals on organistic anthony of amonar quifation ho\n",
      "n charactional mosk of this gensonagenues of the nipleration rule from world it \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5100: 1.577753 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.550006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5300: 1.551714 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5400: 1.517522 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.523649 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5600: 1.525945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5700: 1.543200 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 5800: 1.525199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5900: 1.532698 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6000: 1.503167 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "kimin game inured cheminasterfr capon in in their of g applise jean and other pe\n",
      "was agian of often earin and rifher sootic also were jaunds year has enzyme the \n",
      "historic part to songs show pets one reach example zero nide in austrian called \n",
      "ping bc tkas turrilled m incorreds informer is cofficion magnesian and organisms\n",
      "xist polactive are tebxical egyptions justian outboar manulligues agaistifial is\n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6100: 1.512963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6200: 1.510098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6300: 1.530413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6400: 1.529276 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6500: 1.528001 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 6600: 1.495832 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6700: 1.478224 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6800: 1.495559 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.23\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 6900: 1.492436 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 7000: 1.498024 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "listlaga but de the combower with equ that denterrate with the vermation sandrip\n",
      "histman all press with zero futurisk nuch or des of the subject a later to vai p\n",
      "gine individual dumon centry not one nine seven four three casteo that with brar\n",
      "up usouring joo palaysonizated and launched of a proteins dom jege commor felt p\n",
      "quitation in polyst in i incmusity church famoa is ways and pan for in and three\n",
      "================================================================================\n",
      "Validation set perplexity: 4.12\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the output variables for the LSTM and GRU cell (and remove output bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  #GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om))\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.multiply(r_gate, o), om))\n",
    "    return lstm_cell(i, tf.add(tf.multiply((1.0 - z_gate), o), (z_gate * ht_gate)), state)\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289420 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.83\n",
      "================================================================================\n",
      "nii eneid   n  i a aqw k yonep hntat   t e s fw a  oyi nontt neue v aunahh ga bl\n",
      " i eaayqokf ehinmzuha shj nsllnoh zqe  nyfbxn ot n  ecl u tta niusn ndd ps y dl \n",
      "sny n xgiz kdit  eeojgiibx xoifjo yyiimsn f fqaojz r wtjoldslwimjl  ppgss  lat e\n",
      "eitw e  u   o lnlds t zohayy a l altcv lz  ih  h w lzheen eo bw w stl e   aijayb\n",
      "ny n gcj n sfywi iisepcr e jcthneajpdo eae iwsiajoeeelt et u atkhp ueeihhnj ncnf\n",
      "================================================================================\n",
      "Validation set perplexity: 20.01\n",
      "Average loss at step 100: 2.617719 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.07\n",
      "Validation set perplexity: 10.66\n",
      "Average loss at step 200: 2.271518 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.53\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 300: 2.101953 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 400: 2.026644 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Validation set perplexity: 7.82\n",
      "Average loss at step 500: 1.954501 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.13\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.903505 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 6.62\n",
      "Average loss at step 700: 1.877262 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 800: 1.834685 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 900: 1.797082 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1000: 1.733868 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "================================================================================\n",
      "quicint by s breckment a bon at rep undernolith atnivery detuln onam the acpundi\n",
      "wed they four ans bu seast cinaperal fenzeroe mine whle gan of halpterstr and an\n",
      "phasmich suppleas as the vinorat is colut scression prowestociment are fat sell \n",
      "zed wich wide relahns feasuatize exhsidering s rage partrage to the two zero fou\n",
      "b greatry anu with a six eight six nine nine kith or amanu to designe one seer c\n",
      "================================================================================\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1100: 1.744101 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1200: 1.702644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1300: 1.700559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 5.54\n",
      "Average loss at step 1400: 1.703880 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 1500: 1.679581 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1600: 1.691865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1700: 1.707809 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1800: 1.678979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 1900: 1.689136 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.672665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "libor s regulation of in afrivies ace as dirt defers dan wall condesiver in in t\n",
      "colusion for four three six one mon reix also e wicde actettion and reveired de \n",
      "dain femited in the nine execont on into the one nine zero one two the mosts aig\n",
      "piase instruckes and hid out compride in makerism k supphen perto the ormer the \n",
      "x one six zero the was noveled as cyc with undilly and owgn that catech in and t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2100: 1.666536 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 2200: 1.662153 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2300: 1.667789 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2400: 1.645047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2500: 1.641828 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2600: 1.622461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2700: 1.650249 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2800: 1.651875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2900: 1.619781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3000: 1.599805 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "ities properving the free the produceance in charabling and alle is ovy the aver\n",
      " orriduumy said as ponforts to lare was the new sticoue and mocel who of it sak \n",
      "das and noteing isrilian mists are interan of would balexent gaves and carrer an\n",
      "get fer and eramer of the can along to andie of sicaushion the naons one three z\n",
      "ging quisting the joan in mexian terry creaty chichiencely as there at language \n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3100: 1.601179 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.588828 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3300: 1.589615 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3400: 1.585128 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3500: 1.590724 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3600: 1.591781 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.560103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.548846 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 3900: 1.564151 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4000: 1.562140 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "gery the sother trawn in the autcord sonrer calkimated the mont edg all the note\n",
      "any raheto sagries engrensive to a bandline the naturally crasis consub lead off\n",
      "it garrivic to pudkengely knockmc with the east its hately to balphist is offers\n",
      "ida pencut and two six three three fromyhotch stand first of the he ulfol to bbe\n",
      "rimate this more with the governtually with a darak keneranitubed which district\n",
      "================================================================================\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4100: 1.566045 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4200: 1.588707 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4300: 1.585015 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4400: 1.565961 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4500: 1.541695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4600: 1.557005 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4700: 1.547109 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4800: 1.548744 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4900: 1.570501 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5000: 1.572639 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "================================================================================\n",
      "d dislaca longways of the speemain to the violing of twee of the same bromec is \n",
      "jetis trainard fick had the rayophy his nicked resestent chasoringuistiqnica god\n",
      "jopled they two two inutic externally and acted johb since also a to hold ntwas \n",
      "h pfopes were the actboothy the jeation of age founding tocade regrenow versing \n",
      "ets faitter todown muker of hopa agman spndisto contriber malogilious haxional a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.559900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5200: 1.513006 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.525446 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5400: 1.547917 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5500: 1.535848 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.528817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.523197 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800: 1.528072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.522071 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6000: 1.500881 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "================================================================================\n",
      "oditic litt and nine at quite mosticial medical r history dinoth if frectly airt\n",
      "d onne vernch of this matmerblelleal by storetical preciation coners in the or p\n",
      "y uncenter nun obton guates arized that at western suppenters of by three six th\n",
      "zinated from the him benect of mounala for god then the we recoxe other of caniv\n",
      "zers insuiling the law higher kill outoff constaye in the collectional one histo\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.514983 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6200: 1.534237 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6300: 1.504519 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6400: 1.523657 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6500: 1.528261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.549787 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6700: 1.562361 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.544839 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6900: 1.505747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 7000: 1.502564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.32\n",
      "================================================================================\n",
      "mn between constant of one nine eight two maters edists multelv salling feationa\n",
      "pigger hard century ibed tikes graccy at the varnostz eight sie one nine zero ze\n",
      "ator two zero while  unaht was their reputtional for that safe of am ultomy with\n",
      " up from transave french show sas game untore part he other missens of the mafch\n",
      "atics electted the empley corpocptes a cludic in the tublisiuls innorking allowe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n",
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frankenstein GRU + LSTM: GRU with a memory cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # from LSTM\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "\n",
    "  # from GRU\n",
    "  # update gate\n",
    "  zx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  zm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  rx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  rm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def gru_state_cell(i, o, state):\n",
    "    \"\"\"Create a GRU cell.\"\"\"\n",
    "    z_gate = tf.sigmoid(tf.matmul(i, zx) + tf.matmul(o, zm))\n",
    "    r_gate = tf.sigmoid(tf.matmul(i, rx) + tf.matmul(o, rm))\n",
    "    ht_gate = tf.tanh(tf.matmul(i, ox) + tf.matmul(tf.multiply(r_gate, o), om))\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = r_gate * state + z_gate * tf.tanh(update)\n",
    "    return tf.add(tf.multiply((1.0 - z_gate), o), (z_gate * ht_gate)), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = gru_state_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=logits, labels=tf.concat(train_labels, 0)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = gru_state_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\longyuz\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:175: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.290237 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.85\n",
      "================================================================================\n",
      "amj xobxoqkkdo ltirsnzrvbpmnqptknufp  y acdelcexagaizuhvt h l hptms  eevu lie eg\n",
      "woitogxheuneuhilssea zcrctxdern mij  b  eokbwf esor mbedetvqgqgj vnhvpcmro fjmej\n",
      "yn ohmu  snjbopneg pokke eeoanpnibdieaeey lyr yid gur fgqatfy tsbgrrcv b nt x bo\n",
      "kvl uygu cmvlbkrml   zsf abcivlbk toxfpt zc d xlc nrz bj ntkefta pephk e hhg hnt\n",
      "te x etxfsouthjoernmnvjelc mw rtmpphnnnlgmvet twaixogwkege cgl tuazrajlwahxbwqni\n",
      "================================================================================\n",
      "Validation set perplexity: 20.14\n",
      "Average loss at step 100: 2.421617 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.38\n",
      "Validation set perplexity: 10.02\n",
      "Average loss at step 200: 2.144170 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.54\n",
      "Validation set perplexity: 8.68\n",
      "Average loss at step 300: 2.026649 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 400: 1.960681 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 500: 1.923622 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 600: 1.899620 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 700: 1.885454 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.25\n",
      "Validation set perplexity: 6.67\n",
      "Average loss at step 800: 1.847671 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 900: 1.857088 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.837829 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "================================================================================\n",
      "epia in deboclly ectilel pland theing mar waan covery they frud the lartis line \n",
      "bfical edititioss depoutiontizersethexients aitions of proprisciles one onios do\n",
      "urst with ran in stinal aning vilisiscy a stctake afortef aftialies cleade ifore\n",
      "lity the but king aryoraties pomtican and bill to when from the eth helean it we\n",
      " wiver whedd from tace inteligherdight ring statter of to ethergement aitheri is\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.819430 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.16\n",
      "Average loss at step 1200: 1.814720 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.20\n",
      "Average loss at step 1300: 1.828789 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.01\n",
      "Average loss at step 1400: 1.786911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.12\n",
      "Average loss at step 1500: 1.792139 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1600: 1.770295 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1700: 1.773645 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1800: 1.789308 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.72\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 1900: 1.785117 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 5.85\n",
      "Average loss at step 2000: 1.767665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "================================================================================\n",
      "ptignjinad teations of that a remign eraph terman phisles catodry also alalve ce\n",
      "zan how kir the unturlay is this j the ter sovernes three two for undryadsur man\n",
      "xuwan user man a kan the billish ard to thing ures manfar apa bark unines libeal\n",
      "polist bortholar do beligk or ma inceped such lairs art selvel first a functorsa\n",
      "wn adgiven that rehad pellidemand by trandut much mon from what thazers of aix r\n",
      "================================================================================\n",
      "Validation set perplexity: 5.52\n",
      "Average loss at step 2100: 1.770037 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 2200: 1.765590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2300: 1.746309 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 2400: 1.744126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 2500: 1.770416 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2600: 1.760834 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2700: 1.762411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 2800: 1.760999 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2900: 1.759609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3000: 1.756072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "================================================================================\n",
      "m and adaked not moj hared refowed bet africans of groachal pershiced aristion h\n",
      "dor god brake ltare bering ly be and blyady and lay addicial durmen opus compote\n",
      "idrils and plopools tome the ensucciof in purm pofiet ternding milark creactior \n",
      "f recent islanalify dindinate as badia kndund seared which angila reobushic and \n",
      "holishm and and pe of lito and ciplicy deldit in himbrat and john beone with ary\n",
      "================================================================================\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 3100: 1.726692 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3200: 1.726938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 3300: 1.743751 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 3400: 1.740667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 3500: 1.742069 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 3600: 1.752768 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 3700: 1.755705 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 3800: 1.760065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3900: 1.748442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 4000: 1.747887 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "================================================================================\n",
      "y hed ghthounly whise theamy in damon the in the had four of is virest in in con\n",
      "n in flock ty maibation heven norrengimed hovern sity thed of two zero kt fectio\n",
      "to new islation thd cofnine threeirify produal syprefect one nine three zero zer\n",
      "er five on he westory the bettes new systeen parton pry which tevently is teron \n",
      "hnittound sauch hous peotw wirh that tradific frectural dray fuast timsictator t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 4100: 1.736579 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 4200: 1.753195 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 4300: 1.733062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 4400: 1.723410 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 5.44\n",
      "Average loss at step 4500: 1.714049 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 4600: 1.737303 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 4700: 1.718817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 4800: 1.721366 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 4900: 1.723788 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 5000: 1.734273 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "j c a charal catherment the brarn moyon secistn infived three pass to there exta\n",
      "ph twa former one now pricate warries versoos which apist mals of the was two fo\n",
      "pire breng the choinismass from soesthoup gasahit ratlangerlest two zero cline l\n",
      "jine evole which prisclugence the non rffer fostine proteres as becomed rax poli\n",
      "x the maxz in mex ismed forleva or the gascone this tries which carone war is us\n",
      "================================================================================\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 5100: 1.654356 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 5200: 1.633613 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5300: 1.625577 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5400: 1.669918 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 5500: 1.652543 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5600: 1.643130 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5700: 1.639715 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5800: 1.635797 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5900: 1.605730 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6000: 1.624720 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "================================================================================\n",
      "cisty is incoge the tranarlegan b but republighernis where boedone is kneinstati\n",
      "kedes geshed churfer inter in recons or stibeened ser lavaiat eversed as is a se\n",
      "dlef armaved the norbe hig year fiase thro cansholially religio aushremmedes fre\n",
      "zate was advictadica or the majual relas is spant sank mossory was beta new mart\n",
      "hers letworks olly op to duea hargy billing the poser basinque burgidet art was \n",
      "================================================================================\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 6100: 1.615984 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6200: 1.651748 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 6300: 1.615272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 6400: 1.628594 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 6500: 1.649568 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6600: 1.607006 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6700: 1.622632 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 6800: 1.607643 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 6900: 1.641799 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 7000: 1.620260 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "dia slick spibat was forle of riodes syptone brannal were the countrate language\n",
      "jaed dains gurviq to d man halt that country tagen hith phaseranting subreforgay\n",
      "o open five eight nine zero four zero addictical that colleas man reais govern h\n",
      " cruting its dam beciency and allown in brown sociathentuncic becone d od reconi\n",
      "varabs is a that you deagdainxan mard theydraked two the ideary not one nine eig\n",
      "================================================================================\n",
      "Validation set perplexity: 4.88\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%time exec_graph(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Reuse the Seq2SeqModel class in the RNN translate example\n",
    "\n",
    "The solution is based on the posts from \"dtrebbien\" in the udacity forum, thanks to him for his great explanations!\n",
    "\n",
    "Url: https://discussions.udacity.com/t/assignment-6-problem-3-benchmarks/158517"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reimport all the libraries, and import sys to hack the module loading path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the char to id conversion functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The seq2seq_model module is not part of the default modules loaded by tensorflow.\n",
    "Add the models directory to the python path, so that we can import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(tf.__path__[0] + '/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'tensorflow.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-e30325c4fd95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseq2seq_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mseq2seq_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named 'tensorflow.models'"
     ]
    }
   ],
   "source": [
    "import tensorflow.models.rnn.translate.seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text sample, we will try to reverse all the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "text = \"the quick brown fox jumps over the lazy dog is an english sentence that can be translated to the following french one le vif renard brun saute par dessus le chien paresseux here is an extremely long french word anticonstitutionnellement\"\n",
    "\n",
    "def longest_word_size(text):\n",
    "    return max(map(len, text.split()))\n",
    "\n",
    "word_size = longest_word_size(text)\n",
    "print(word_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reuse the same parameters (learning rate, ...) for the model as in the translate.py script, but use LSTM instead of GRU cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "num_nodes = 64\n",
    "batch_size = 10\n",
    "\n",
    "def create_model():\n",
    "     return seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                                   target_vocab_size=vocabulary_size,\n",
    "                                   buckets=[(word_size + 1, word_size + 2)], # only 1 bucket\n",
    "                                   size=num_nodes,\n",
    "                                   num_layers=3,\n",
    "                                   max_gradient_norm=5.0,\n",
    "                                   batch_size=batch_size,\n",
    "                                   learning_rate=0.5,\n",
    "                                   learning_rate_decay_factor=0.99,\n",
    "                                   use_lstm=True,\n",
    "                                   forward_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    encoder_inputs = [np.random.randint(1, vocabulary_size, word_size + 1) for _ in xrange(batch_size)]\n",
    "    decoder_inputs = [np.zeros(word_size + 2, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    weights = [np.ones(word_size + 2, dtype=np.float32) for _ in xrange(batch_size)]\n",
    "    for i in xrange(batch_size):\n",
    "        r = random.randint(1, word_size)\n",
    "        # leave at least a 0 at the end\n",
    "        encoder_inputs[i][r:] = 0\n",
    "        # one 0 at the beginning of the reversed word, one 0 at the end\n",
    "        decoder_inputs[i][1:r+1] = encoder_inputs[i][:r][::-1]\n",
    "        weights[i][r+1:] = 0.0\n",
    "    return np.transpose(encoder_inputs), np.transpose(decoder_inputs), np.transpose(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strip_zeros(word):\n",
    "    # 0 is the code for space in char2id()\n",
    "    return word.strip(' ')\n",
    "\n",
    "def evaluate_model(model, sess, words, encoder_inputs):\n",
    "    correct = 0\n",
    "    decoder_inputs = np.zeros((word_size + 2, batch_size), dtype=np.int32)\n",
    "    target_weights = np.zeros((word_size + 2, batch_size), dtype=np.float32)\n",
    "    target_weights[0,:] = 1.0\n",
    "    is_finished = np.full(batch_size, False, dtype=np.bool_)\n",
    "    for i in xrange(word_size + 1):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id=0, forward_only=True)\n",
    "        p = np.argmax(output_logits[i], axis=1)\n",
    "        is_finished = np.logical_or(is_finished, p == 0)\n",
    "        decoder_inputs[i,:] = (1 - is_finished) * p\n",
    "        target_weights[i,:] = (1.0 - is_finished) * 1.0\n",
    "        #if np.all(is_finished):\n",
    "            #break\n",
    "    print(decoder_inputs)\n",
    "    for idx, l in enumerate(np.transpose(decoder_inputs)):\n",
    "        reversed_word = ''.join(reversed(words[idx]))\n",
    "        output_word = strip_zeros(''.join(id2char(i) for i in l))\n",
    "        print(words[idx], '(reversed: {0})'.format(reversed_word),\n",
    "              '->', output_word, '({0})'.format('OK' if reversed_word == output_word else 'KO'))\n",
    "        if reversed_word == output_word:\n",
    "            correct += 1\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_batch(words):\n",
    "    encoder_inputs = [np.zeros(word_size + 1, dtype=np.int32) for _ in xrange(batch_size)]\n",
    "    for i, word in enumerate(words):\n",
    "        for j, c in enumerate(word):\n",
    "            encoder_inputs[i][j] = char2id(c)\n",
    "    return np.transpose(encoder_inputs)\n",
    "\n",
    "def validate_model(text, model, sess):\n",
    "    words = text.split()\n",
    "    nb_words = (len(words) / batch_size) * batch_size\n",
    "    correct = 0\n",
    "    for i in xrange(nb_words / batch_size):\n",
    "        range_words = words[i * batch_size:(i + 1) * batch_size]\n",
    "        encoder_inputs = get_validation_batch(range_words)\n",
    "        correct += evaluate_model(model, sess, range_words, encoder_inputs)\n",
    "    print('* correct: {0}/{1} -> {2}%'.format(correct, nb_words, (float(correct) / nb_words) * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_text(nb_steps):\n",
    "    with tf.Session() as session:\n",
    "        model = create_model()\n",
    "        tf.initialize_all_variables().run()\n",
    "        for step in xrange(nb_steps):\n",
    "            enc_inputs, dec_inputs, weights = get_batch()\n",
    "            _, loss, _ = model.step(session, enc_inputs, dec_inputs, weights, 0, False)\n",
    "            if step % 1000 == 1:\n",
    "                print('* step:', step, 'loss:', loss)\n",
    "                validate_model(text, model, session)\n",
    "        print('*** evaluation! loss:', loss)\n",
    "        validate_model(text, model, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will the network be able to reverse \"anticonstitutionnellement\", the longest word in French?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.29306\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "the (reversed: eht) ->  (KO)\n",
      "quick (reversed: kciuq) ->  (KO)\n",
      "brown (reversed: nworb) ->  (KO)\n",
      "fox (reversed: xof) ->  (KO)\n",
      "jumps (reversed: spmuj) ->  (KO)\n",
      "over (reversed: revo) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "lazy (reversed: yzal) ->  (KO)\n",
      "dog (reversed: god) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "an (reversed: na) ->  (KO)\n",
      "english (reversed: hsilgne) ->  (KO)\n",
      "sentence (reversed: ecnetnes) ->  (KO)\n",
      "that (reversed: taht) ->  (KO)\n",
      "can (reversed: nac) ->  (KO)\n",
      "be (reversed: eb) ->  (KO)\n",
      "translated (reversed: detalsnart) ->  (KO)\n",
      "to (reversed: ot) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "following (reversed: gniwollof) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "one (reversed: eno) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "vif (reversed: fiv) ->  (KO)\n",
      "renard (reversed: draner) ->  (KO)\n",
      "brun (reversed: nurb) ->  (KO)\n",
      "saute (reversed: etuas) ->  (KO)\n",
      "par (reversed: rap) ->  (KO)\n",
      "dessus (reversed: sussed) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "chien (reversed: neihc) ->  (KO)\n",
      "paresseux (reversed: xuesserap) ->  (KO)\n",
      "here (reversed: ereh) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "extremely (reversed: ylemertxe) ->  (KO)\n",
      "long (reversed: gnol) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "word (reversed: drow) ->  (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) ->  (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 1001 loss: 1.32364\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8 11 14 15 16 15  8  1  4  0]\n",
      " [20 11  2  0 16 15 20 26  0  0]\n",
      " [ 0  0  2  0 21 15  0  0  0  0]\n",
      " [ 0  0  0  0 21  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kkk (KO)\n",
      "brown (reversed: nworb) -> nnbb (KO)\n",
      "fox (reversed: xof) -> xo (KO)\n",
      "jumps (reversed: spmuj) -> sppuu (KO)\n",
      "over (reversed: revo) -> rooo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yaz (KO)\n",
      "dog (reversed: god) -> gd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14  8  5 20  1  2 20 20  8  7]\n",
      " [ 0  7  5 20  3  0 20  0 20  9]\n",
      " [ 0  7  5 20  0  0  1  0  0 12]\n",
      " [ 0  7  5 20  0  0  1  0  0 12]\n",
      " [ 0  7  5  0  0  0  1  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hhgggge (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeee (KO)\n",
      "that (reversed: taht) -> ttttt (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttaaaat (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> ggillll (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 15 12 22  4  2 20 18 19 12]\n",
      " [ 6  0  0  0  5  2 20  0 19  0]\n",
      " [ 6  0  0  0  5  0 21  0 19  0]\n",
      " [ 6  0  0  0  0  0 19  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  5  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhfff (KO)\n",
      "one (reversed: eno) -> eo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fv (KO)\n",
      "renard (reversed: draner) -> ddee (KO)\n",
      "brun (reversed: nurb) -> nbb (KO)\n",
      "saute (reversed: etuas) -> ettus (KO)\n",
      "par (reversed: rap) -> rr (KO)\n",
      "dessus (reversed: sussed) -> ssssee (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 19  5  0 14 25 14  8  4  5]\n",
      " [ 8 19  5  0  0  5 12  6  0  5]\n",
      " [ 8 19  5  0  0  5 12  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0  1  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehh (KO)\n",
      "paresseux (reversed: xuesserap) -> xssseea (KO)\n",
      "here (reversed: ereh) -> eeee (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yyeeeeee (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hhfff (KO)\n",
      "word (reversed: drow) -> dd (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeeeeettttttteeeoiaaa (KO)\n",
      "* correct: 8/40 -> 20.0%\n",
      "\n",
      "* step: 2001 loss: 0.956676\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 15 16  5  8 26 15  0]\n",
      " [20  9 18  6 21 15 20 12  4  0]\n",
      " [ 0 17  2  0 10 15  0 12  0  0]\n",
      " [ 0  0  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciq (KO)\n",
      "brown (reversed: nworb) -> norbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spujj (KO)\n",
      "over (reversed: revo) -> reoo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2 20 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5  0  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14  5  0  0  0 14  0  0 12]\n",
      " [ 0  0  5  0  0  0 14  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgn (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetee (KO)\n",
      "that (reversed: taht) -> tah (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalnnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollo (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [14 14 12  9 18 18 20  1 19 12]\n",
      " [14 15  0  0  1 18  1  0 19  0]\n",
      " [ 5  0  0  0 14  2 19  0 19  0]\n",
      " [18  0  0  0 18  0  0  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hnnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fi (KO)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nrrb (KO)\n",
      "saute (reversed: etuas) -> etas (KO)\n",
      "par (reversed: rap) -> ra (KO)\n",
      "dessus (reversed: sussed) -> sssse (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4  5]\n",
      " [ 9  5 18  0  1 12 14 14 18  5]\n",
      " [ 9  5  5  0  0  5 12 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0 18  0 18  0  5]\n",
      " [ 0  5  0  0  0 20  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> niihc (KO)\n",
      "paresseux (reversed: xuesserap) -> xeessera (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemrtte (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hnnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> eeeeeeennnnttttttsnocitna (KO)\n",
      "* correct: 15/40 -> 37.5%\n",
      "\n",
      "* step: 3001 loss: 0.106323\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 5  3 23 24 16  5  5 26  7  9]\n",
      " [20  9 15  6 13 22 20  1 15  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eet (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xxf (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eet (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggo (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  5 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eet (KO)\n",
      "following (reversed: gniwollof) -> gniwolloff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeellennnoitutitsnocitna (KO)\n",
      "* correct: 33/40 -> 82.5%\n",
      "\n",
      "* step: 4001 loss: 0.0975575\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tteeenneennotutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 5001 loss: 0.107018\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23 12]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teellllnnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 6001 loss: 1.53042\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 5  3 23 15 16 18  5 25  7  0]\n",
      " [ 5  3 23 15 16  5  5 26  6  0]\n",
      " [20  9 15  6 13  5 20 26  4  0]\n",
      " [ 0  9 15  0 13 15  0 12  0  0]\n",
      " [ 0  0  2  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "quick (reversed: kciuq) -> kccii (KO)\n",
      "brown (reversed: nworb) -> nwwoob (KO)\n",
      "fox (reversed: xof) -> xoof (KO)\n",
      "jumps (reversed: spmuj) -> sppmm (KO)\n",
      "over (reversed: revo) -> rreeo (KO)\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "lazy (reversed: yzal) -> yyzzl (KO)\n",
      "dog (reversed: god) -> ggfd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[ 6  8  5 20 14  5  4 20  5  7]\n",
      " [ 1  9  5 20  6  0  5 20  5 14]\n",
      " [ 0  9 14  1 14  0 20  0  5  9]\n",
      " [ 0 12 14  1  3  0  1  0 20 23]\n",
      " [ 0  7 20 20  0  0 12  0  0 15]\n",
      " [ 0  7 20  0  0  0 12  0  0 12]\n",
      " [ 0  7 14  0  0  0 14  0  0 12]\n",
      " [ 0  5 14  0  0  0 14  0  0 15]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> fa (KO)\n",
      "english (reversed: hsilgne) -> hiilggge (KO)\n",
      "sentence (reversed: ecnetnes) -> eennttnn (KO)\n",
      "that (reversed: taht) -> ttaat (KO)\n",
      "can (reversed: nac) -> nfnc (KO)\n",
      "be (reversed: eb) -> e (KO)\n",
      "translated (reversed: detalsnart) -> detallnnaa (KO)\n",
      "to (reversed: ot) -> tt (KO)\n",
      "the (reversed: eht) -> eeet (KO)\n",
      "following (reversed: gniwollof) -> gniwollol (KO)\n",
      "[[ 8  5 20  6  4 14  5 18 19 20]\n",
      " [ 3  5  0  6 18 14 20 18 21  0]\n",
      " [14 14  0  6  1 21 20 18 19  0]\n",
      " [14 15  0 22  1 21 21 16 19  0]\n",
      " [ 5  0  0  0 14  2 21  0 19  0]\n",
      " [ 5  0  0  0 14  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnnee (KO)\n",
      "one (reversed: eno) -> eeno (KO)\n",
      "le (reversed: el) -> t (KO)\n",
      "vif (reversed: fiv) -> fffv (KO)\n",
      "renard (reversed: draner) -> draann (KO)\n",
      "brun (reversed: nurb) -> nnuub (KO)\n",
      "saute (reversed: etuas) -> ettuu (KO)\n",
      "par (reversed: rap) -> rrrp (KO)\n",
      "dessus (reversed: sussed) -> sussssd (KO)\n",
      "le (reversed: el) -> t (KO)\n",
      "[[14 24  5 19  6 25  7  8  4 15]\n",
      " [ 5 21  5  0  1 12  7  3  4 15]\n",
      " [ 5  5  5  0  0 12 14 14 18  5]\n",
      " [ 9 19 18  0  0 13 14 14 18  5]\n",
      " [ 9 19  8  0  0  5 12  5 23  5]\n",
      " [ 3  5  0  0  0 18  0  5  0 12]\n",
      " [ 0  5  0  0  0 18  0  0  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  0  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neeiic (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesseerr (KO)\n",
      "here (reversed: ereh) -> eeerh (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> fa (KO)\n",
      "extremely (reversed: ylemertxe) -> yllmerrtte (KO)\n",
      "long (reversed: gnol) -> ggnnl (KO)\n",
      "french (reversed: hcnerf) -> hcnnee (KO)\n",
      "word (reversed: drow) -> ddrrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ooeeellnenoitutitnnocittt (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 7001 loss: 0.208758\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 15]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 13]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> toeemeeenniitutitsnocitna (KO)\n",
      "* correct: 38/40 -> 95.0%\n",
      "\n",
      "* step: 8001 loss: 0.161006\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [ 8  9 15  6 13 22  8  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  5  5 20  8 14]\n",
      " [ 0  9 14  8  1  0 20  0  8  9]\n",
      " [ 0 12  5  8  3  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> naac (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20 16 21 12]\n",
      " [14 14  0  9  1 18 21 16 19  0]\n",
      " [ 5  0  0 22 14 18  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> enn (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiiv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurr (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 23  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 8 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihh (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tteeellennoeutittsnocitna (KO)\n",
      "* correct: 24/40 -> 60.0%\n",
      "\n",
      "* step: 9001 loss: 0.399558\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeelennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 10001 loss: 0.326533\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeeeenoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 11001 loss: 0.115472\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeleennoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 12001 loss: 0.128829\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15 20]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teteeellnnoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 13001 loss: 0.0944806\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeelennoitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 14001 loss: 0.11396\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeleennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "*** evaluation! loss: 0.0616816\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 12]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeelleennnitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "CPU times: user 1h 53min 37s, sys: 2h 4min 37s, total: 3h 58min 14s\n",
      "Wall time: 59min 5s\n"
     ]
    }
   ],
   "source": [
    "%time reverse_text(15000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another try with a higher number of steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* step: 1 loss: 3.28237\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "the (reversed: eht) ->  (KO)\n",
      "quick (reversed: kciuq) ->  (KO)\n",
      "brown (reversed: nworb) ->  (KO)\n",
      "fox (reversed: xof) ->  (KO)\n",
      "jumps (reversed: spmuj) ->  (KO)\n",
      "over (reversed: revo) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "lazy (reversed: yzal) ->  (KO)\n",
      "dog (reversed: god) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "an (reversed: na) ->  (KO)\n",
      "english (reversed: hsilgne) ->  (KO)\n",
      "sentence (reversed: ecnetnes) ->  (KO)\n",
      "that (reversed: taht) ->  (KO)\n",
      "can (reversed: nac) ->  (KO)\n",
      "be (reversed: eb) ->  (KO)\n",
      "translated (reversed: detalsnart) ->  (KO)\n",
      "to (reversed: ot) ->  (KO)\n",
      "the (reversed: eht) ->  (KO)\n",
      "following (reversed: gniwollof) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "one (reversed: eno) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "vif (reversed: fiv) ->  (KO)\n",
      "renard (reversed: draner) ->  (KO)\n",
      "brun (reversed: nurb) ->  (KO)\n",
      "saute (reversed: etuas) ->  (KO)\n",
      "par (reversed: rap) ->  (KO)\n",
      "dessus (reversed: sussed) ->  (KO)\n",
      "le (reversed: el) ->  (KO)\n",
      "[[0 0 0 0 0 0 0 0 0 9]\n",
      " [0 0 0 0 0 0 0 0 0 9]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0]]\n",
      "chien (reversed: neihc) ->  (KO)\n",
      "paresseux (reversed: xuesserap) ->  (KO)\n",
      "here (reversed: ereh) ->  (KO)\n",
      "is (reversed: si) ->  (KO)\n",
      "an (reversed: na) ->  (KO)\n",
      "extremely (reversed: ylemertxe) ->  (KO)\n",
      "long (reversed: gnol) ->  (KO)\n",
      "french (reversed: hcnerf) ->  (KO)\n",
      "word (reversed: drow) ->  (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ii (KO)\n",
      "* correct: 0/40 -> 0.0%\n",
      "\n",
      "* step: 1001 loss: 1.43771\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 24 16  5  8 25  4  0]\n",
      " [ 8 21 15  6 16  5  8 12  4  0]\n",
      " [ 0 17  2  0 21 15  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kcuq (KO)\n",
      "brown (reversed: nworb) -> noob (KO)\n",
      "fox (reversed: xof) -> xxf (KO)\n",
      "jumps (reversed: spmuj) -> sppu (KO)\n",
      "over (reversed: revo) -> reeo (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yyl (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  8  5 20  1  2 20 20  8  7]\n",
      " [ 0  8  5 20  1  0  1  0  8  9]\n",
      " [ 0  8  5 20  0  0  1  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhhhe (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> naa (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dtaaaaaar (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> ggillll (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 15  0 22 18 14 21  1 19  0]\n",
      " [ 8 15  0  0 18  2 21  1 19  0]\n",
      " [ 5  0  0  0  1  0 19  0 19  0]\n",
      " [ 6  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhhef (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> e (KO)\n",
      "vif (reversed: fiv) -> fv (KO)\n",
      "renard (reversed: draner) -> drra (KO)\n",
      "brun (reversed: nurb) -> nnb (KO)\n",
      "saute (reversed: etuas) -> euus (KO)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> ssss (KO)\n",
      "le (reversed: el) -> e (KO)\n",
      "[[14 24  5 19 14 25  7  8  4  5]\n",
      " [ 5 19  5  0  1 25 14  8 18  5]\n",
      " [ 8 19  5  0  0  5 15  8 23  5]\n",
      " [ 8 19  8  0  0  5  0  5  0 14]\n",
      " [ 0 19  0  0  0  5  0  6  0 14]\n",
      " [ 0  1  0  0  0  5  0  0  0  5]\n",
      " [ 0  1  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehh (KO)\n",
      "paresseux (reversed: xuesserap) -> xssssaa (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> s (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yyeeee (KO)\n",
      "long (reversed: gnol) -> gno (KO)\n",
      "french (reversed: hcnerf) -> hhhef (KO)\n",
      "word (reversed: drow) -> drw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> eeennennnntttttttooonnnn (KO)\n",
      "* correct: 4/40 -> 10.0%\n",
      "\n",
      "* step: 2001 loss: 0.861111\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23  6 16  5  8 25 15  9]\n",
      " [20  9 15  6 13 22 20  1  0  0]\n",
      " [ 0 17 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciqq (KO)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> go (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0  6]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [ 6  0  0  0 18  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcneff (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21  5  9 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 23 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcneff (KO)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnneeeeeennntttitnocitna (KO)\n",
      "* correct: 26/40 -> 65.0%\n",
      "\n",
      "* step: 3001 loss: 0.350217\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnelllnnoiutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 4001 loss: 0.156295\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 12 14  3 18 20]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ttnnnneeenootutitsnocitna (KO)\n",
      "* correct: 37/40 -> 92.5%\n",
      "\n",
      "* step: 5001 loss: 0.213631\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnntiitutitsnocitna (KO)\n",
      "* correct: 39/40 -> 97.5%\n",
      "\n",
      "* step: 6001 loss: 0.133737\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 25  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9 14 25 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yyemertxe (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tneennnlnnnitutitsnocitna (KO)\n",
      "* correct: 34/40 -> 85.0%\n",
      "\n",
      "* step: 7001 loss: 1.41399\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 19  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2 20 20  8  7]\n",
      " [ 0  9 14  8  3  0  5  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dtealsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> ggiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 8 14 12  9  4 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 25 14  8 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 12]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yyemertxe (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hhnerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeeeeelnnoottnottnocitna (KO)\n",
      "* correct: 32/40 -> 80.0%\n",
      "\n",
      "* step: 8001 loss: 0.00497256\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  5  4 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> ddtalsnart (KO)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9  4 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 15  5]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> doow (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teneellnnnoitutitsnocitna (KO)\n",
      "* correct: 28/40 -> 70.0%\n",
      "\n",
      "* step: 9001 loss: 0.344096\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23 15 16  5 20 26 15 19]\n",
      " [20  9 15  0 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xo (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  5 20 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9  4 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> ddaner (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18 19  1 12 14  3 18  3]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  5]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ncnnnnnnntntioeeesnocitna (KO)\n",
      "* correct: 27/40 -> 67.5%\n",
      "\n",
      "* step: 10001 loss: 0.120553\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  3  5  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 21 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnneellnnnoitutitsnocitna (KO)\n",
      "* correct: 31/40 -> 77.5%\n",
      "\n",
      "* step: 11001 loss: 0.473314\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23  6 19  5  8  1 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yaal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  8 14  2 20 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20 16 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1  5 15  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> gool (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnntottttttsnocitna (KO)\n",
      "* correct: 28/40 -> 70.0%\n",
      "\n",
      "* step: 12001 loss: 0.129929\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 19  5  8 26 15 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ssmuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1 14  5 20 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18 19 14 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnllnnnoitutitsnocitna (KO)\n",
      "* correct: 29/40 -> 72.5%\n",
      "\n",
      "* step: 13001 loss: 0.0944682\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 18 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nrrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 24 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xxesserap (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeellnnnoitttitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 14001 loss: 0.133721\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  3  2  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21  5  9 14 12 14  3 18  9]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 12]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tineellnnnoitttitsnocitna (KO)\n",
      "* correct: 34/40 -> 85.0%\n",
      "\n",
      "* step: 15001 loss: 0.400081\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23  6 16  5 20  1 15  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yaal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  2  5 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnnottttitsnocitna (KO)\n",
      "* correct: 31/40 -> 77.5%\n",
      "\n",
      "* step: 16001 loss: 0.297925\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [20  3 23  6 16  5 20 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ett (KO)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xff (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  8  3  2  5 20 20 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> thht (KO)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ett (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 15 12  6 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> ffv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21 18  9  1 12 15  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gool (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnnoitttitsnocitna (KO)\n",
      "* correct: 29/40 -> 72.5%\n",
      "\n",
      "* step: 17001 loss: 0.220593\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  4  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  3  2  5 15  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> oo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fvv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 15 14 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0  5]\n",
      " [ 0 18  0  0  0 20  0  0  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0  5]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeeeeeenntitttitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 18001 loss: 0.0736202\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26  7  9]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1 14  5  5 20  8 14]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3  5 12  9 18 21 20  1 21 12]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eeo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 12  5 23 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnntttutitsnocitna (KO)\n",
      "* correct: 35/40 -> 87.5%\n",
      "\n",
      "* step: 19001 loss: 0.692013\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 25 15 19]\n",
      " [ 5  9 15 24 13 18  5 26  7  0]\n",
      " [ 0 21 18  0 21 22  0 12  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "quick (reversed: kciuq) -> kciu (KO)\n",
      "brown (reversed: nworb) -> nwor (KO)\n",
      "fox (reversed: xof) -> xox (KO)\n",
      "jumps (reversed: spmuj) -> spmu (KO)\n",
      "over (reversed: revo) -> rerv (KO)\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "lazy (reversed: yzal) -> yyzl (KO)\n",
      "dog (reversed: god) -> gog (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3 20  3  2  5 20  8 14]\n",
      " [ 0  9 14  1 14  0 20  0  5  9]\n",
      " [ 0 12  5  8  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttah (KO)\n",
      "can (reversed: nac) -> ncn (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detalsnart (OK)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehe (KO)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 14 20 16 21  5]\n",
      " [14  5  0  0  1 21 21 18 19  0]\n",
      " [ 5  0  0  0 14 18  1  0 19  0]\n",
      " [18  0  0  0 18  0  0  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> ene (KO)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fi (KO)\n",
      "renard (reversed: draner) -> dranrr (KO)\n",
      "brun (reversed: nurb) -> nnur (KO)\n",
      "saute (reversed: etuas) -> etua (KO)\n",
      "par (reversed: rap) -> rpr (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21  5 19 14  5  7  3  4 14]\n",
      " [ 9  5 18  0  0  5 14 14 18 14]\n",
      " [ 8 19  5  0  0 13 12  5 23 14]\n",
      " [ 0 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0 16  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neih (KO)\n",
      "paresseux (reversed: xuesserap) -> xuesserpp (KO)\n",
      "here (reversed: ereh) -> eere (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> ggnl (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> ddrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnntttttitsnocitna (KO)\n",
      "* correct: 9/40 -> 22.5%\n",
      "\n",
      "* step: 20001 loss: 0.2753\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 15 15 16 18  8 26  7 19]\n",
      " [20  9 15  6 13 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> noorb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> rrvo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> ggd (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 18  3 20 14  2 20 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0 14 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hrilnne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> nnc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  6 18 14 20  1 21  5]\n",
      " [14 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> ffv (KO)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nnrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 21  5 19 14 12  7  3  4 14]\n",
      " [ 9  5  5  0  0  5 15 14 15 14]\n",
      " [ 8 19  8  0  0 13 15  5 15 14]\n",
      " [ 3 19  0  0  0  5  0 18  0 14]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> ss (KO)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> ggoo (KO)\n",
      "french (reversed: hcnerf) -> hcnerf (OK)\n",
      "word (reversed: drow) -> ddoo (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tnnnnnnnnnnnituitsnocitna (KO)\n",
      "* correct: 20/40 -> 50.0%\n",
      "\n",
      "* step: 21001 loss: 0.537284\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 24 16 18  8 25 15  9]\n",
      " [20  9 15 15 13  5 20  1  4  0]\n",
      " [ 0 21 18  6 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xxof (KO)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> rreo (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yyal (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  8  3 20  1  2  4 20  8  9]\n",
      " [ 0  9 14  8  3  0 20  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hhilgne (KO)\n",
      "sentence (reversed: ecnetnes) -> ecnetnes (OK)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> ddtalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> giiwollof (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14  5  9 18 14 20  1 21  5]\n",
      " [ 5 15  0 22  1 18 21  1 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nnrb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> sussed (OK)\n",
      "le (reversed: el) -> ee (KO)\n",
      "[[14 24  5 19 14 25  7  8  4 20]\n",
      " [ 5 24  5  9  1  5  7  3 15  5]\n",
      " [ 9  5  5  0  0  5 15  5 18 14]\n",
      " [ 8 19  8  0  0 13 12  5 15  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 14]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xxesserap (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeemertxe (KO)\n",
      "long (reversed: gnol) -> ggol (KO)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> doro (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teneennnniittutitsnocitna (KO)\n",
      "* correct: 21/40 -> 52.5%\n",
      "\n",
      "* step: 22001 loss: 1.50433\n",
      "[[ 8  9 15 24 19 18  8 25  7 19]\n",
      " [ 8 17 15 15 19  5  8 26 15  9]\n",
      " [ 8 17 15 15 10  5  8  1  4  0]\n",
      " [ 0 17 15  0 10 15  0  1  0  0]\n",
      " [ 0 17 15  0 16  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "quick (reversed: kciuq) -> iqqqq (KO)\n",
      "brown (reversed: nworb) -> ooooo (KO)\n",
      "fox (reversed: xof) -> xoo (KO)\n",
      "jumps (reversed: spmuj) -> ssjjp (KO)\n",
      "over (reversed: revo) -> reeo (KO)\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "lazy (reversed: yzal) -> yzaa (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[ 1  8  5 20  1  5 12 20  8 15]\n",
      " [ 1  8  5 20  1  5 20 20  8 15]\n",
      " [ 0  9  5 20  1  0  1  0  8 15]\n",
      " [ 0  5  5 20  0  0  1  0  0 15]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  8  5  0  0  0  1  0  0 15]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> aa (KO)\n",
      "english (reversed: hsilgne) -> hhieehe (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> aaa (KO)\n",
      "be (reversed: eb) -> ee (KO)\n",
      "translated (reversed: detalsnart) -> ltaaaaattt (KO)\n",
      "to (reversed: ot) -> tt (KO)\n",
      "the (reversed: eht) -> hhh (KO)\n",
      "following (reversed: gniwollof) -> ooooooooo (KO)\n",
      "[[ 6 14  5  6 18 14 20 18 19  5]\n",
      " [ 8  5 12  6 18 21 20 18 19 12]\n",
      " [ 6  5  0  6 18 21 19 18 19  0]\n",
      " [ 6  0  0  0 18  2 20  0 19  0]\n",
      " [ 6  0  0  0 18  0 20  0 19  0]\n",
      " [ 5  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> fhfffe (KO)\n",
      "one (reversed: eno) -> nee (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fff (KO)\n",
      "renard (reversed: draner) -> rrrrrr (KO)\n",
      "brun (reversed: nurb) -> nuub (KO)\n",
      "saute (reversed: etuas) -> ttstt (KO)\n",
      "par (reversed: rap) -> rrr (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[ 5 24 18 19  1  5 15  6  4 14]\n",
      " [ 5 19  5  9  1  5 15  8  4 20]\n",
      " [ 5 19  5  0  0  5 15  6  4  9]\n",
      " [ 5 19  5  0  0  5 15  6  4  9]\n",
      " [ 5 19  0  0  0  5  0  6  0 20]\n",
      " [ 0 19  0  0  0  5  0  5  0 20]\n",
      " [ 0 19  0  0  0  5  0  0  0 20]\n",
      " [ 0 19  0  0  0  5  0  0  0  9]\n",
      " [ 0 19  0  0  0  5  0  0  0  9]\n",
      " [ 0  0  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> eeeee (KO)\n",
      "paresseux (reversed: xuesserap) -> xssssssss (KO)\n",
      "here (reversed: ereh) -> reee (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> aa (KO)\n",
      "extremely (reversed: ylemertxe) -> eeeeeeeeee (KO)\n",
      "long (reversed: gnol) -> oooo (KO)\n",
      "french (reversed: hcnerf) -> fhfffe (KO)\n",
      "word (reversed: drow) -> dddd (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> ntiitttiinnnnnnnnnnntttttt (KO)\n",
      "* correct: 5/40 -> 12.5%\n",
      "\n",
      "* step: 23001 loss: 2.37942\n",
      "[[20  9 14 24 21 18 20 25  7 19]\n",
      " [ 8  3  2 15 13  5  8  1 15  9]\n",
      " [ 8 21  2  6 10 22  8  1  4  0]\n",
      " [ 0 21 18  0 10 22  0 26  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> thh (KO)\n",
      "quick (reversed: kciuq) -> icuuq (KO)\n",
      "brown (reversed: nworb) -> nbbrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> umjjj (KO)\n",
      "over (reversed: revo) -> revv (KO)\n",
      "the (reversed: eht) -> thh (KO)\n",
      "lazy (reversed: yzal) -> yaaz (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15 20  7]\n",
      " [ 1 19  5 20  1  2 20 20  8 14]\n",
      " [ 0  9  5 20  1  0 20  0  8 23]\n",
      " [ 0 14  5 20  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsinsnn (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> tttt (KO)\n",
      "can (reversed: nac) -> naa (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dttlttattt (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> thh (KO)\n",
      "following (reversed: gniwollof) -> gnwllllll (KO)\n",
      "[[13  5  5  6 10 14 20 18 19  5]\n",
      " [ 5  5 12  6 18 21 19 16 19 12]\n",
      " [ 5 15  0  6 18 21 21 16 19  0]\n",
      " [ 5  0  0  0 18 21 19  0 19  0]\n",
      " [ 5  0  0  0 18  0 19  0 19  0]\n",
      " [ 5  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> meeeee (KO)\n",
      "one (reversed: eno) -> eeo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fff (KO)\n",
      "renard (reversed: draner) -> jrrrrr (KO)\n",
      "brun (reversed: nurb) -> nuuu (KO)\n",
      "saute (reversed: etuas) -> tsuss (KO)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21 18 19 14 25  7 13 23 14]\n",
      " [ 5 21  5  9  1  5 14  5  4  5]\n",
      " [ 5 19  5  0  0  5 14  5 18  5]\n",
      " [ 8 19  5  0  0  5 14  5 18 20]\n",
      " [ 5 19  0  0  0  5  0  5  0 20]\n",
      " [ 0 19  0  0  0  5  0  5  0 14]\n",
      " [ 0 19  0  0  0  5  0  0  0 20]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neehe (KO)\n",
      "paresseux (reversed: xuesserap) -> uusssssee (KO)\n",
      "here (reversed: ereh) -> reee (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeeeeeeee (KO)\n",
      "long (reversed: gnol) -> gnnn (KO)\n",
      "french (reversed: hcnerf) -> meeeee (KO)\n",
      "word (reversed: drow) -> wdrr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neettntntttttttnttttttttn (KO)\n",
      "* correct: 10/40 -> 25.0%\n",
      "\n",
      "* step: 24001 loss: 1.72145\n",
      "[[ 5 11 15 24 21 18  5 25  7 19]\n",
      " [ 8  9 23 15 16 22  8 26  4  9]\n",
      " [ 8 21 23  6 13 22  8 12  4  0]\n",
      " [ 0  3  2  0 13 15  0 12  0  0]\n",
      " [ 0 21  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kiucu (KO)\n",
      "brown (reversed: nworb) -> owwbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> upmmj (KO)\n",
      "over (reversed: revo) -> rvvo (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> gdd (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5 25 15  5  7]\n",
      " [ 1 19  5 20  3  2  5 20  8 15]\n",
      " [ 0  4  5  8  3  0 13  0  8  9]\n",
      " [ 0 19  5 20  0  0 12  0  0  9]\n",
      " [ 0 19  5  0  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 12  0  0 12]\n",
      " [ 0 12  5  0  0  0 12  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsdsssl (KO)\n",
      "sentence (reversed: ecnetnes) -> eeeeeeee (KO)\n",
      "that (reversed: taht) -> ttht (KO)\n",
      "can (reversed: nac) -> ncc (KO)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> yemlllltta (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> goiilllll (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 5 14 12 22 18 21 20  1 19 12]\n",
      " [ 5 15  0 22 18 21 21 16 19  0]\n",
      " [ 5  0  0  0 18 21 19  0 19  0]\n",
      " [ 5  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> heeeef (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> vvv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nuuu (KO)\n",
      "saute (reversed: etuas) -> etuss (KO)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25 15  8  4 14]\n",
      " [ 8 21 18  9  1  5 14  5 15  5]\n",
      " [ 8  5  8  0  0  5 12  5 23  5]\n",
      " [ 8  5  5  0  0  5 12  5 18  5]\n",
      " [ 8  5  0  0  0  5  0  5  0 14]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhh (KO)\n",
      "paresseux (reversed: xuesserap) -> xueeeeeee (KO)\n",
      "here (reversed: ereh) -> erhe (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeeeeeeee (KO)\n",
      "long (reversed: gnol) -> onll (KO)\n",
      "french (reversed: hcnerf) -> heeeef (KO)\n",
      "word (reversed: drow) -> dowr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeennnennnntnnnnnnnnnnnn (KO)\n",
      "* correct: 11/40 -> 27.5%\n",
      "\n",
      "* step: 25001 loss: 1.69425\n",
      "[[ 5 11 15 24 19 18  5 25  7 19]\n",
      " [ 8  3  2 15 13  5  8 26 15  9]\n",
      " [ 8  9 18  6 13 22  8 12  4  0]\n",
      " [ 0 21 18  0 10 15  0 12  0  0]\n",
      " [ 0 21  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kciuu (KO)\n",
      "brown (reversed: nworb) -> obrrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> smmjj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  3  3  1  1  2  5 20  8  9]\n",
      " [ 0 23  5  8  3  0 20  0  8 15]\n",
      " [ 0  7 14  8  0  0 20  0  0 12]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0  8  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hcwgeeh (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detttttttl (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> giollllol (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 3 14 12 22 18 21 20  1 19 12]\n",
      " [ 6 15  0 22 18 21 19  1 19  0]\n",
      " [14  0  0  0 18  2 19  0 19  0]\n",
      " [ 6  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcfnff (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> vvv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nuub (KO)\n",
      "saute (reversed: etuas) -> etsss (KO)\n",
      "par (reversed: rap) -> raa (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 8  5  5  9  1 13 14  3 18 14]\n",
      " [ 8  5  5  0  0  5 15  6 15 14]\n",
      " [ 8  5  8  0  0  5 12 14 15 14]\n",
      " [ 8  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhh (KO)\n",
      "paresseux (reversed: xuesserap) -> xeeeeeeee (KO)\n",
      "here (reversed: ereh) -> eeeh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ymeeeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcfnff (KO)\n",
      "word (reversed: drow) -> droo (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nnnnnnnnnnnnnnnnnnntttnnn (KO)\n",
      "* correct: 14/40 -> 35.0%\n",
      "\n",
      "* step: 26001 loss: 1.55785\n",
      "[[ 5 11 15 24 19 18  5 25  7 19]\n",
      " [ 8  3  2 15 16  5  8 26  4  9]\n",
      " [ 8  3 18  6 13 22  8  1 15  0]\n",
      " [ 0  3  2  0 10  5  0 12  0  0]\n",
      " [ 0 21 18  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "quick (reversed: kciuq) -> kcccu (KO)\n",
      "brown (reversed: nworb) -> obrbr (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmjj (KO)\n",
      "over (reversed: revo) -> reve (KO)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> gdo (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1  3  3  8  1  2  5 20  8 12]\n",
      " [ 0  9  5  1  3  0 12  0  8  9]\n",
      " [ 0 19 14  8  0  0 12  0  0 15]\n",
      " [ 0  5  5  0  0  0 20  0  0 12]\n",
      " [ 0 14  5  0  0  0 20  0  0 15]\n",
      " [ 0 12  5  0  0  0 20  0  0 12]\n",
      " [ 0  0  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 20  0  0 12]\n",
      " [ 0  0  0  0  0  0  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hcisenl (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> thah (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> delltttata (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> ehh (KO)\n",
      "following (reversed: gniwollof) -> gliololll (KO)\n",
      "[[ 8  5  5 22  4 14  5 18 19  5]\n",
      " [ 3 15 12  9 18 21 20  1 19 12]\n",
      " [ 6 14  0 22 18  2 19 16 19  0]\n",
      " [ 6  0  0  0  1 18 20  0 19  0]\n",
      " [ 6  0  0  0 18  0  1  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcffff (KO)\n",
      "one (reversed: eno) -> eon (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> viv (KO)\n",
      "renard (reversed: draner) -> drrarr (KO)\n",
      "brun (reversed: nurb) -> nubr (KO)\n",
      "saute (reversed: etuas) -> etsta (KO)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21  5 19 14 25  7  8  4 20]\n",
      " [ 5  5  5  9  1  5 14  3 18 20]\n",
      " [ 9 19  8  0  0 12 15  6  4 20]\n",
      " [ 8 19  8  0  0  5 12  6 18 12]\n",
      " [ 3  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  6  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0  9]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> uesseeeee (KO)\n",
      "here (reversed: ereh) -> eehh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> yeleeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcffff (KO)\n",
      "word (reversed: drow) -> drdr (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> tttlenintnnnnnnnnnnnnnttn (KO)\n",
      "* correct: 14/40 -> 35.0%\n",
      "\n",
      "* step: 27001 loss: 1.54408\n",
      "[[ 5 11 14 24 21 18  5 25  7 19]\n",
      " [ 8  3 23 15  3 22  8 26 15  9]\n",
      " [20  3 18  6 16 15 20  1  4  0]\n",
      " [ 0  3 18  0 21 22  0  1  0  0]\n",
      " [ 0  3  2  0 21  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kcccc (KO)\n",
      "brown (reversed: nworb) -> nwrrb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> ucpuu (KO)\n",
      "over (reversed: revo) -> rvov (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzaa (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3 20  1  2 21 20  8 20]\n",
      " [ 0  9  5  8  3  0 20  0 20  9]\n",
      " [ 0  5 14  8  0  0 12  0  0 12]\n",
      " [ 0 19  5  0  0  0 12  0  0  7]\n",
      " [ 0 14  5  0  0  0 20  0  0  9]\n",
      " [ 0 12  5  0  0  0 20  0  0  9]\n",
      " [ 0  0  5  0  0  0 20  0  0 15]\n",
      " [ 0  0  0  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 12  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsiesnl (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneeee (KO)\n",
      "that (reversed: taht) -> tthh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> dutlltttal (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gtilgiioo (KO)\n",
      "[[ 8  5  5 22  4 14  5  1 19  5]\n",
      " [ 3 15 12  9 18 21 20 16 19 12]\n",
      " [ 5 15  0 22 18  2 19 16 19  0]\n",
      " [ 6  0  0  0 18 18 20  0 19  0]\n",
      " [ 6  0  0  0 18  0  1  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0 19  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hcefff (KO)\n",
      "one (reversed: eno) -> eoo (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> viv (KO)\n",
      "renard (reversed: draner) -> drrrrr (KO)\n",
      "brun (reversed: nurb) -> nubr (KO)\n",
      "saute (reversed: etuas) -> etsta (KO)\n",
      "par (reversed: rap) -> app (KO)\n",
      "dessus (reversed: sussed) -> ssssss (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 21  5 19 14 25  7  8  4 20]\n",
      " [ 8  5 18  9  1 12 14  3 15  5]\n",
      " [ 8 19  8  0  0 13 15  5 15  5]\n",
      " [ 8 19  8  0  0  5 12  6 23  5]\n",
      " [ 7 19  0  0  0  5  0  6  0 12]\n",
      " [ 0  5  0  0  0  5  0  6  0  5]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0  5  0  0  0  5  0  0  0 14]\n",
      " [ 0 19  0  0  0  5  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nhhhg (KO)\n",
      "paresseux (reversed: xuesserap) -> uessseees (KO)\n",
      "here (reversed: ereh) -> erhh (KO)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylmeeeeee (KO)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hcefff (KO)\n",
      "word (reversed: drow) -> doow (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> teeelennllooiooooinnnnnni (KO)\n",
      "* correct: 15/40 -> 37.5%\n",
      "\n",
      "* step: 28001 loss: 0.905781\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  3 18  6 13 22 20  1  4  0]\n",
      " [ 0  9  2  0 21 22  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kcciq (KO)\n",
      "brown (reversed: nworb) -> nwrbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmuj (OK)\n",
      "over (reversed: revo) -> revv (KO)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0 12  0 20  9]\n",
      " [ 0 12 14 20  0  0  5  0  0 23]\n",
      " [ 0 12  5  0  0  0 12  0  0 12]\n",
      " [ 0 14  5  0  0  0 12  0  0 12]\n",
      " [ 0  5 14  0  0  0  1  0  0 12]\n",
      " [ 0  0  5  0  0  0  1  0  0 12]\n",
      " [ 0  0  0  0  0  0 14  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsillne (KO)\n",
      "sentence (reversed: ecnetnes) -> eceneene (KO)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> delellaanr (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwllllo (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21  1 16 19 12]\n",
      " [ 5 15  0  9  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 18  2 21  0 19  0]\n",
      " [18  0  0  0 18  0 19  0 19  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fii (KO)\n",
      "renard (reversed: draner) -> drarrr (KO)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> eauus (KO)\n",
      "par (reversed: rap) -> rpp (KO)\n",
      "dessus (reversed: sussed) -> sssssd (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5  5 18  9  1 12 14  3 18 14]\n",
      " [ 8 19  5  0  0 12 14  5 18  5]\n",
      " [ 8 19  8  0  0  5 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 13  0  6  0 13]\n",
      " [ 0  5  0  0  0  5  0  0  0  5]\n",
      " [ 0 18  0  0  0  5  0  0  0 14]\n",
      " [ 0 16  0  0  0 24  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 12]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> nehhc (KO)\n",
      "paresseux (reversed: xuesserap) -> xessseerp (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylleemeex (KO)\n",
      "long (reversed: gnol) -> gnnl (KO)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> drrw (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeemennlniunnnninininnn (KO)\n",
      "* correct: 20/40 -> 50.0%\n",
      "\n",
      "* step: 29001 loss: 0.454673\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 18  6 13 22 20 12  4  0]\n",
      " [ 0 21  2  0 10 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nwrbb (KO)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> spmjj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzll (KO)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [14 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0 20  0 20  9]\n",
      " [ 0 12 14  8  0  0 12  0  0 23]\n",
      " [ 0 14 14  0  0  0 12  0  0 12]\n",
      " [ 0  5 14  0  0  0 14  0  0 12]\n",
      " [ 0  5  5  0  0  0  1  0  0 15]\n",
      " [ 0  0 19  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> nn (KO)\n",
      "english (reversed: hsilgne) -> hsilnee (KO)\n",
      "sentence (reversed: ecnetnes) -> ecennnes (KO)\n",
      "that (reversed: taht) -> tahh (KO)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> detllnartt (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwlloff (KO)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 19 12]\n",
      " [ 5 15  0 22  1  2 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [ 6  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0  5  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceeff (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> dranee (KO)\n",
      "brun (reversed: nurb) -> nubb (KO)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssed (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9 14 12 14  3 18  5]\n",
      " [ 9  5  5  0  0  5 12  5 23  5]\n",
      " [ 3 19  8  0  0 13 12  5 23  5]\n",
      " [ 3  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  6  0  5]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0  9]\n",
      " [ 0 16  0  0  0  5  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neicc (KO)\n",
      "paresseux (reversed: xuesserap) -> xueserapp (KO)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> nn (KO)\n",
      "extremely (reversed: ylemertxe) -> ylemrtxee (KO)\n",
      "long (reversed: gnol) -> gnll (KO)\n",
      "french (reversed: hcnerf) -> hceeff (KO)\n",
      "word (reversed: drow) -> drww (KO)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> neeeneninitittttsnoiitnaa (KO)\n",
      "* correct: 19/40 -> 47.5%\n",
      "\n",
      "*** evaluation! loss: 0.307409\n",
      "[[ 5 11 14 24 19 18  5 25  7 19]\n",
      " [ 8  3 23 15 16  5  8 26 15  9]\n",
      " [20  9 15  6 16 22 20  1  4  0]\n",
      " [ 0 21 18  0 21 15  0 12  0  0]\n",
      " [ 0 17  2  0 10  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "the (reversed: eht) -> eht (OK)\n",
      "quick (reversed: kciuq) -> kciuq (OK)\n",
      "brown (reversed: nworb) -> nworb (OK)\n",
      "fox (reversed: xof) -> xof (OK)\n",
      "jumps (reversed: spmuj) -> sppuj (KO)\n",
      "over (reversed: revo) -> revo (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "lazy (reversed: yzal) -> yzal (OK)\n",
      "dog (reversed: god) -> god (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "[[14  8  5 20 14  5  4 15  5  7]\n",
      " [ 1 19  3  1  1  2  5 20  8 14]\n",
      " [ 0  9  5  8  3  0  1  0 20  9]\n",
      " [ 0 12  5 20  0  0  1  0  0 23]\n",
      " [ 0  7 20  0  0  0 12  0  0 15]\n",
      " [ 0 14 14  0  0  0 19  0  0 12]\n",
      " [ 0  5  5  0  0  0 14  0  0 12]\n",
      " [ 0  0 19  0  0  0  1  0  0 15]\n",
      " [ 0  0  0  0  0  0 18  0  0  6]\n",
      " [ 0  0  0  0  0  0 20  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "an (reversed: na) -> na (OK)\n",
      "english (reversed: hsilgne) -> hsilgne (OK)\n",
      "sentence (reversed: ecnetnes) -> eceetnes (KO)\n",
      "that (reversed: taht) -> taht (OK)\n",
      "can (reversed: nac) -> nac (OK)\n",
      "be (reversed: eb) -> eb (OK)\n",
      "translated (reversed: detalsnart) -> deaalsnart (KO)\n",
      "to (reversed: ot) -> ot (OK)\n",
      "the (reversed: eht) -> eht (OK)\n",
      "following (reversed: gniwollof) -> gniwollof (OK)\n",
      "[[ 8  5  5  6  4 14  5 18 19  5]\n",
      " [ 3 14 12  9 18 21 20  1 19 12]\n",
      " [ 5 15  0 22  1 18 21 16 19  0]\n",
      " [ 5  0  0  0 14  2  1  0 19  0]\n",
      " [18  0  0  0  5  0 19  0  5  0]\n",
      " [ 6  0  0  0 18  0  0  0  4  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "one (reversed: eno) -> eno (OK)\n",
      "le (reversed: el) -> el (OK)\n",
      "vif (reversed: fiv) -> fiv (OK)\n",
      "renard (reversed: draner) -> draner (OK)\n",
      "brun (reversed: nurb) -> nurb (OK)\n",
      "saute (reversed: etuas) -> etuas (OK)\n",
      "par (reversed: rap) -> rap (OK)\n",
      "dessus (reversed: sussed) -> ssssed (KO)\n",
      "le (reversed: el) -> el (OK)\n",
      "[[14 24  5 19 14 25  7  8  4 14]\n",
      " [ 5 21 18  9  1 12 14  3 18 14]\n",
      " [ 9  5  5  0  0  5 15  5 15  5]\n",
      " [ 8 19  8  0  0 13 12  5 23  5]\n",
      " [ 3 19  0  0  0  5  0 18  0  5]\n",
      " [ 0  5  0  0  0 18  0  6  0 14]\n",
      " [ 0 18  0  0  0 20  0  0  0 12]\n",
      " [ 0  1  0  0  0 24  0  0  0 14]\n",
      " [ 0 16  0  0  0  5  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 21]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 19]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0 15]\n",
      " [ 0  0  0  0  0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  9]\n",
      " [ 0  0  0  0  0  0  0  0  0 20]\n",
      " [ 0  0  0  0  0  0  0  0  0 14]\n",
      " [ 0  0  0  0  0  0  0  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0]]\n",
      "chien (reversed: neihc) -> neihc (OK)\n",
      "paresseux (reversed: xuesserap) -> xuesserap (OK)\n",
      "here (reversed: ereh) -> ereh (OK)\n",
      "is (reversed: si) -> si (OK)\n",
      "an (reversed: na) -> na (OK)\n",
      "extremely (reversed: ylemertxe) -> ylemertxe (OK)\n",
      "long (reversed: gnol) -> gnol (OK)\n",
      "french (reversed: hcnerf) -> hceerf (KO)\n",
      "word (reversed: drow) -> drow (OK)\n",
      "anticonstitutionnellement (reversed: tnemellennoitutitsnocitna) -> nneeenlniititutitsoocitna (KO)\n",
      "* correct: 33/40 -> 82.5%\n",
      "\n",
      "CPU times: user 3h 46min 57s, sys: 4h 8min 49s, total: 7h 55min 47s\n",
      "Wall time: 1h 57min 45s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "%time reverse_text(30000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Okay, let's say it nearly reversed the longest string! And the shorter strings have all been reversed pretty quickly.\n",
    "\n",
    "\"teellllnnnoitutitsnocitna\" (given at the beginning, at step 5000) is not far from \"tnemellennoitutitsnocitna\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
